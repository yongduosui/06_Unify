----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='citeseer', drop_percent=0.05, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6932]
Loss:[0.6921]
Loss:[0.6894]
Loss:[0.6866]
Loss:[0.6820]
Loss:[0.6779]
Loss:[0.6712]
Loss:[0.6648]
Loss:[0.6565]
Loss:[0.6471]
Loss:[0.6388]
Loss:[0.6259]
Loss:[0.6141]
Loss:[0.6024]
Loss:[0.5877]
Loss:[0.5740]
Loss:[0.5596]
Loss:[0.5459]
Loss:[0.5284]
Loss:[0.5116]
Loss:[0.4966]
Loss:[0.4838]
Loss:[0.4655]
Loss:[0.4448]
Loss:[0.4320]
Loss:[0.4206]
Loss:[0.4029]
Loss:[0.3866]
Loss:[0.3739]
Loss:[0.3627]
Loss:[0.3483]
Loss:[0.3355]
Loss:[0.3241]
Loss:[0.3122]
Loss:[0.3054]
Loss:[0.2915]
Loss:[0.2805]
Loss:[0.2684]
Loss:[0.2629]
Loss:[0.2538]
Loss:[0.2444]
Loss:[0.2366]
Loss:[0.2320]
Loss:[0.2264]
Loss:[0.2162]
Loss:[0.2055]
Loss:[0.2048]
Loss:[0.2009]
Loss:[0.1952]
Loss:[0.1936]
Loss:[0.1874]
Loss:[0.1893]
Loss:[0.1817]
Loss:[0.1803]
Loss:[0.1769]
Loss:[0.1730]
Loss:[0.1707]
Loss:[0.1607]
Loss:[0.1618]
Loss:[0.1664]
Loss:[0.1584]
Loss:[0.1630]
Loss:[0.1488]
Loss:[0.1473]
Loss:[0.1542]
Loss:[0.1458]
Loss:[0.1529]
Loss:[0.1421]
Loss:[0.1423]
Loss:[0.1381]
Loss:[0.1405]
Loss:[0.1306]
Loss:[0.1360]
Loss:[0.1326]
Loss:[0.1317]
Loss:[0.1334]
Loss:[0.1317]
Loss:[0.1269]
Loss:[0.1291]
Loss:[0.1210]
Loss:[0.1273]
Loss:[0.1257]
Loss:[0.1197]
Loss:[0.1216]
Loss:[0.1213]
Loss:[0.1154]
Loss:[0.1232]
Loss:[0.1160]
Loss:[0.1158]
Loss:[0.1162]
Loss:[0.1167]
Loss:[0.1108]
Loss:[0.1149]
Loss:[0.1135]
Loss:[0.1184]
Loss:[0.1150]
Loss:[0.1158]
Loss:[0.1108]
Loss:[0.1109]
Loss:[0.1113]
Loss:[0.1077]
Loss:[0.1079]
Loss:[0.1190]
Loss:[0.1081]
Loss:[0.1097]
Loss:[0.1042]
Loss:[0.1097]
Loss:[0.1033]
Loss:[0.1069]
Loss:[0.1061]
Loss:[0.1012]
Loss:[0.1011]
Loss:[0.1050]
Loss:[0.1072]
Loss:[0.0984]
Loss:[0.1086]
Loss:[0.1021]
Loss:[0.1013]
Loss:[0.1021]
Loss:[0.0999]
Loss:[0.0990]
Loss:[0.1002]
Loss:[0.1039]
Loss:[0.0969]
Loss:[0.0977]
Loss:[0.1015]
Loss:[0.0935]
Loss:[0.0965]
Loss:[0.0995]
Loss:[0.0983]
Loss:[0.1013]
Loss:[0.0989]
Loss:[0.0951]
Loss:[0.0962]
Loss:[0.1001]
Loss:[0.0974]
Loss:[0.0929]
Loss:[0.0953]
Loss:[0.0975]
Loss:[0.0963]
Loss:[0.0962]
Loss:[0.0972]
Loss:[0.0948]
Loss:[0.0951]
Loss:[0.1053]
Loss:[0.0985]
Loss:[0.0935]
Loss:[0.0956]
Loss:[0.0919]
Loss:[0.0921]
Loss:[0.0923]
Loss:[0.0914]
Loss:[0.0959]
Loss:[0.0873]
Loss:[0.0982]
Loss:[0.0956]
Loss:[0.0916]
Loss:[0.0914]
Loss:[0.0931]
Loss:[0.0917]
Loss:[0.0916]
Loss:[0.0960]
Loss:[0.0888]
Loss:[0.0916]
Loss:[0.0918]
Loss:[0.0916]
Loss:[0.0970]
Loss:[0.0889]
Loss:[0.0890]
Loss:[0.0942]
Loss:[0.0844]
Loss:[0.0936]
Loss:[0.0901]
Loss:[0.0864]
Loss:[0.0922]
Loss:[0.0891]
Loss:[0.0857]
Loss:[0.0875]
Loss:[0.0838]
Loss:[0.0893]
Loss:[0.0929]
Loss:[0.0959]
Loss:[0.0966]
Loss:[0.0931]
Loss:[0.0910]
Loss:[0.0850]
Loss:[0.0835]
Loss:[0.0870]
Loss:[0.0933]
Loss:[0.0845]
Loss:[0.0846]
Loss:[0.0894]
Loss:[0.0924]
Loss:[0.0860]
Loss:[0.0884]
Loss:[0.0870]
Loss:[0.0884]
Loss:[0.0887]
Loss:[0.0855]
Loss:[0.0910]
Loss:[0.0896]
Loss:[0.0837]
Loss:[0.0925]
Loss:[0.0871]
Loss:[0.0857]
Loss:[0.0909]
Loss:[0.0860]
Early stopping!
Loading 186th epoch
acc:[0.7270]
acc:[0.7260]
acc:[0.7250]
acc:[0.7230]
acc:[0.7260]
acc:[0.7250]
acc:[0.7240]
acc:[0.7250]
acc:[0.7250]
acc:[0.7240]
acc:[0.7230]
acc:[0.7250]
acc:[0.7250]
acc:[0.7250]
acc:[0.7240]
acc:[0.7240]
acc:[0.7260]
acc:[0.7220]
acc:[0.7260]
acc:[0.7260]
acc:[0.7250]
acc:[0.7240]
acc:[0.7250]
acc:[0.7240]
acc:[0.7250]
acc:[0.7300]
acc:[0.7250]
acc:[0.7260]
acc:[0.7280]
acc:[0.7250]
acc:[0.7230]
acc:[0.7240]
acc:[0.7270]
acc:[0.7250]
acc:[0.7280]
acc:[0.7230]
acc:[0.7270]
acc:[0.7250]
acc:[0.7240]
acc:[0.7230]
acc:[0.7240]
acc:[0.7250]
acc:[0.7240]
acc:[0.7250]
acc:[0.7260]
acc:[0.7260]
acc:[0.7240]
acc:[0.7250]
acc:[0.7250]
acc:[0.7240]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7250]
Mean:[72.5000]
Std :[0.1457]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='citeseer', drop_percent=0.05, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6918]
Loss:[0.6890]
Loss:[0.6857]
Loss:[0.6813]
Loss:[0.6757]
Loss:[0.6703]
Loss:[0.6620]
Loss:[0.6553]
Loss:[0.6447]
Loss:[0.6345]
Loss:[0.6247]
Loss:[0.6105]
Loss:[0.5994]
Loss:[0.5866]
Loss:[0.5715]
Loss:[0.5551]
Loss:[0.5427]
Loss:[0.5275]
Loss:[0.5093]
Loss:[0.4941]
Loss:[0.4814]
Loss:[0.4627]
Loss:[0.4428]
Loss:[0.4286]
Loss:[0.4187]
Loss:[0.4046]
Loss:[0.3869]
Loss:[0.3701]
Loss:[0.3585]
Loss:[0.3478]
Loss:[0.3332]
Loss:[0.3212]
Loss:[0.3062]
Loss:[0.3008]
Loss:[0.2854]
Loss:[0.2804]
Loss:[0.2716]
Loss:[0.2635]
Loss:[0.2557]
Loss:[0.2416]
Loss:[0.2390]
Loss:[0.2330]
Loss:[0.2251]
Loss:[0.2133]
Loss:[0.2120]
Loss:[0.2108]
Loss:[0.1949]
Loss:[0.1954]
Loss:[0.1874]
Loss:[0.1843]
Loss:[0.1861]
Loss:[0.1815]
Loss:[0.1770]
Loss:[0.1697]
Loss:[0.1669]
Loss:[0.1669]
Loss:[0.1601]
Loss:[0.1567]
Loss:[0.1534]
Loss:[0.1582]
Loss:[0.1498]
Loss:[0.1475]
Loss:[0.1467]
Loss:[0.1476]
Loss:[0.1400]
Loss:[0.1447]
Loss:[0.1366]
Loss:[0.1411]
Loss:[0.1407]
Loss:[0.1342]
Loss:[0.1391]
Loss:[0.1389]
Loss:[0.1318]
Loss:[0.1334]
Loss:[0.1391]
Loss:[0.1418]
Loss:[0.1195]
Loss:[0.1401]
Loss:[0.1429]
Loss:[0.1261]
Loss:[0.1410]
Loss:[0.1229]
Loss:[0.1406]
Loss:[0.1223]
Loss:[0.1211]
Loss:[0.1201]
Loss:[0.1152]
Loss:[0.1219]
Loss:[0.1117]
Loss:[0.1164]
Loss:[0.1115]
Loss:[0.1151]
Loss:[0.1122]
Loss:[0.1199]
Loss:[0.1131]
Loss:[0.1160]
Loss:[0.1160]
Loss:[0.1078]
Loss:[0.1074]
Loss:[0.1101]
Loss:[0.1097]
Loss:[0.1108]
Loss:[0.1098]
Loss:[0.1059]
Loss:[0.1095]
Loss:[0.1117]
Loss:[0.1121]
Loss:[0.1032]
Loss:[0.1036]
Loss:[0.1040]
Loss:[0.1051]
Loss:[0.1036]
Loss:[0.1155]
Loss:[0.1097]
Loss:[0.0994]
Loss:[0.1034]
Loss:[0.0979]
Loss:[0.0926]
Loss:[0.1006]
Loss:[0.0951]
Loss:[0.1028]
Loss:[0.1021]
Loss:[0.0962]
Loss:[0.1038]
Loss:[0.0922]
Loss:[0.0925]
Loss:[0.0947]
Loss:[0.0972]
Loss:[0.1017]
Loss:[0.1012]
Loss:[0.1018]
Loss:[0.0999]
Loss:[0.0955]
Loss:[0.0963]
Loss:[0.0966]
Loss:[0.0914]
Loss:[0.1031]
Loss:[0.0965]
Loss:[0.0891]
Loss:[0.0959]
Loss:[0.0967]
Loss:[0.0908]
Loss:[0.0956]
Loss:[0.0925]
Loss:[0.0911]
Loss:[0.0905]
Loss:[0.0910]
Loss:[0.0981]
Loss:[0.0929]
Loss:[0.0860]
Loss:[0.0877]
Loss:[0.0917]
Loss:[0.0848]
Loss:[0.0918]
Loss:[0.0981]
Loss:[0.0943]
Loss:[0.0934]
Loss:[0.0943]
Loss:[0.0917]
Loss:[0.0932]
Loss:[0.0954]
Loss:[0.0866]
Loss:[0.0894]
Loss:[0.0939]
Loss:[0.0883]
Loss:[0.0961]
Loss:[0.0959]
Loss:[0.1020]
Loss:[0.0869]
Loss:[0.0923]
Loss:[0.0866]
Loss:[0.0945]
Loss:[0.0859]
Early stopping!
Loading 153th epoch
acc:[0.7240]
acc:[0.7250]
acc:[0.7240]
acc:[0.7260]
acc:[0.7260]
acc:[0.7250]
acc:[0.7270]
acc:[0.7240]
acc:[0.7260]
acc:[0.7250]
acc:[0.7240]
acc:[0.7240]
acc:[0.7240]
acc:[0.7250]
acc:[0.7260]
acc:[0.7240]
acc:[0.7250]
acc:[0.7250]
acc:[0.7260]
acc:[0.7250]
acc:[0.7260]
acc:[0.7250]
acc:[0.7240]
acc:[0.7250]
acc:[0.7280]
acc:[0.7260]
acc:[0.7270]
acc:[0.7240]
acc:[0.7260]
acc:[0.7240]
acc:[0.7250]
acc:[0.7270]
acc:[0.7270]
acc:[0.7250]
acc:[0.7250]
acc:[0.7240]
acc:[0.7270]
acc:[0.7240]
acc:[0.7250]
acc:[0.7250]
acc:[0.7210]
acc:[0.7250]
acc:[0.7250]
acc:[0.7250]
acc:[0.7260]
acc:[0.7250]
acc:[0.7220]
acc:[0.7250]
acc:[0.7260]
acc:[0.7270]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7251]
Mean:[72.5120]
Std :[0.1272]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='citeseer', drop_percent=0.05, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6917]
Loss:[0.6889]
Loss:[0.6853]
Loss:[0.6814]
Loss:[0.6752]
Loss:[0.6698]
Loss:[0.6610]
Loss:[0.6546]
Loss:[0.6440]
Loss:[0.6336]
Loss:[0.6244]
Loss:[0.6091]
Loss:[0.5976]
Loss:[0.5848]
Loss:[0.5681]
Loss:[0.5535]
Loss:[0.5407]
Loss:[0.5241]
Loss:[0.5078]
Loss:[0.4920]
Loss:[0.4808]
Loss:[0.4617]
Loss:[0.4433]
Loss:[0.4258]
Loss:[0.4142]
Loss:[0.4015]
Loss:[0.3867]
Loss:[0.3684]
Loss:[0.3539]
Loss:[0.3410]
Loss:[0.3351]
Loss:[0.3205]
Loss:[0.3057]
Loss:[0.2940]
Loss:[0.2884]
Loss:[0.2791]
Loss:[0.2640]
Loss:[0.2575]
Loss:[0.2470]
Loss:[0.2431]
Loss:[0.2383]
Loss:[0.2301]
Loss:[0.2157]
Loss:[0.2137]
Loss:[0.2100]
Loss:[0.2021]
Loss:[0.2024]
Loss:[0.1936]
Loss:[0.1904]
Loss:[0.1887]
Loss:[0.1775]
Loss:[0.1739]
Loss:[0.1723]
Loss:[0.1722]
Loss:[0.1636]
Loss:[0.1640]
Loss:[0.1607]
Loss:[0.1633]
Loss:[0.1579]
Loss:[0.1564]
Loss:[0.1497]
Loss:[0.1445]
Loss:[0.1496]
Loss:[0.1448]
Loss:[0.1493]
Loss:[0.1520]
Loss:[0.1498]
Loss:[0.1409]
Loss:[0.1390]
Loss:[0.1391]
Loss:[0.1339]
Loss:[0.1310]
Loss:[0.1314]
Loss:[0.1317]
Loss:[0.1289]
Loss:[0.1288]
Loss:[0.1346]
Loss:[0.1321]
Loss:[0.1251]
Loss:[0.1211]
Loss:[0.1294]
Loss:[0.1210]
Loss:[0.1234]
Loss:[0.1235]
Loss:[0.1195]
Loss:[0.1175]
Loss:[0.1132]
Loss:[0.1188]
Loss:[0.1175]
Loss:[0.1101]
Loss:[0.1150]
Loss:[0.1128]
Loss:[0.1108]
Loss:[0.1190]
Loss:[0.1174]
Loss:[0.1167]
Loss:[0.1104]
Loss:[0.1106]
Loss:[0.1100]
Loss:[0.1146]
Loss:[0.1141]
Loss:[0.1043]
Loss:[0.1095]
Loss:[0.1167]
Loss:[0.1111]
Loss:[0.1061]
Loss:[0.1042]
Loss:[0.1051]
Loss:[0.0997]
Loss:[0.1089]
Loss:[0.1051]
Loss:[0.0960]
Loss:[0.1010]
Loss:[0.1015]
Loss:[0.1058]
Loss:[0.1030]
Loss:[0.1014]
Loss:[0.1023]
Loss:[0.0999]
Loss:[0.1004]
Loss:[0.1015]
Loss:[0.1071]
Loss:[0.0948]
Loss:[0.1027]
Loss:[0.0929]
Loss:[0.1000]
Loss:[0.1012]
Loss:[0.1036]
Loss:[0.1015]
Loss:[0.0936]
Loss:[0.0991]
Loss:[0.0986]
Loss:[0.0964]
Loss:[0.1013]
Loss:[0.1004]
Loss:[0.0885]
Loss:[0.1026]
Loss:[0.1007]
Loss:[0.1015]
Loss:[0.0940]
Loss:[0.1074]
Loss:[0.0971]
Loss:[0.0983]
Loss:[0.0992]
Loss:[0.0996]
Loss:[0.0972]
Loss:[0.0973]
Loss:[0.0916]
Loss:[0.0982]
Loss:[0.0938]
Loss:[0.0933]
Loss:[0.1045]
Loss:[0.0988]
Loss:[0.0920]
Loss:[0.0942]
Loss:[0.0921]
Early stopping!
Loading 136th epoch
acc:[0.7250]
acc:[0.7220]
acc:[0.7250]
acc:[0.7250]
acc:[0.7270]
acc:[0.7250]
acc:[0.7250]
acc:[0.7260]
acc:[0.7240]
acc:[0.7270]
acc:[0.7240]
acc:[0.7250]
acc:[0.7250]
acc:[0.7250]
acc:[0.7250]
acc:[0.7230]
acc:[0.7240]
acc:[0.7270]
acc:[0.7270]
acc:[0.7240]
acc:[0.7250]
acc:[0.7250]
acc:[0.7250]
acc:[0.7220]
acc:[0.7240]
acc:[0.7280]
acc:[0.7260]
acc:[0.7260]
acc:[0.7250]
acc:[0.7230]
acc:[0.7260]
acc:[0.7230]
acc:[0.7230]
acc:[0.7250]
acc:[0.7230]
acc:[0.7220]
acc:[0.7270]
acc:[0.7260]
acc:[0.7240]
acc:[0.7260]
acc:[0.7220]
acc:[0.7240]
acc:[0.7250]
acc:[0.7280]
acc:[0.7240]
acc:[0.7270]
acc:[0.7220]
acc:[0.7270]
acc:[0.7260]
acc:[0.7250]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7249]
Mean:[72.4880]
Std :[0.1599]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='citeseer', drop_percent=0.05, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6932]
Loss:[0.6919]
Loss:[0.6891]
Loss:[0.6859]
Loss:[0.6815]
Loss:[0.6762]
Loss:[0.6705]
Loss:[0.6623]
Loss:[0.6556]
Loss:[0.6451]
Loss:[0.6352]
Loss:[0.6255]
Loss:[0.6116]
Loss:[0.6003]
Loss:[0.5864]
Loss:[0.5699]
Loss:[0.5559]
Loss:[0.5442]
Loss:[0.5259]
Loss:[0.5084]
Loss:[0.4940]
Loss:[0.4793]
Loss:[0.4675]
Loss:[0.4457]
Loss:[0.4284]
Loss:[0.4192]
Loss:[0.4048]
Loss:[0.3862]
Loss:[0.3727]
Loss:[0.3628]
Loss:[0.3444]
Loss:[0.3322]
Loss:[0.3241]
Loss:[0.3179]
Loss:[0.3006]
Loss:[0.2867]
Loss:[0.2818]
Loss:[0.2720]
Loss:[0.2561]
Loss:[0.2555]
Loss:[0.2503]
Loss:[0.2377]
Loss:[0.2320]
Loss:[0.2261]
Loss:[0.2203]
Loss:[0.2102]
Loss:[0.2107]
Loss:[0.2046]
Loss:[0.2002]
Loss:[0.1914]
Loss:[0.1930]
Loss:[0.1872]
Loss:[0.1787]
Loss:[0.1815]
Loss:[0.1752]
Loss:[0.1751]
Loss:[0.1703]
Loss:[0.1662]
Loss:[0.1618]
Loss:[0.1562]
Loss:[0.1559]
Loss:[0.1570]
Loss:[0.1527]
Loss:[0.1530]
Loss:[0.1547]
Loss:[0.1501]
Loss:[0.1427]
Loss:[0.1460]
Loss:[0.1359]
Loss:[0.1404]
Loss:[0.1384]
Loss:[0.1341]
Loss:[0.1397]
Loss:[0.1327]
Loss:[0.1373]
Loss:[0.1322]
Loss:[0.1263]
Loss:[0.1392]
Loss:[0.1248]
Loss:[0.1274]
Loss:[0.1238]
Loss:[0.1274]
Loss:[0.1258]
Loss:[0.1204]
Loss:[0.1205]
Loss:[0.1282]
Loss:[0.1248]
Loss:[0.1210]
Loss:[0.1171]
Loss:[0.1220]
Loss:[0.1159]
Loss:[0.1247]
Loss:[0.1191]
Loss:[0.1139]
Loss:[0.1141]
Loss:[0.1191]
Loss:[0.1158]
Loss:[0.1111]
Loss:[0.1086]
Loss:[0.1187]
Loss:[0.1083]
Loss:[0.1049]
Loss:[0.1096]
Loss:[0.1171]
Loss:[0.1078]
Loss:[0.1131]
Loss:[0.1076]
Loss:[0.1107]
Loss:[0.1015]
Loss:[0.1032]
Loss:[0.1039]
Loss:[0.1034]
Loss:[0.1091]
Loss:[0.1064]
Loss:[0.1052]
Loss:[0.0956]
Loss:[0.1007]
Loss:[0.1055]
Loss:[0.1069]
Loss:[0.1095]
Loss:[0.1011]
Loss:[0.1022]
Loss:[0.1041]
Loss:[0.1022]
Loss:[0.1038]
Loss:[0.0930]
Loss:[0.1025]
Loss:[0.0973]
Loss:[0.0944]
Loss:[0.0943]
Loss:[0.1015]
Loss:[0.0955]
Loss:[0.0991]
Loss:[0.1051]
Loss:[0.0902]
Loss:[0.1002]
Loss:[0.0962]
Loss:[0.0958]
Loss:[0.0944]
Loss:[0.0951]
Loss:[0.0946]
Loss:[0.0911]
Loss:[0.0953]
Loss:[0.0969]
Loss:[0.0910]
Loss:[0.0968]
Loss:[0.0928]
Loss:[0.0961]
Loss:[0.0906]
Loss:[0.0934]
Loss:[0.0901]
Loss:[0.0898]
Loss:[0.0972]
Loss:[0.0950]
Loss:[0.0867]
Loss:[0.1056]
Loss:[0.0976]
Loss:[0.0932]
Loss:[0.0948]
Loss:[0.0864]
Loss:[0.0966]
Loss:[0.0936]
Loss:[0.0963]
Loss:[0.0873]
Loss:[0.0943]
Loss:[0.0956]
Loss:[0.0856]
Loss:[0.0957]
Loss:[0.0888]
Loss:[0.0897]
Loss:[0.0936]
Loss:[0.0966]
Loss:[0.0892]
Loss:[0.0869]
Loss:[0.0870]
Loss:[0.0910]
Loss:[0.0906]
Loss:[0.0941]
Loss:[0.0871]
Loss:[0.0920]
Loss:[0.0870]
Loss:[0.0917]
Loss:[0.0882]
Loss:[0.0894]
Loss:[0.0970]
Loss:[0.0896]
Loss:[0.0855]
Loss:[0.0866]
Loss:[0.0836]
Loss:[0.0944]
Loss:[0.0837]
Loss:[0.0909]
Loss:[0.0872]
Loss:[0.0832]
Loss:[0.0928]
Loss:[0.0828]
Loss:[0.0831]
Loss:[0.0898]
Loss:[0.0840]
Loss:[0.0861]
Loss:[0.0787]
Loss:[0.0883]
Loss:[0.0841]
Loss:[0.0825]
Loss:[0.0883]
Loss:[0.0840]
Loss:[0.0802]
Loss:[0.0845]
Loss:[0.0910]
Loss:[0.0908]
Loss:[0.0880]
Loss:[0.0854]
Loss:[0.0790]
Loss:[0.0901]
Loss:[0.0895]
Loss:[0.0888]
Loss:[0.0898]
Loss:[0.0816]
Loss:[0.0851]
Loss:[0.0882]
Loss:[0.0885]
Early stopping!
Loading 200th epoch
acc:[0.7130]
acc:[0.7130]
acc:[0.7110]
acc:[0.7150]
acc:[0.7130]
acc:[0.7150]
acc:[0.7090]
acc:[0.7120]
acc:[0.7130]
acc:[0.7130]
acc:[0.7100]
acc:[0.7120]
acc:[0.7130]
acc:[0.7140]
acc:[0.7120]
acc:[0.7140]
acc:[0.7130]
acc:[0.7100]
acc:[0.7120]
acc:[0.7120]
acc:[0.7120]
acc:[0.7090]
acc:[0.7130]
acc:[0.7130]
acc:[0.7110]
acc:[0.7130]
acc:[0.7140]
acc:[0.7130]
acc:[0.7120]
acc:[0.7150]
acc:[0.7130]
acc:[0.7140]
acc:[0.7110]
acc:[0.7110]
acc:[0.7090]
acc:[0.7120]
acc:[0.7150]
acc:[0.7120]
acc:[0.7130]
acc:[0.7140]
acc:[0.7120]
acc:[0.7140]
acc:[0.7120]
acc:[0.7120]
acc:[0.7150]
acc:[0.7140]
acc:[0.7110]
acc:[0.7140]
acc:[0.7120]
acc:[0.7140]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7126]
Mean:[71.2560]
Std :[0.1554]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='citeseer', drop_percent=0.05, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6916]
Loss:[0.6888]
Loss:[0.6851]
Loss:[0.6811]
Loss:[0.6749]
Loss:[0.6697]
Loss:[0.6609]
Loss:[0.6536]
Loss:[0.6438]
Loss:[0.6330]
Loss:[0.6239]
Loss:[0.6093]
Loss:[0.5971]
Loss:[0.5856]
Loss:[0.5674]
Loss:[0.5532]
Loss:[0.5408]
Loss:[0.5256]
Loss:[0.5084]
Loss:[0.4925]
Loss:[0.4773]
Loss:[0.4621]
Loss:[0.4443]
Loss:[0.4286]
Loss:[0.4134]
Loss:[0.4020]
Loss:[0.3862]
Loss:[0.3678]
Loss:[0.3561]
Loss:[0.3434]
Loss:[0.3348]
Loss:[0.3186]
Loss:[0.3083]
Loss:[0.2968]
Loss:[0.2930]
Loss:[0.2794]
Loss:[0.2720]
Loss:[0.2639]
Loss:[0.2551]
Loss:[0.2505]
Loss:[0.2393]
Loss:[0.2280]
Loss:[0.2289]
Loss:[0.2181]
Loss:[0.2155]
Loss:[0.2098]
Loss:[0.2053]
Loss:[0.2001]
Loss:[0.1920]
Loss:[0.1903]
Loss:[0.1930]
Loss:[0.1771]
Loss:[0.1806]
Loss:[0.1791]
Loss:[0.1661]
Loss:[0.1664]
Loss:[0.1660]
Loss:[0.1559]
Loss:[0.1557]
Loss:[0.1586]
Loss:[0.1525]
Loss:[0.1484]
Loss:[0.1431]
Loss:[0.1525]
Loss:[0.1464]
Loss:[0.1417]
Loss:[0.1438]
Loss:[0.1416]
Loss:[0.1395]
Loss:[0.1344]
Loss:[0.1371]
Loss:[0.1339]
Loss:[0.1321]
Loss:[0.1308]
Loss:[0.1382]
Loss:[0.1302]
Loss:[0.1286]
Loss:[0.1243]
Loss:[0.1314]
Loss:[0.1257]
Loss:[0.1257]
Loss:[0.1230]
Loss:[0.1182]
Loss:[0.1252]
Loss:[0.1282]
Loss:[0.1208]
Loss:[0.1230]
Loss:[0.1237]
Loss:[0.1204]
Loss:[0.1206]
Loss:[0.1214]
Loss:[0.1198]
Loss:[0.1116]
Loss:[0.1215]
Loss:[0.1190]
Loss:[0.1103]
Loss:[0.1170]
Loss:[0.1128]
Loss:[0.1141]
Loss:[0.1217]
Loss:[0.1051]
Loss:[0.1118]
Loss:[0.1001]
Loss:[0.1121]
Loss:[0.1102]
Loss:[0.1063]
Loss:[0.1060]
Loss:[0.1085]
Loss:[0.1037]
Loss:[0.0999]
Loss:[0.1092]
Loss:[0.1013]
Loss:[0.1033]
Loss:[0.1034]
Loss:[0.1096]
Loss:[0.1135]
Loss:[0.1093]
Loss:[0.1063]
Loss:[0.0990]
Loss:[0.0989]
Loss:[0.0984]
Loss:[0.1005]
Loss:[0.0966]
Loss:[0.0991]
Loss:[0.1011]
Loss:[0.0995]
Loss:[0.1022]
Loss:[0.0980]
Loss:[0.0963]
Loss:[0.0916]
Loss:[0.1004]
Loss:[0.0988]
Loss:[0.0972]
Loss:[0.0970]
Loss:[0.1031]
Loss:[0.0995]
Loss:[0.0983]
Loss:[0.0941]
Loss:[0.1041]
Loss:[0.0973]
Loss:[0.0979]
Loss:[0.0952]
Loss:[0.0904]
Loss:[0.0910]
Loss:[0.0931]
Loss:[0.0922]
Loss:[0.0992]
Loss:[0.0917]
Loss:[0.0988]
Loss:[0.0964]
Loss:[0.0904]
Loss:[0.0847]
Loss:[0.0983]
Loss:[0.0992]
Loss:[0.0885]
Loss:[0.0900]
Loss:[0.0986]
Loss:[0.0961]
Loss:[0.0894]
Loss:[0.0871]
Loss:[0.0964]
Loss:[0.0923]
Loss:[0.0879]
Loss:[0.1014]
Loss:[0.0876]
Loss:[0.0952]
Loss:[0.0926]
Loss:[0.0895]
Loss:[0.0910]
Loss:[0.0919]
Loss:[0.0915]
Loss:[0.0938]
Early stopping!
Loading 152th epoch
acc:[0.7220]
acc:[0.7220]
acc:[0.7240]
acc:[0.7250]
acc:[0.7230]
acc:[0.7230]
acc:[0.7220]
acc:[0.7250]
acc:[0.7230]
acc:[0.7240]
acc:[0.7260]
acc:[0.7190]
acc:[0.7220]
acc:[0.7210]
acc:[0.7240]
acc:[0.7220]
acc:[0.7230]
acc:[0.7190]
acc:[0.7250]
acc:[0.7250]
acc:[0.7240]
acc:[0.7230]
acc:[0.7230]
acc:[0.7210]
acc:[0.7250]
acc:[0.7270]
acc:[0.7230]
acc:[0.7200]
acc:[0.7230]
acc:[0.7230]
acc:[0.7240]
acc:[0.7230]
acc:[0.7250]
acc:[0.7250]
acc:[0.7200]
acc:[0.7220]
acc:[0.7240]
acc:[0.7240]
acc:[0.7240]
acc:[0.7220]
acc:[0.7240]
acc:[0.7230]
acc:[0.7200]
acc:[0.7210]
acc:[0.7210]
acc:[0.7220]
acc:[0.7190]
acc:[0.7230]
acc:[0.7240]
acc:[0.7240]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7229]
Mean:[72.2900]
Std :[0.1821]
----------------------------------------------------------------------------------------------------
