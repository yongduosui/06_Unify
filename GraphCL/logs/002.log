----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='citeseer', drop_percent=0.2, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[edge]
Using CUDA
Loss:[0.6931]
Loss:[0.6941]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6794]
Loss:[0.6811]
Loss:[0.6694]
Loss:[0.6649]
Loss:[0.6573]
Loss:[0.6438]
Loss:[0.6386]
Loss:[0.6211]
Loss:[0.6134]
Loss:[0.5933]
Loss:[0.5840]
Loss:[0.5651]
Loss:[0.5454]
Loss:[0.5348]
Loss:[0.5093]
Loss:[0.4966]
Loss:[0.4813]
Loss:[0.4547]
Loss:[0.4392]
Loss:[0.4281]
Loss:[0.4100]
Loss:[0.3837]
Loss:[0.3711]
Loss:[0.3628]
Loss:[0.3432]
Loss:[0.3211]
Loss:[0.3129]
Loss:[0.3018]
Loss:[0.2856]
Loss:[0.2771]
Loss:[0.2618]
Loss:[0.2532]
Loss:[0.2448]
Loss:[0.2384]
Loss:[0.2307]
Loss:[0.2221]
Loss:[0.2155]
Loss:[0.2090]
Loss:[0.2099]
Loss:[0.1946]
Loss:[0.1887]
Loss:[0.1815]
Loss:[0.1813]
Loss:[0.1746]
Loss:[0.1719]
Loss:[0.1629]
Loss:[0.1627]
Loss:[0.1568]
Loss:[0.1622]
Loss:[0.1523]
Loss:[0.1520]
Loss:[0.1467]
Loss:[0.1384]
Loss:[0.1457]
Loss:[0.1380]
Loss:[0.1360]
Loss:[0.1310]
Loss:[0.1368]
Loss:[0.1352]
Loss:[0.1340]
Loss:[0.1253]
Loss:[0.1310]
Loss:[0.1249]
Loss:[0.1288]
Loss:[0.1262]
Loss:[0.1251]
Loss:[0.1327]
Loss:[0.1287]
Loss:[0.1185]
Loss:[0.1269]
Loss:[0.1242]
Loss:[0.1248]
Loss:[0.1215]
Loss:[0.1236]
Loss:[0.1145]
Loss:[0.1170]
Loss:[0.1314]
Loss:[0.1352]
Loss:[0.1111]
Loss:[0.1343]
Loss:[0.1254]
Loss:[0.1184]
Loss:[0.1421]
Loss:[0.1182]
Loss:[0.1149]
Loss:[0.1097]
Loss:[0.1074]
Loss:[0.1207]
Loss:[0.1092]
Loss:[0.1170]
Loss:[0.1037]
Loss:[0.1101]
Loss:[0.0985]
Loss:[0.1161]
Loss:[0.1023]
Loss:[0.1056]
Loss:[0.0969]
Loss:[0.0991]
Loss:[0.0985]
Loss:[0.0989]
Loss:[0.0938]
Loss:[0.1038]
Loss:[0.1014]
Loss:[0.0986]
Loss:[0.1035]
Loss:[0.0939]
Loss:[0.0934]
Loss:[0.0951]
Loss:[0.1017]
Loss:[0.0939]
Loss:[0.0901]
Loss:[0.0970]
Loss:[0.0963]
Loss:[0.0920]
Loss:[0.1028]
Loss:[0.1002]
Loss:[0.1032]
Loss:[0.0978]
Loss:[0.0946]
Loss:[0.0953]
Loss:[0.0985]
Loss:[0.1036]
Loss:[0.0881]
Loss:[0.0974]
Loss:[0.0974]
Loss:[0.0979]
Loss:[0.0927]
Loss:[0.1011]
Loss:[0.0926]
Loss:[0.0934]
Loss:[0.0949]
Loss:[0.0921]
Loss:[0.0912]
Loss:[0.1006]
Loss:[0.0919]
Loss:[0.0975]
Loss:[0.0967]
Loss:[0.0987]
Loss:[0.0929]
Loss:[0.0910]
Loss:[0.0918]
Loss:[0.0958]
Loss:[0.0869]
Loss:[0.0861]
Loss:[0.0952]
Loss:[0.0920]
Loss:[0.0947]
Loss:[0.0906]
Loss:[0.0957]
Loss:[0.0875]
Loss:[0.0920]
Loss:[0.0920]
Loss:[0.0909]
Loss:[0.0946]
Loss:[0.0885]
Loss:[0.0951]
Loss:[0.0910]
Loss:[0.0829]
Loss:[0.0871]
Loss:[0.0863]
Loss:[0.0885]
Loss:[0.0871]
Loss:[0.0942]
Loss:[0.0850]
Loss:[0.0975]
Loss:[0.0898]
Loss:[0.0933]
Loss:[0.0964]
Loss:[0.0847]
Loss:[0.0896]
Loss:[0.0862]
Loss:[0.0854]
Loss:[0.0952]
Loss:[0.0874]
Loss:[0.0929]
Loss:[0.0848]
Loss:[0.0837]
Loss:[0.0836]
Early stopping!
Loading 161th epoch
acc:[0.7250]
acc:[0.7250]
acc:[0.7240]
acc:[0.7230]
acc:[0.7250]
acc:[0.7270]
acc:[0.7260]
acc:[0.7260]
acc:[0.7280]
acc:[0.7280]
acc:[0.7260]
acc:[0.7250]
acc:[0.7230]
acc:[0.7260]
acc:[0.7220]
acc:[0.7240]
acc:[0.7250]
acc:[0.7270]
acc:[0.7250]
acc:[0.7260]
acc:[0.7250]
acc:[0.7240]
acc:[0.7230]
acc:[0.7260]
acc:[0.7250]
acc:[0.7240]
acc:[0.7250]
acc:[0.7220]
acc:[0.7220]
acc:[0.7250]
acc:[0.7250]
acc:[0.7250]
acc:[0.7270]
acc:[0.7240]
acc:[0.7250]
acc:[0.7250]
acc:[0.7250]
acc:[0.7260]
acc:[0.7230]
acc:[0.7250]
acc:[0.7250]
acc:[0.7240]
acc:[0.7250]
acc:[0.7240]
acc:[0.7260]
acc:[0.7230]
acc:[0.7220]
acc:[0.7250]
acc:[0.7280]
acc:[0.7250]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7249]
Mean:[72.4880]
Std :[0.1493]
----------------------------------------------------------------------------------------------------
