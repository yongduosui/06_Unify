----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.2, gpu=4, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6941]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6794]
Loss:[0.6811]
Loss:[0.6694]
Loss:[0.6649]
Loss:[0.6573]
Loss:[0.6438]
Loss:[0.6386]
Loss:[0.6211]
Loss:[0.6134]
Loss:[0.5933]
Loss:[0.5839]
Loss:[0.5651]
Loss:[0.5454]
Loss:[0.5348]
Loss:[0.5093]
Loss:[0.4965]
Loss:[0.4813]
Loss:[0.4547]
Loss:[0.4390]
Loss:[0.4280]
Loss:[0.4101]
Loss:[0.3837]
Loss:[0.3709]
Loss:[0.3627]
Loss:[0.3433]
Loss:[0.3211]
Loss:[0.3128]
Loss:[0.3017]
Loss:[0.2856]
Loss:[0.2772]
Loss:[0.2619]
Loss:[0.2532]
Loss:[0.2447]
Loss:[0.2384]
Loss:[0.2308]
Loss:[0.2221]
Loss:[0.2155]
Loss:[0.2088]
Loss:[0.2098]
Loss:[0.1946]
Loss:[0.1887]
Loss:[0.1814]
Loss:[0.1812]
Loss:[0.1746]
Loss:[0.1721]
Loss:[0.1631]
Loss:[0.1626]
Loss:[0.1565]
Loss:[0.1622]
Loss:[0.1523]
Loss:[0.1518]
Loss:[0.1465]
Loss:[0.1384]
Loss:[0.1457]
Loss:[0.1377]
Loss:[0.1356]
Loss:[0.1310]
Loss:[0.1367]
Loss:[0.1346]
Loss:[0.1337]
Loss:[0.1258]
Loss:[0.1312]
Loss:[0.1240]
Loss:[0.1279]
Loss:[0.1270]
Loss:[0.1247]
Loss:[0.1309]
Loss:[0.1294]
Loss:[0.1199]
Loss:[0.1255]
Loss:[0.1251]
Loss:[0.1288]
Loss:[0.1213]
Loss:[0.1254]
Loss:[0.1196]
Loss:[0.1168]
Loss:[0.1376]
Loss:[0.1443]
Loss:[0.1123]
Loss:[0.1495]
Loss:[0.1313]
Loss:[0.1321]
Loss:[0.1393]
Loss:[0.1072]
Loss:[0.1164]
Loss:[0.1034]
Loss:[0.1218]
Loss:[0.1117]
Loss:[0.1188]
Loss:[0.1046]
Loss:[0.1144]
Loss:[0.1071]
Loss:[0.1055]
Loss:[0.1133]
Loss:[0.1107]
Loss:[0.1081]
Loss:[0.1003]
Loss:[0.1000]
Loss:[0.1031]
Loss:[0.1017]
Loss:[0.0951]
Loss:[0.1041]
Loss:[0.1059]
Loss:[0.1009]
Loss:[0.1066]
Loss:[0.0951]
Loss:[0.0959]
Loss:[0.0972]
Loss:[0.1038]
Loss:[0.0961]
Loss:[0.0920]
Loss:[0.0998]
Loss:[0.0984]
Loss:[0.0957]
Loss:[0.1019]
Loss:[0.1027]
Loss:[0.1024]
Loss:[0.1003]
Loss:[0.0941]
Loss:[0.0982]
Loss:[0.0982]
Loss:[0.1063]
Loss:[0.0878]
Loss:[0.0987]
Loss:[0.0969]
Loss:[0.0987]
Loss:[0.0926]
Loss:[0.1021]
Loss:[0.0923]
Loss:[0.0931]
Loss:[0.0952]
Loss:[0.0912]
Loss:[0.0909]
Loss:[0.0993]
Loss:[0.0896]
Loss:[0.0968]
Loss:[0.0961]
Loss:[0.0976]
Loss:[0.0924]
Loss:[0.0902]
Loss:[0.0915]
Loss:[0.0945]
Loss:[0.0868]
Loss:[0.0854]
Loss:[0.0950]
Loss:[0.0915]
Loss:[0.0940]
Loss:[0.0901]
Loss:[0.0950]
Loss:[0.0871]
Loss:[0.0911]
Loss:[0.0913]
Loss:[0.0897]
Loss:[0.0935]
Loss:[0.0878]
Loss:[0.0934]
Loss:[0.0917]
Loss:[0.0820]
Loss:[0.0878]
Loss:[0.0848]
Loss:[0.0888]
Loss:[0.0859]
Loss:[0.0935]
Loss:[0.0852]
Loss:[0.0945]
Loss:[0.0887]
Loss:[0.0923]
Loss:[0.0952]
Loss:[0.0847]
Loss:[0.0890]
Loss:[0.0859]
Loss:[0.0856]
Loss:[0.0946]
Loss:[0.0871]
Loss:[0.0917]
Loss:[0.0846]
Loss:[0.0827]
Loss:[0.0831]
Early stopping!
Loading 161th epoch
acc:[0.7280]
acc:[0.7280]
acc:[0.7270]
acc:[0.7270]
acc:[0.7270]
acc:[0.7280]
acc:[0.7250]
acc:[0.7280]
acc:[0.7280]
acc:[0.7280]
acc:[0.7290]
acc:[0.7280]
acc:[0.7270]
acc:[0.7290]
acc:[0.7260]
acc:[0.7270]
acc:[0.7250]
acc:[0.7270]
acc:[0.7240]
acc:[0.7290]
acc:[0.7290]
acc:[0.7280]
acc:[0.7260]
acc:[0.7280]
acc:[0.7270]
acc:[0.7260]
acc:[0.7260]
acc:[0.7270]
acc:[0.7270]
acc:[0.7260]
acc:[0.7260]
acc:[0.7270]
acc:[0.7280]
acc:[0.7270]
acc:[0.7260]
acc:[0.7260]
acc:[0.7260]
acc:[0.7270]
acc:[0.7250]
acc:[0.7260]
acc:[0.7300]
acc:[0.7270]
acc:[0.7300]
acc:[0.7260]
acc:[0.7250]
acc:[0.7270]
acc:[0.7260]
acc:[0.7270]
acc:[0.7270]
acc:[0.7290]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7271]
Mean:[72.7060]
Std :[0.1316]
----------------------------------------------------------------------------------------------------
