----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='cora', drop_percent=0.3, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6927]
Loss:[0.6881]
Loss:[0.6871]
Loss:[0.6832]
Loss:[0.6783]
Loss:[0.6748]
Loss:[0.6685]
Loss:[0.6641]
Loss:[0.6565]
Loss:[0.6490]
Loss:[0.6403]
Loss:[0.6320]
Loss:[0.6230]
Loss:[0.6118]
Loss:[0.6027]
Loss:[0.5911]
Loss:[0.5780]
Loss:[0.5675]
Loss:[0.5555]
Loss:[0.5372]
Loss:[0.5331]
Loss:[0.5140]
Loss:[0.5036]
Loss:[0.4947]
Loss:[0.4801]
Loss:[0.4673]
Loss:[0.4567]
Loss:[0.4456]
Loss:[0.4276]
Loss:[0.4164]
Loss:[0.4118]
Loss:[0.3954]
Loss:[0.3796]
Loss:[0.3698]
Loss:[0.3667]
Loss:[0.3598]
Loss:[0.3482]
Loss:[0.3329]
Loss:[0.3271]
Loss:[0.3264]
Loss:[0.3131]
Loss:[0.3080]
Loss:[0.3018]
Loss:[0.2984]
Loss:[0.2860]
Loss:[0.2707]
Loss:[0.2777]
Loss:[0.2696]
Loss:[0.2690]
Loss:[0.2562]
Loss:[0.2583]
Loss:[0.2511]
Loss:[0.2489]
Loss:[0.2395]
Loss:[0.2407]
Loss:[0.2333]
Loss:[0.2200]
Loss:[0.2253]
Loss:[0.2057]
Loss:[0.2103]
Loss:[0.2312]
Loss:[0.2149]
Loss:[0.2031]
Loss:[0.2020]
Loss:[0.1920]
Loss:[0.1974]
Loss:[0.1887]
Loss:[0.1851]
Loss:[0.1937]
Loss:[0.1886]
Loss:[0.1895]
Loss:[0.1709]
Loss:[0.1808]
Loss:[0.1799]
Loss:[0.1778]
Loss:[0.1728]
Loss:[0.1758]
Loss:[0.1712]
Loss:[0.1657]
Loss:[0.1795]
Loss:[0.1642]
Loss:[0.1651]
Loss:[0.1608]
Loss:[0.1643]
Loss:[0.1605]
Loss:[0.1550]
Loss:[0.1502]
Loss:[0.1599]
Loss:[0.1588]
Loss:[0.1457]
Loss:[0.1401]
Loss:[0.1459]
Loss:[0.1396]
Loss:[0.1452]
Loss:[0.1357]
Loss:[0.1488]
Loss:[0.1368]
Loss:[0.1355]
Loss:[0.1287]
Loss:[0.1468]
Loss:[0.1396]
Loss:[0.1473]
Loss:[0.1319]
Loss:[0.1336]
Loss:[0.1513]
Loss:[0.1261]
Loss:[0.1472]
Loss:[0.1332]
Loss:[0.1426]
Loss:[0.1624]
Loss:[0.1192]
Loss:[0.1483]
Loss:[0.1169]
Loss:[0.1324]
Loss:[0.1251]
Loss:[0.1248]
Loss:[0.1117]
Loss:[0.1200]
Loss:[0.1234]
Loss:[0.1235]
Loss:[0.1119]
Loss:[0.1053]
Loss:[0.1172]
Loss:[0.1102]
Loss:[0.1039]
Loss:[0.1094]
Loss:[0.1119]
Loss:[0.1104]
Loss:[0.1068]
Loss:[0.1045]
Loss:[0.1024]
Loss:[0.1019]
Loss:[0.1171]
Loss:[0.0992]
Loss:[0.1007]
Loss:[0.1011]
Loss:[0.0981]
Loss:[0.0989]
Loss:[0.0978]
Loss:[0.1055]
Loss:[0.0889]
Loss:[0.0956]
Loss:[0.1014]
Loss:[0.0867]
Loss:[0.0935]
Loss:[0.0927]
Loss:[0.0898]
Loss:[0.0994]
Loss:[0.0902]
Loss:[0.0896]
Loss:[0.0971]
Loss:[0.0867]
Loss:[0.0898]
Loss:[0.0888]
Loss:[0.0909]
Loss:[0.0904]
Loss:[0.0925]
Loss:[0.0862]
Loss:[0.0830]
Loss:[0.0876]
Loss:[0.0783]
Loss:[0.0886]
Loss:[0.0808]
Loss:[0.0958]
Loss:[0.0850]
Loss:[0.0897]
Loss:[0.0896]
Loss:[0.0768]
Loss:[0.0817]
Loss:[0.0903]
Loss:[0.0795]
Loss:[0.0787]
Loss:[0.0805]
Loss:[0.0729]
Loss:[0.0845]
Loss:[0.0922]
Loss:[0.0844]
Loss:[0.0812]
Loss:[0.0745]
Loss:[0.0734]
Loss:[0.0679]
Loss:[0.0801]
Loss:[0.0658]
Loss:[0.0702]
Loss:[0.0652]
Loss:[0.0696]
Loss:[0.0704]
Loss:[0.0690]
Loss:[0.0742]
Loss:[0.0797]
Loss:[0.0815]
Loss:[0.0774]
Loss:[0.0728]
Loss:[0.0706]
Loss:[0.0662]
Loss:[0.0594]
Loss:[0.0781]
Loss:[0.0693]
Loss:[0.0744]
Loss:[0.0669]
Loss:[0.0614]
Loss:[0.0661]
Loss:[0.0733]
Loss:[0.0756]
Loss:[0.0663]
Loss:[0.0615]
Loss:[0.0568]
Loss:[0.0660]
Loss:[0.0658]
Loss:[0.0661]
Loss:[0.0677]
Loss:[0.0656]
Loss:[0.0681]
Loss:[0.0573]
Loss:[0.0627]
Loss:[0.0625]
Loss:[0.0671]
Loss:[0.0571]
Loss:[0.0753]
Loss:[0.0559]
Loss:[0.0642]
Loss:[0.0577]
Loss:[0.0627]
Loss:[0.0505]
Loss:[0.0607]
Loss:[0.0518]
Loss:[0.0530]
Loss:[0.0534]
Loss:[0.0691]
Loss:[0.0628]
Loss:[0.0581]
Loss:[0.0636]
Loss:[0.0588]
Loss:[0.0605]
Loss:[0.0592]
Loss:[0.0531]
Loss:[0.0667]
Loss:[0.0546]
Loss:[0.0511]
Loss:[0.0579]
Loss:[0.0504]
Loss:[0.0506]
Loss:[0.0521]
Loss:[0.0552]
Loss:[0.0588]
Loss:[0.0560]
Loss:[0.0613]
Loss:[0.0504]
Loss:[0.0491]
Loss:[0.0503]
Loss:[0.0549]
Loss:[0.0484]
Loss:[0.0502]
Loss:[0.0546]
Loss:[0.0573]
Loss:[0.0550]
Loss:[0.0529]
Loss:[0.0469]
Loss:[0.0468]
Loss:[0.0512]
Loss:[0.0526]
Loss:[0.0533]
Loss:[0.0496]
Loss:[0.0458]
Loss:[0.0544]
Loss:[0.0461]
Loss:[0.0478]
Loss:[0.0511]
Loss:[0.0567]
Loss:[0.0441]
Loss:[0.0511]
Loss:[0.0522]
Loss:[0.0456]
Loss:[0.0430]
Loss:[0.0593]
Loss:[0.0488]
Loss:[0.0522]
Loss:[0.0495]
Loss:[0.0461]
Loss:[0.0469]
Loss:[0.0467]
Loss:[0.0503]
Loss:[0.0451]
Loss:[0.0468]
Loss:[0.0493]
Loss:[0.0431]
Loss:[0.0463]
Loss:[0.0485]
Loss:[0.0449]
Loss:[0.0500]
Loss:[0.0410]
Loss:[0.0485]
Loss:[0.0457]
Loss:[0.0407]
Loss:[0.0418]
Loss:[0.0428]
Loss:[0.0413]
Loss:[0.0451]
Loss:[0.0402]
Loss:[0.0384]
Loss:[0.0389]
Loss:[0.0404]
Loss:[0.0444]
Loss:[0.0374]
Loss:[0.0476]
Loss:[0.0399]
Loss:[0.0439]
Loss:[0.0376]
Loss:[0.0387]
Loss:[0.0418]
Loss:[0.0393]
Loss:[0.0440]
Loss:[0.0366]
Loss:[0.0405]
Loss:[0.0404]
Loss:[0.0434]
Loss:[0.0391]
Loss:[0.0428]
Loss:[0.0375]
Loss:[0.0391]
Loss:[0.0339]
Loss:[0.0349]
Loss:[0.0385]
Loss:[0.0394]
Loss:[0.0353]
Loss:[0.0354]
Loss:[0.0419]
Loss:[0.0448]
Loss:[0.0383]
Loss:[0.0319]
Loss:[0.0351]
Loss:[0.0361]
Loss:[0.0298]
Loss:[0.0388]
Loss:[0.0378]
Loss:[0.0373]
Loss:[0.0356]
Loss:[0.0393]
Loss:[0.0338]
Loss:[0.0394]
Loss:[0.0426]
Loss:[0.0380]
Loss:[0.0350]
Loss:[0.0325]
Loss:[0.0336]
Loss:[0.0374]
Loss:[0.0338]
Loss:[0.0290]
Loss:[0.0362]
Loss:[0.0288]
Loss:[0.0346]
Loss:[0.0323]
Loss:[0.0306]
Loss:[0.0309]
Loss:[0.0357]
Loss:[0.0299]
Loss:[0.0321]
Loss:[0.0297]
Loss:[0.0267]
Loss:[0.0341]
Loss:[0.0288]
Loss:[0.0282]
Loss:[0.0309]
Loss:[0.0293]
Loss:[0.0302]
Loss:[0.0315]
Loss:[0.0317]
Loss:[0.0283]
Loss:[0.0293]
Loss:[0.0304]
Loss:[0.0250]
Loss:[0.0326]
Loss:[0.0261]
Loss:[0.0259]
Loss:[0.0280]
Loss:[0.0268]
Loss:[0.0306]
Loss:[0.0356]
Loss:[0.0258]
Loss:[0.0272]
Loss:[0.0348]
Loss:[0.0311]
Loss:[0.0350]
Loss:[0.0265]
Loss:[0.0301]
Loss:[0.0344]
Loss:[0.0276]
Loss:[0.0303]
Loss:[0.0271]
Loss:[0.0301]
Loss:[0.0256]
Early stopping!
Loading 371th epoch
acc:[0.8180]
acc:[0.8180]
acc:[0.8180]
acc:[0.8180]
acc:[0.8190]
acc:[0.8180]
acc:[0.8150]
acc:[0.8170]
acc:[0.8170]
acc:[0.8170]
acc:[0.8180]
acc:[0.8170]
acc:[0.8170]
acc:[0.8160]
acc:[0.8190]
acc:[0.8180]
acc:[0.8190]
acc:[0.8180]
acc:[0.8180]
acc:[0.8170]
acc:[0.8180]
acc:[0.8190]
acc:[0.8200]
acc:[0.8190]
acc:[0.8170]
acc:[0.8190]
acc:[0.8160]
acc:[0.8190]
acc:[0.8190]
acc:[0.8190]
acc:[0.8160]
acc:[0.8180]
acc:[0.8190]
acc:[0.8160]
acc:[0.8180]
acc:[0.8170]
acc:[0.8200]
acc:[0.8180]
acc:[0.8170]
acc:[0.8190]
acc:[0.8170]
acc:[0.8170]
acc:[0.8170]
acc:[0.8190]
acc:[0.8170]
acc:[0.8180]
acc:[0.8170]
acc:[0.8160]
acc:[0.8180]
acc:[0.8160]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8177]
Mean:[81.7740]
Std :[0.1139]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='cora', drop_percent=0.4, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6928]
Loss:[0.6883]
Loss:[0.6876]
Loss:[0.6838]
Loss:[0.6793]
Loss:[0.6759]
Loss:[0.6700]
Loss:[0.6656]
Loss:[0.6587]
Loss:[0.6512]
Loss:[0.6433]
Loss:[0.6350]
Loss:[0.6269]
Loss:[0.6156]
Loss:[0.6071]
Loss:[0.5954]
Loss:[0.5834]
Loss:[0.5729]
Loss:[0.5614]
Loss:[0.5449]
Loss:[0.5408]
Loss:[0.5211]
Loss:[0.5126]
Loss:[0.5027]
Loss:[0.4887]
Loss:[0.4762]
Loss:[0.4660]
Loss:[0.4551]
Loss:[0.4371]
Loss:[0.4263]
Loss:[0.4212]
Loss:[0.4071]
Loss:[0.3930]
Loss:[0.3800]
Loss:[0.3785]
Loss:[0.3716]
Loss:[0.3604]
Loss:[0.3448]
Loss:[0.3395]
Loss:[0.3408]
Loss:[0.3247]
Loss:[0.3202]
Loss:[0.3168]
Loss:[0.3133]
Loss:[0.2992]
Loss:[0.2844]
Loss:[0.2879]
Loss:[0.2830]
Loss:[0.2780]
Loss:[0.2697]
Loss:[0.2713]
Loss:[0.2673]
Loss:[0.2606]
Loss:[0.2553]
Loss:[0.2540]
Loss:[0.2466]
Loss:[0.2345]
Loss:[0.2403]
Loss:[0.2205]
Loss:[0.2268]
Loss:[0.2513]
Loss:[0.2300]
Loss:[0.2157]
Loss:[0.2181]
Loss:[0.2081]
Loss:[0.2104]
Loss:[0.2025]
Loss:[0.1970]
Loss:[0.2060]
Loss:[0.2000]
Loss:[0.2015]
Loss:[0.1839]
Loss:[0.1903]
Loss:[0.1889]
Loss:[0.1843]
Loss:[0.1856]
Loss:[0.1877]
Loss:[0.1821]
Loss:[0.1737]
Loss:[0.1865]
Loss:[0.1711]
Loss:[0.1749]
Loss:[0.1724]
Loss:[0.1756]
Loss:[0.1677]
Loss:[0.1660]
Loss:[0.1610]
Loss:[0.1635]
Loss:[0.1616]
Loss:[0.1549]
Loss:[0.1505]
Loss:[0.1543]
Loss:[0.1504]
Loss:[0.1568]
Loss:[0.1461]
Loss:[0.1569]
Loss:[0.1458]
Loss:[0.1557]
Loss:[0.1442]
Loss:[0.1550]
Loss:[0.1423]
Loss:[0.1475]
Loss:[0.1427]
Loss:[0.1337]
Loss:[0.1481]
Loss:[0.1412]
Loss:[0.1303]
Loss:[0.1269]
Loss:[0.1353]
Loss:[0.1433]
Loss:[0.1276]
Loss:[0.1371]
Loss:[0.1243]
Loss:[0.1227]
Loss:[0.1264]
Loss:[0.1232]
Loss:[0.1194]
Loss:[0.1244]
Loss:[0.1251]
Loss:[0.1250]
Loss:[0.1189]
Loss:[0.1132]
Loss:[0.1248]
Loss:[0.1151]
Loss:[0.1120]
Loss:[0.1125]
Loss:[0.1183]
Loss:[0.1218]
Loss:[0.1160]
Loss:[0.1157]
Loss:[0.1164]
Loss:[0.1050]
Loss:[0.1308]
Loss:[0.1102]
Loss:[0.1084]
Loss:[0.1233]
Loss:[0.1073]
Loss:[0.1137]
Loss:[0.1205]
Loss:[0.1118]
Loss:[0.0997]
Loss:[0.0992]
Loss:[0.1045]
Loss:[0.0945]
Loss:[0.1008]
Loss:[0.1008]
Loss:[0.0931]
Loss:[0.1051]
Loss:[0.0947]
Loss:[0.0971]
Loss:[0.1000]
Loss:[0.0888]
Loss:[0.0947]
Loss:[0.0932]
Loss:[0.0942]
Loss:[0.0936]
Loss:[0.0970]
Loss:[0.0907]
Loss:[0.0877]
Loss:[0.0916]
Loss:[0.0840]
Loss:[0.0935]
Loss:[0.0821]
Loss:[0.0986]
Loss:[0.0876]
Loss:[0.0887]
Loss:[0.0910]
Loss:[0.0802]
Loss:[0.0875]
Loss:[0.0939]
Loss:[0.0843]
Loss:[0.0818]
Loss:[0.0840]
Loss:[0.0772]
Loss:[0.0894]
Loss:[0.0926]
Loss:[0.0838]
Loss:[0.0829]
Loss:[0.0799]
Loss:[0.0753]
Loss:[0.0715]
Loss:[0.0802]
Loss:[0.0695]
Loss:[0.0728]
Loss:[0.0682]
Loss:[0.0717]
Loss:[0.0741]
Loss:[0.0713]
Loss:[0.0729]
Loss:[0.0796]
Loss:[0.0802]
Loss:[0.0805]
Loss:[0.0775]
Loss:[0.0754]
Loss:[0.0689]
Loss:[0.0667]
Loss:[0.0763]
Loss:[0.0707]
Loss:[0.0695]
Loss:[0.0662]
Loss:[0.0594]
Loss:[0.0663]
Loss:[0.0728]
Loss:[0.0722]
Loss:[0.0692]
Loss:[0.0610]
Loss:[0.0577]
Loss:[0.0686]
Loss:[0.0700]
Loss:[0.0664]
Loss:[0.0741]
Loss:[0.0647]
Loss:[0.0730]
Loss:[0.0574]
Loss:[0.0641]
Loss:[0.0663]
Loss:[0.0694]
Loss:[0.0592]
Loss:[0.0747]
Loss:[0.0592]
Loss:[0.0656]
Loss:[0.0579]
Loss:[0.0621]
Loss:[0.0513]
Loss:[0.0617]
Loss:[0.0568]
Loss:[0.0525]
Loss:[0.0535]
Loss:[0.0664]
Loss:[0.0640]
Loss:[0.0606]
Loss:[0.0655]
Loss:[0.0591]
Loss:[0.0626]
Loss:[0.0609]
Loss:[0.0534]
Loss:[0.0679]
Loss:[0.0570]
Loss:[0.0509]
Loss:[0.0610]
Loss:[0.0537]
Loss:[0.0552]
Loss:[0.0539]
Loss:[0.0550]
Loss:[0.0604]
Loss:[0.0566]
Loss:[0.0623]
Loss:[0.0533]
Loss:[0.0472]
Loss:[0.0560]
Loss:[0.0549]
Loss:[0.0512]
Loss:[0.0518]
Loss:[0.0565]
Loss:[0.0605]
Loss:[0.0535]
Loss:[0.0562]
Loss:[0.0456]
Loss:[0.0473]
Loss:[0.0492]
Loss:[0.0508]
Loss:[0.0547]
Loss:[0.0495]
Loss:[0.0498]
Loss:[0.0537]
Loss:[0.0493]
Loss:[0.0497]
Loss:[0.0548]
Loss:[0.0558]
Loss:[0.0460]
Loss:[0.0498]
Loss:[0.0529]
Loss:[0.0476]
Loss:[0.0479]
Loss:[0.0555]
Loss:[0.0515]
Loss:[0.0550]
Loss:[0.0547]
Early stopping!
Loading 258th epoch
acc:[0.8120]
acc:[0.8100]
acc:[0.8090]
acc:[0.8140]
acc:[0.8100]
acc:[0.8090]
acc:[0.8060]
acc:[0.8140]
acc:[0.8100]
acc:[0.8100]
acc:[0.8090]
acc:[0.8080]
acc:[0.8080]
acc:[0.8080]
acc:[0.8080]
acc:[0.8130]
acc:[0.8090]
acc:[0.8080]
acc:[0.8070]
acc:[0.8120]
acc:[0.8090]
acc:[0.8100]
acc:[0.8110]
acc:[0.8110]
acc:[0.8120]
acc:[0.8070]
acc:[0.8090]
acc:[0.8100]
acc:[0.8100]
acc:[0.8110]
acc:[0.8110]
acc:[0.8100]
acc:[0.8080]
acc:[0.8100]
acc:[0.8140]
acc:[0.8110]
acc:[0.8090]
acc:[0.8120]
acc:[0.8110]
acc:[0.8100]
acc:[0.8130]
acc:[0.8070]
acc:[0.8110]
acc:[0.8080]
acc:[0.8120]
acc:[0.8130]
acc:[0.8080]
acc:[0.8110]
acc:[0.8090]
acc:[0.8140]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8101]
Mean:[81.0120]
Std :[0.2027]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='cora', drop_percent=0.5, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6886]
Loss:[0.6880]
Loss:[0.6845]
Loss:[0.6802]
Loss:[0.6769]
Loss:[0.6713]
Loss:[0.6672]
Loss:[0.6605]
Loss:[0.6534]
Loss:[0.6457]
Loss:[0.6379]
Loss:[0.6298]
Loss:[0.6195]
Loss:[0.6106]
Loss:[0.6001]
Loss:[0.5881]
Loss:[0.5773]
Loss:[0.5673]
Loss:[0.5501]
Loss:[0.5462]
Loss:[0.5295]
Loss:[0.5192]
Loss:[0.5081]
Loss:[0.4977]
Loss:[0.4846]
Loss:[0.4730]
Loss:[0.4634]
Loss:[0.4483]
Loss:[0.4361]
Loss:[0.4291]
Loss:[0.4161]
Loss:[0.4057]
Loss:[0.3911]
Loss:[0.3870]
Loss:[0.3774]
Loss:[0.3710]
Loss:[0.3621]
Loss:[0.3518]
Loss:[0.3467]
Loss:[0.3385]
Loss:[0.3350]
Loss:[0.3253]
Loss:[0.3255]
Loss:[0.3150]
Loss:[0.2956]
Loss:[0.3001]
Loss:[0.2984]
Loss:[0.2899]
Loss:[0.2795]
Loss:[0.2817]
Loss:[0.2808]
Loss:[0.2716]
Loss:[0.2650]
Loss:[0.2660]
Loss:[0.2621]
Loss:[0.2487]
Loss:[0.2513]
Loss:[0.2288]
Loss:[0.2347]
Loss:[0.2547]
Loss:[0.2441]
Loss:[0.2258]
Loss:[0.2300]
Loss:[0.2173]
Loss:[0.2223]
Loss:[0.2188]
Loss:[0.2118]
Loss:[0.2184]
Loss:[0.2112]
Loss:[0.2175]
Loss:[0.2029]
Loss:[0.1977]
Loss:[0.1994]
Loss:[0.2036]
Loss:[0.2100]
Loss:[0.1985]
Loss:[0.2046]
Loss:[0.1986]
Loss:[0.1991]
Loss:[0.2080]
Loss:[0.1962]
Loss:[0.1889]
Loss:[0.2019]
Loss:[0.1762]
Loss:[0.1933]
Loss:[0.1718]
Loss:[0.1860]
Loss:[0.1709]
Loss:[0.1693]
Loss:[0.1638]
Loss:[0.1694]
Loss:[0.1671]
Loss:[0.1667]
Loss:[0.1610]
Loss:[0.1664]
Loss:[0.1581]
Loss:[0.1612]
Loss:[0.1531]
Loss:[0.1668]
Loss:[0.1543]
Loss:[0.1602]
Loss:[0.1526]
Loss:[0.1461]
Loss:[0.1510]
Loss:[0.1494]
Loss:[0.1419]
Loss:[0.1394]
Loss:[0.1474]
Loss:[0.1553]
Loss:[0.1347]
Loss:[0.1545]
Loss:[0.1391]
Loss:[0.1352]
Loss:[0.1449]
Loss:[0.1371]
Loss:[0.1329]
Loss:[0.1339]
Loss:[0.1389]
Loss:[0.1396]
Loss:[0.1303]
Loss:[0.1223]
Loss:[0.1372]
Loss:[0.1251]
Loss:[0.1229]
Loss:[0.1263]
Loss:[0.1280]
Loss:[0.1259]
Loss:[0.1291]
Loss:[0.1214]
Loss:[0.1183]
Loss:[0.1187]
Loss:[0.1313]
Loss:[0.1161]
Loss:[0.1212]
Loss:[0.1179]
Loss:[0.1144]
Loss:[0.1145]
Loss:[0.1110]
Loss:[0.1194]
Loss:[0.1081]
Loss:[0.1067]
Loss:[0.1140]
Loss:[0.1027]
Loss:[0.1104]
Loss:[0.1105]
Loss:[0.1045]
Loss:[0.1125]
Loss:[0.1036]
Loss:[0.1044]
Loss:[0.1074]
Loss:[0.1022]
Loss:[0.1059]
Loss:[0.1004]
Loss:[0.1040]
Loss:[0.1072]
Loss:[0.1105]
Loss:[0.1030]
Loss:[0.0978]
Loss:[0.1031]
Loss:[0.0905]
Loss:[0.1109]
Loss:[0.0943]
Loss:[0.1067]
Loss:[0.0939]
Loss:[0.0938]
Loss:[0.1022]
Loss:[0.0860]
Loss:[0.0984]
Loss:[0.1001]
Loss:[0.0888]
Loss:[0.0933]
Loss:[0.0904]
Loss:[0.0913]
Loss:[0.1019]
Loss:[0.1008]
Loss:[0.0966]
Loss:[0.0880]
Loss:[0.0926]
Loss:[0.0892]
Loss:[0.0798]
Loss:[0.0906]
Loss:[0.0775]
Loss:[0.0836]
Loss:[0.0758]
Loss:[0.0803]
Loss:[0.0834]
Loss:[0.0801]
Loss:[0.0848]
Loss:[0.0866]
Loss:[0.0895]
Loss:[0.0904]
Loss:[0.0824]
Loss:[0.0858]
Loss:[0.0783]
Loss:[0.0790]
Loss:[0.0813]
Loss:[0.0826]
Loss:[0.0772]
Loss:[0.0775]
Loss:[0.0714]
Loss:[0.0756]
Loss:[0.0782]
Loss:[0.0756]
Loss:[0.0779]
Loss:[0.0686]
Loss:[0.0648]
Loss:[0.0757]
Loss:[0.0772]
Loss:[0.0738]
Loss:[0.0808]
Loss:[0.0729]
Loss:[0.0795]
Loss:[0.0651]
Loss:[0.0719]
Loss:[0.0721]
Loss:[0.0735]
Loss:[0.0666]
Loss:[0.0766]
Loss:[0.0643]
Loss:[0.0711]
Loss:[0.0642]
Loss:[0.0684]
Loss:[0.0567]
Loss:[0.0666]
Loss:[0.0619]
Loss:[0.0582]
Loss:[0.0586]
Loss:[0.0737]
Loss:[0.0701]
Loss:[0.0643]
Loss:[0.0749]
Loss:[0.0657]
Loss:[0.0682]
Loss:[0.0692]
Loss:[0.0591]
Loss:[0.0705]
Loss:[0.0609]
Loss:[0.0569]
Loss:[0.0638]
Loss:[0.0595]
Loss:[0.0631]
Loss:[0.0586]
Loss:[0.0614]
Early stopping!
Loading 224th epoch
acc:[0.8140]
acc:[0.8130]
acc:[0.8130]
acc:[0.8150]
acc:[0.8170]
acc:[0.8150]
acc:[0.8140]
acc:[0.8150]
acc:[0.8130]
acc:[0.8140]
acc:[0.8150]
acc:[0.8140]
acc:[0.8180]
acc:[0.8150]
acc:[0.8130]
acc:[0.8150]
acc:[0.8140]
acc:[0.8150]
acc:[0.8130]
acc:[0.8130]
acc:[0.8140]
acc:[0.8130]
acc:[0.8130]
acc:[0.8150]
acc:[0.8150]
acc:[0.8110]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8140]
acc:[0.8140]
acc:[0.8120]
acc:[0.8140]
acc:[0.8120]
acc:[0.8150]
acc:[0.8110]
acc:[0.8140]
acc:[0.8140]
acc:[0.8170]
acc:[0.8140]
acc:[0.8130]
acc:[0.8140]
acc:[0.8120]
acc:[0.8130]
acc:[0.8160]
acc:[0.8150]
acc:[0.8160]
acc:[0.8160]
acc:[0.8160]
acc:[0.8140]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8142]
Mean:[81.4160]
Std :[0.1434]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='cora', drop_percent=0.6, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6931]
Loss:[0.6890]
Loss:[0.6886]
Loss:[0.6853]
Loss:[0.6812]
Loss:[0.6784]
Loss:[0.6731]
Loss:[0.6692]
Loss:[0.6631]
Loss:[0.6562]
Loss:[0.6493]
Loss:[0.6416]
Loss:[0.6345]
Loss:[0.6243]
Loss:[0.6162]
Loss:[0.6055]
Loss:[0.5947]
Loss:[0.5849]
Loss:[0.5745]
Loss:[0.5588]
Loss:[0.5553]
Loss:[0.5375]
Loss:[0.5282]
Loss:[0.5179]
Loss:[0.5073]
Loss:[0.4945]
Loss:[0.4841]
Loss:[0.4748]
Loss:[0.4611]
Loss:[0.4470]
Loss:[0.4417]
Loss:[0.4280]
Loss:[0.4170]
Loss:[0.4025]
Loss:[0.4001]
Loss:[0.3926]
Loss:[0.3848]
Loss:[0.3729]
Loss:[0.3643]
Loss:[0.3626]
Loss:[0.3512]
Loss:[0.3458]
Loss:[0.3396]
Loss:[0.3368]
Loss:[0.3247]
Loss:[0.3061]
Loss:[0.3125]
Loss:[0.3068]
Loss:[0.3019]
Loss:[0.2929]
Loss:[0.2901]
Loss:[0.2875]
Loss:[0.2826]
Loss:[0.2763]
Loss:[0.2755]
Loss:[0.2687]
Loss:[0.2610]
Loss:[0.2584]
Loss:[0.2420]
Loss:[0.2435]
Loss:[0.2662]
Loss:[0.2531]
Loss:[0.2348]
Loss:[0.2406]
Loss:[0.2284]
Loss:[0.2351]
Loss:[0.2293]
Loss:[0.2185]
Loss:[0.2297]
Loss:[0.2250]
Loss:[0.2295]
Loss:[0.2083]
Loss:[0.2026]
Loss:[0.2164]
Loss:[0.2098]
Loss:[0.2065]
Loss:[0.2104]
Loss:[0.2056]
Loss:[0.1945]
Loss:[0.2098]
Loss:[0.2008]
Loss:[0.1998]
Loss:[0.1921]
Loss:[0.2008]
Loss:[0.1844]
Loss:[0.1880]
Loss:[0.1850]
Loss:[0.1871]
Loss:[0.1779]
Loss:[0.1717]
Loss:[0.1697]
Loss:[0.1780]
Loss:[0.1700]
Loss:[0.1699]
Loss:[0.1612]
Loss:[0.1711]
Loss:[0.1678]
Loss:[0.1676]
Loss:[0.1569]
Loss:[0.1668]
Loss:[0.1603]
Loss:[0.1683]
Loss:[0.1573]
Loss:[0.1487]
Loss:[0.1602]
Loss:[0.1529]
Loss:[0.1498]
Loss:[0.1440]
Loss:[0.1529]
Loss:[0.1667]
Loss:[0.1452]
Loss:[0.1584]
Loss:[0.1522]
Loss:[0.1380]
Loss:[0.1560]
Loss:[0.1487]
Loss:[0.1333]
Loss:[0.1427]
Loss:[0.1409]
Loss:[0.1522]
Loss:[0.1337]
Loss:[0.1266]
Loss:[0.1420]
Loss:[0.1283]
Loss:[0.1279]
Loss:[0.1288]
Loss:[0.1357]
Loss:[0.1261]
Loss:[0.1346]
Loss:[0.1292]
Loss:[0.1198]
Loss:[0.1252]
Loss:[0.1357]
Loss:[0.1228]
Loss:[0.1277]
Loss:[0.1222]
Loss:[0.1229]
Loss:[0.1221]
Loss:[0.1149]
Loss:[0.1217]
Loss:[0.1117]
Loss:[0.1123]
Loss:[0.1172]
Loss:[0.1100]
Loss:[0.1093]
Loss:[0.1166]
Loss:[0.1128]
Loss:[0.1163]
Loss:[0.1064]
Loss:[0.1099]
Loss:[0.1067]
Loss:[0.1085]
Loss:[0.1064]
Loss:[0.1075]
Loss:[0.1047]
Loss:[0.1099]
Loss:[0.1096]
Loss:[0.1074]
Loss:[0.1015]
Loss:[0.1015]
Loss:[0.0970]
Loss:[0.1071]
Loss:[0.0981]
Loss:[0.1079]
Loss:[0.0975]
Loss:[0.1029]
Loss:[0.1055]
Loss:[0.0951]
Loss:[0.0998]
Loss:[0.1066]
Loss:[0.0983]
Loss:[0.0919]
Loss:[0.0958]
Loss:[0.0921]
Loss:[0.1026]
Loss:[0.1031]
Loss:[0.0934]
Loss:[0.0928]
Loss:[0.0906]
Loss:[0.0931]
Loss:[0.0850]
Loss:[0.0909]
Loss:[0.0793]
Loss:[0.0840]
Loss:[0.0761]
Loss:[0.0791]
Loss:[0.0827]
Loss:[0.0829]
Loss:[0.0836]
Loss:[0.0924]
Loss:[0.0888]
Loss:[0.0883]
Loss:[0.0824]
Loss:[0.0853]
Loss:[0.0827]
Loss:[0.0726]
Loss:[0.0902]
Loss:[0.0845]
Loss:[0.0818]
Loss:[0.0806]
Loss:[0.0734]
Loss:[0.0808]
Loss:[0.0820]
Loss:[0.0823]
Loss:[0.0775]
Loss:[0.0763]
Loss:[0.0675]
Loss:[0.0798]
Loss:[0.0807]
Loss:[0.0798]
Loss:[0.0828]
Loss:[0.0728]
Loss:[0.0784]
Loss:[0.0680]
Loss:[0.0764]
Loss:[0.0704]
Loss:[0.0762]
Loss:[0.0678]
Loss:[0.0803]
Loss:[0.0693]
Loss:[0.0728]
Loss:[0.0659]
Loss:[0.0718]
Loss:[0.0610]
Loss:[0.0695]
Loss:[0.0628]
Loss:[0.0619]
Loss:[0.0607]
Loss:[0.0774]
Loss:[0.0673]
Loss:[0.0621]
Loss:[0.0747]
Loss:[0.0662]
Loss:[0.0695]
Loss:[0.0650]
Loss:[0.0595]
Loss:[0.0694]
Loss:[0.0608]
Loss:[0.0546]
Loss:[0.0647]
Loss:[0.0598]
Loss:[0.0646]
Loss:[0.0605]
Loss:[0.0612]
Loss:[0.0634]
Loss:[0.0657]
Loss:[0.0673]
Loss:[0.0604]
Loss:[0.0523]
Loss:[0.0629]
Loss:[0.0600]
Loss:[0.0578]
Loss:[0.0553]
Loss:[0.0606]
Loss:[0.0644]
Loss:[0.0640]
Loss:[0.0609]
Loss:[0.0544]
Loss:[0.0608]
Loss:[0.0577]
Loss:[0.0562]
Loss:[0.0625]
Loss:[0.0552]
Loss:[0.0548]
Loss:[0.0572]
Loss:[0.0505]
Loss:[0.0546]
Loss:[0.0580]
Loss:[0.0589]
Loss:[0.0523]
Loss:[0.0522]
Loss:[0.0550]
Loss:[0.0509]
Loss:[0.0536]
Loss:[0.0614]
Loss:[0.0559]
Loss:[0.0558]
Loss:[0.0568]
Loss:[0.0484]
Loss:[0.0464]
Loss:[0.0485]
Loss:[0.0513]
Loss:[0.0521]
Loss:[0.0465]
Loss:[0.0547]
Loss:[0.0466]
Loss:[0.0521]
Loss:[0.0549]
Loss:[0.0482]
Loss:[0.0552]
Loss:[0.0448]
Loss:[0.0518]
Loss:[0.0497]
Loss:[0.0440]
Loss:[0.0469]
Loss:[0.0464]
Loss:[0.0467]
Loss:[0.0487]
Loss:[0.0434]
Loss:[0.0421]
Loss:[0.0432]
Loss:[0.0459]
Loss:[0.0450]
Loss:[0.0476]
Loss:[0.0496]
Loss:[0.0433]
Loss:[0.0464]
Loss:[0.0418]
Loss:[0.0422]
Loss:[0.0456]
Loss:[0.0472]
Loss:[0.0457]
Loss:[0.0368]
Loss:[0.0478]
Loss:[0.0442]
Loss:[0.0454]
Loss:[0.0406]
Loss:[0.0435]
Loss:[0.0394]
Loss:[0.0429]
Loss:[0.0374]
Loss:[0.0406]
Loss:[0.0390]
Loss:[0.0410]
Loss:[0.0373]
Loss:[0.0404]
Loss:[0.0416]
Loss:[0.0444]
Loss:[0.0421]
Loss:[0.0333]
Loss:[0.0349]
Loss:[0.0376]
Loss:[0.0325]
Loss:[0.0386]
Loss:[0.0408]
Loss:[0.0378]
Loss:[0.0385]
Loss:[0.0417]
Loss:[0.0346]
Loss:[0.0370]
Loss:[0.0459]
Loss:[0.0443]
Loss:[0.0407]
Loss:[0.0338]
Loss:[0.0350]
Loss:[0.0398]
Loss:[0.0347]
Loss:[0.0354]
Loss:[0.0384]
Loss:[0.0334]
Loss:[0.0381]
Loss:[0.0359]
Loss:[0.0337]
Early stopping!
Loading 333th epoch
acc:[0.8130]
acc:[0.8130]
acc:[0.8180]
acc:[0.8130]
acc:[0.8130]
acc:[0.8160]
acc:[0.8130]
acc:[0.8110]
acc:[0.8140]
acc:[0.8130]
acc:[0.8160]
acc:[0.8160]
acc:[0.8150]
acc:[0.8110]
acc:[0.8130]
acc:[0.8150]
acc:[0.8130]
acc:[0.8100]
acc:[0.8150]
acc:[0.8150]
acc:[0.8130]
acc:[0.8120]
acc:[0.8160]
acc:[0.8180]
acc:[0.8110]
acc:[0.8140]
acc:[0.8160]
acc:[0.8140]
acc:[0.8140]
acc:[0.8130]
acc:[0.8160]
acc:[0.8170]
acc:[0.8180]
acc:[0.8150]
acc:[0.8160]
acc:[0.8140]
acc:[0.8140]
acc:[0.8120]
acc:[0.8140]
acc:[0.8150]
acc:[0.8130]
acc:[0.8090]
acc:[0.8150]
acc:[0.8150]
acc:[0.8120]
acc:[0.8120]
acc:[0.8150]
acc:[0.8160]
acc:[0.8160]
acc:[0.8140]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8141]
Mean:[81.4100]
Std :[0.2003]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='cora', drop_percent=0.02, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6920]
Loss:[0.6871]
Loss:[0.6852]
Loss:[0.6795]
Loss:[0.6756]
Loss:[0.6686]
Loss:[0.6630]
Loss:[0.6568]
Loss:[0.6473]
Loss:[0.6397]
Loss:[0.6274]
Loss:[0.6198]
Loss:[0.6080]
Loss:[0.5947]
Loss:[0.5862]
Loss:[0.5686]
Loss:[0.5575]
Loss:[0.5437]
Loss:[0.5298]
Loss:[0.5100]
Loss:[0.5022]
Loss:[0.4846]
Loss:[0.4729]
Loss:[0.4581]
Loss:[0.4430]
Loss:[0.4309]
Loss:[0.4210]
Loss:[0.4149]
Loss:[0.3944]
Loss:[0.3787]
Loss:[0.3761]
Loss:[0.3592]
Loss:[0.3404]
Loss:[0.3335]
Loss:[0.3284]
Loss:[0.3175]
Loss:[0.3072]
Loss:[0.2983]
Loss:[0.2889]
Loss:[0.2931]
Loss:[0.2770]
Loss:[0.2755]
Loss:[0.2705]
Loss:[0.2662]
Loss:[0.2537]
Loss:[0.2398]
Loss:[0.2508]
Loss:[0.2343]
Loss:[0.2334]
Loss:[0.2224]
Loss:[0.2249]
Loss:[0.2179]
Loss:[0.2183]
Loss:[0.2047]
Loss:[0.2113]
Loss:[0.1995]
Loss:[0.1906]
Loss:[0.1933]
Loss:[0.1780]
Loss:[0.1796]
Loss:[0.1954]
Loss:[0.1839]
Loss:[0.1718]
Loss:[0.1687]
Loss:[0.1615]
Loss:[0.1687]
Loss:[0.1642]
Loss:[0.1592]
Loss:[0.1671]
Loss:[0.1648]
Loss:[0.1637]
Loss:[0.1410]
Loss:[0.1489]
Loss:[0.1522]
Loss:[0.1478]
Loss:[0.1464]
Loss:[0.1533]
Loss:[0.1442]
Loss:[0.1401]
Loss:[0.1502]
Loss:[0.1382]
Loss:[0.1353]
Loss:[0.1356]
Loss:[0.1394]
Loss:[0.1320]
Loss:[0.1334]
Loss:[0.1270]
Loss:[0.1323]
Loss:[0.1286]
Loss:[0.1211]
Loss:[0.1168]
Loss:[0.1232]
Loss:[0.1153]
Loss:[0.1257]
Loss:[0.1121]
Loss:[0.1239]
Loss:[0.1153]
Loss:[0.1162]
Loss:[0.1089]
Loss:[0.1220]
Loss:[0.1098]
Loss:[0.1156]
Loss:[0.1114]
Loss:[0.1024]
Loss:[0.1146]
Loss:[0.1076]
Loss:[0.1023]
Loss:[0.0981]
Loss:[0.1068]
Loss:[0.1257]
Loss:[0.1075]
Loss:[0.1151]
Loss:[0.1138]
Loss:[0.0953]
Loss:[0.1104]
Loss:[0.1018]
Loss:[0.0964]
Loss:[0.0983]
Loss:[0.1017]
Loss:[0.1038]
Loss:[0.0932]
Loss:[0.0814]
Loss:[0.0946]
Loss:[0.0922]
Loss:[0.0827]
Loss:[0.0907]
Loss:[0.0893]
Loss:[0.0949]
Loss:[0.0873]
Loss:[0.0876]
Loss:[0.0854]
Loss:[0.0864]
Loss:[0.0950]
Loss:[0.0789]
Loss:[0.0832]
Loss:[0.0797]
Loss:[0.0787]
Loss:[0.0783]
Loss:[0.0793]
Loss:[0.0841]
Loss:[0.0712]
Loss:[0.0777]
Loss:[0.0853]
Loss:[0.0665]
Loss:[0.0792]
Loss:[0.0749]
Loss:[0.0756]
Loss:[0.0803]
Loss:[0.0729]
Loss:[0.0742]
Loss:[0.0762]
Loss:[0.0686]
Loss:[0.0719]
Loss:[0.0737]
Loss:[0.0725]
Loss:[0.0752]
Loss:[0.0748]
Loss:[0.0733]
Loss:[0.0642]
Loss:[0.0690]
Loss:[0.0596]
Loss:[0.0719]
Loss:[0.0634]
Loss:[0.0811]
Loss:[0.0681]
Loss:[0.0707]
Loss:[0.0701]
Loss:[0.0609]
Loss:[0.0674]
Loss:[0.0709]
Loss:[0.0629]
Loss:[0.0600]
Loss:[0.0632]
Loss:[0.0596]
Loss:[0.0702]
Loss:[0.0693]
Loss:[0.0659]
Loss:[0.0660]
Loss:[0.0574]
Loss:[0.0644]
Loss:[0.0556]
Loss:[0.0616]
Loss:[0.0488]
Loss:[0.0541]
Loss:[0.0517]
Loss:[0.0577]
Loss:[0.0569]
Loss:[0.0584]
Loss:[0.0575]
Loss:[0.0632]
Loss:[0.0619]
Loss:[0.0622]
Loss:[0.0564]
Loss:[0.0575]
Loss:[0.0543]
Loss:[0.0486]
Loss:[0.0593]
Loss:[0.0562]
Loss:[0.0556]
Loss:[0.0539]
Loss:[0.0512]
Loss:[0.0542]
Loss:[0.0546]
Loss:[0.0578]
Loss:[0.0516]
Loss:[0.0495]
Loss:[0.0439]
Loss:[0.0522]
Loss:[0.0519]
Loss:[0.0538]
Loss:[0.0525]
Loss:[0.0508]
Loss:[0.0562]
Loss:[0.0455]
Loss:[0.0544]
Loss:[0.0464]
Loss:[0.0502]
Loss:[0.0447]
Loss:[0.0603]
Loss:[0.0458]
Loss:[0.0488]
Loss:[0.0437]
Loss:[0.0518]
Loss:[0.0409]
Loss:[0.0439]
Loss:[0.0438]
Loss:[0.0409]
Loss:[0.0417]
Loss:[0.0488]
Loss:[0.0453]
Loss:[0.0431]
Loss:[0.0493]
Loss:[0.0460]
Loss:[0.0485]
Loss:[0.0481]
Loss:[0.0418]
Loss:[0.0557]
Loss:[0.0416]
Loss:[0.0370]
Loss:[0.0464]
Loss:[0.0375]
Loss:[0.0408]
Loss:[0.0424]
Loss:[0.0442]
Loss:[0.0464]
Loss:[0.0424]
Loss:[0.0527]
Loss:[0.0412]
Loss:[0.0352]
Loss:[0.0412]
Loss:[0.0460]
Loss:[0.0398]
Loss:[0.0398]
Loss:[0.0459]
Loss:[0.0481]
Loss:[0.0463]
Loss:[0.0453]
Loss:[0.0392]
Loss:[0.0365]
Loss:[0.0414]
Loss:[0.0412]
Loss:[0.0459]
Loss:[0.0389]
Loss:[0.0370]
Loss:[0.0455]
Loss:[0.0354]
Loss:[0.0411]
Loss:[0.0382]
Loss:[0.0441]
Early stopping!
Loading 249th epoch
acc:[0.8230]
acc:[0.8210]
acc:[0.8230]
acc:[0.8230]
acc:[0.8230]
acc:[0.8220]
acc:[0.8200]
acc:[0.8240]
acc:[0.8230]
acc:[0.8230]
acc:[0.8220]
acc:[0.8210]
acc:[0.8230]
acc:[0.8220]
acc:[0.8220]
acc:[0.8250]
acc:[0.8230]
acc:[0.8210]
acc:[0.8230]
acc:[0.8210]
acc:[0.8200]
acc:[0.8220]
acc:[0.8220]
acc:[0.8240]
acc:[0.8230]
acc:[0.8250]
acc:[0.8210]
acc:[0.8230]
acc:[0.8220]
acc:[0.8230]
acc:[0.8220]
acc:[0.8220]
acc:[0.8220]
acc:[0.8230]
acc:[0.8240]
acc:[0.8210]
acc:[0.8230]
acc:[0.8210]
acc:[0.8210]
acc:[0.8230]
acc:[0.8260]
acc:[0.8210]
acc:[0.8230]
acc:[0.8250]
acc:[0.8250]
acc:[0.8220]
acc:[0.8220]
acc:[0.8270]
acc:[0.8230]
acc:[0.8230]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8226]
Mean:[82.2640]
Std :[0.1453]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='cora', drop_percent=0.05, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6909]
Loss:[0.6872]
Loss:[0.6829]
Loss:[0.6774]
Loss:[0.6716]
Loss:[0.6638]
Loss:[0.6575]
Loss:[0.6494]
Loss:[0.6398]
Loss:[0.6313]
Loss:[0.6190]
Loss:[0.6090]
Loss:[0.5981]
Loss:[0.5871]
Loss:[0.5757]
Loss:[0.5607]
Loss:[0.5523]
Loss:[0.5370]
Loss:[0.5254]
Loss:[0.5070]
Loss:[0.5009]
Loss:[0.4780]
Loss:[0.4744]
Loss:[0.4604]
Loss:[0.4477]
Loss:[0.4366]
Loss:[0.4259]
Loss:[0.4127]
Loss:[0.3930]
Loss:[0.3840]
Loss:[0.3798]
Loss:[0.3663]
Loss:[0.3455]
Loss:[0.3386]
Loss:[0.3353]
Loss:[0.3251]
Loss:[0.3095]
Loss:[0.3022]
Loss:[0.2981]
Loss:[0.2964]
Loss:[0.2786]
Loss:[0.2751]
Loss:[0.2725]
Loss:[0.2714]
Loss:[0.2552]
Loss:[0.2373]
Loss:[0.2526]
Loss:[0.2456]
Loss:[0.2399]
Loss:[0.2271]
Loss:[0.2296]
Loss:[0.2242]
Loss:[0.2185]
Loss:[0.2117]
Loss:[0.2167]
Loss:[0.2046]
Loss:[0.1966]
Loss:[0.1986]
Loss:[0.1839]
Loss:[0.1824]
Loss:[0.1969]
Loss:[0.1876]
Loss:[0.1788]
Loss:[0.1745]
Loss:[0.1672]
Loss:[0.1764]
Loss:[0.1643]
Loss:[0.1621]
Loss:[0.1672]
Loss:[0.1616]
Loss:[0.1658]
Loss:[0.1454]
Loss:[0.1525]
Loss:[0.1494]
Loss:[0.1477]
Loss:[0.1491]
Loss:[0.1526]
Loss:[0.1456]
Loss:[0.1433]
Loss:[0.1498]
Loss:[0.1406]
Loss:[0.1373]
Loss:[0.1389]
Loss:[0.1397]
Loss:[0.1299]
Loss:[0.1374]
Loss:[0.1279]
Loss:[0.1347]
Loss:[0.1243]
Loss:[0.1262]
Loss:[0.1180]
Loss:[0.1215]
Loss:[0.1211]
Loss:[0.1280]
Loss:[0.1141]
Loss:[0.1253]
Loss:[0.1177]
Loss:[0.1177]
Loss:[0.1106]
Loss:[0.1245]
Loss:[0.1087]
Loss:[0.1190]
Loss:[0.1107]
Loss:[0.1046]
Loss:[0.1111]
Loss:[0.1092]
Loss:[0.0987]
Loss:[0.0996]
Loss:[0.1055]
Loss:[0.1195]
Loss:[0.1064]
Loss:[0.1146]
Loss:[0.1144]
Loss:[0.1043]
Loss:[0.1074]
Loss:[0.1120]
Loss:[0.0896]
Loss:[0.1031]
Loss:[0.0950]
Loss:[0.1044]
Loss:[0.0919]
Loss:[0.0835]
Loss:[0.0935]
Loss:[0.0905]
Loss:[0.0823]
Loss:[0.0879]
Loss:[0.0884]
Loss:[0.0913]
Loss:[0.0921]
Loss:[0.0818]
Loss:[0.0863]
Loss:[0.0844]
Loss:[0.0923]
Loss:[0.0783]
Loss:[0.0845]
Loss:[0.0789]
Loss:[0.0774]
Loss:[0.0786]
Loss:[0.0784]
Loss:[0.0837]
Loss:[0.0709]
Loss:[0.0770]
Loss:[0.0831]
Loss:[0.0699]
Loss:[0.0749]
Loss:[0.0711]
Loss:[0.0717]
Loss:[0.0773]
Loss:[0.0723]
Loss:[0.0711]
Loss:[0.0758]
Loss:[0.0647]
Loss:[0.0697]
Loss:[0.0709]
Loss:[0.0729]
Loss:[0.0729]
Loss:[0.0718]
Loss:[0.0708]
Loss:[0.0625]
Loss:[0.0648]
Loss:[0.0606]
Loss:[0.0736]
Loss:[0.0618]
Loss:[0.0744]
Loss:[0.0642]
Loss:[0.0684]
Loss:[0.0715]
Loss:[0.0601]
Loss:[0.0659]
Loss:[0.0728]
Loss:[0.0614]
Loss:[0.0596]
Loss:[0.0636]
Loss:[0.0598]
Loss:[0.0682]
Loss:[0.0671]
Loss:[0.0641]
Loss:[0.0650]
Loss:[0.0564]
Loss:[0.0612]
Loss:[0.0580]
Loss:[0.0590]
Loss:[0.0505]
Loss:[0.0523]
Loss:[0.0511]
Loss:[0.0555]
Loss:[0.0571]
Loss:[0.0578]
Loss:[0.0553]
Loss:[0.0583]
Loss:[0.0557]
Loss:[0.0564]
Loss:[0.0538]
Loss:[0.0591]
Loss:[0.0530]
Loss:[0.0464]
Loss:[0.0579]
Loss:[0.0541]
Loss:[0.0535]
Loss:[0.0531]
Loss:[0.0494]
Loss:[0.0519]
Loss:[0.0555]
Loss:[0.0546]
Loss:[0.0517]
Loss:[0.0483]
Loss:[0.0486]
Loss:[0.0446]
Loss:[0.0489]
Loss:[0.0528]
Loss:[0.0525]
Loss:[0.0468]
Loss:[0.0496]
Loss:[0.0442]
Loss:[0.0474]
Loss:[0.0450]
Loss:[0.0461]
Loss:[0.0443]
Loss:[0.0580]
Loss:[0.0424]
Loss:[0.0454]
Loss:[0.0409]
Loss:[0.0474]
Loss:[0.0420]
Loss:[0.0443]
Loss:[0.0398]
Loss:[0.0376]
Loss:[0.0406]
Loss:[0.0506]
Loss:[0.0452]
Loss:[0.0413]
Loss:[0.0508]
Loss:[0.0440]
Loss:[0.0460]
Loss:[0.0485]
Loss:[0.0417]
Loss:[0.0538]
Loss:[0.0408]
Loss:[0.0391]
Loss:[0.0429]
Loss:[0.0407]
Loss:[0.0387]
Loss:[0.0420]
Loss:[0.0408]
Loss:[0.0430]
Loss:[0.0424]
Loss:[0.0487]
Early stopping!
Loading 227th epoch
acc:[0.8200]
acc:[0.8240]
acc:[0.8200]
acc:[0.8210]
acc:[0.8200]
acc:[0.8200]
acc:[0.8190]
acc:[0.8200]
acc:[0.8220]
acc:[0.8220]
acc:[0.8190]
acc:[0.8230]
acc:[0.8210]
acc:[0.8230]
acc:[0.8180]
acc:[0.8230]
acc:[0.8210]
acc:[0.8170]
acc:[0.8230]
acc:[0.8210]
acc:[0.8210]
acc:[0.8190]
acc:[0.8190]
acc:[0.8210]
acc:[0.8200]
acc:[0.8200]
acc:[0.8210]
acc:[0.8210]
acc:[0.8230]
acc:[0.8220]
acc:[0.8200]
acc:[0.8190]
acc:[0.8210]
acc:[0.8210]
acc:[0.8200]
acc:[0.8190]
acc:[0.8180]
acc:[0.8200]
acc:[0.8170]
acc:[0.8220]
acc:[0.8170]
acc:[0.8190]
acc:[0.8190]
acc:[0.8200]
acc:[0.8210]
acc:[0.8190]
acc:[0.8220]
acc:[0.8210]
acc:[0.8200]
acc:[0.8190]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8204]
Mean:[82.0360]
Std :[0.1638]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='cora', drop_percent=0.08, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6898]
Loss:[0.6858]
Loss:[0.6801]
Loss:[0.6734]
Loss:[0.6667]
Loss:[0.6567]
Loss:[0.6510]
Loss:[0.6428]
Loss:[0.6335]
Loss:[0.6259]
Loss:[0.6163]
Loss:[0.6075]
Loss:[0.5985]
Loss:[0.5857]
Loss:[0.5761]
Loss:[0.5664]
Loss:[0.5524]
Loss:[0.5387]
Loss:[0.5308]
Loss:[0.5131]
Loss:[0.5085]
Loss:[0.4865]
Loss:[0.4784]
Loss:[0.4680]
Loss:[0.4565]
Loss:[0.4422]
Loss:[0.4320]
Loss:[0.4226]
Loss:[0.4063]
Loss:[0.3941]
Loss:[0.3866]
Loss:[0.3759]
Loss:[0.3592]
Loss:[0.3467]
Loss:[0.3398]
Loss:[0.3362]
Loss:[0.3247]
Loss:[0.3077]
Loss:[0.3010]
Loss:[0.3042]
Loss:[0.2884]
Loss:[0.2842]
Loss:[0.2725]
Loss:[0.2744]
Loss:[0.2623]
Loss:[0.2466]
Loss:[0.2508]
Loss:[0.2358]
Loss:[0.2395]
Loss:[0.2255]
Loss:[0.2244]
Loss:[0.2177]
Loss:[0.2201]
Loss:[0.2055]
Loss:[0.2099]
Loss:[0.2031]
Loss:[0.1904]
Loss:[0.1981]
Loss:[0.1859]
Loss:[0.1857]
Loss:[0.1920]
Loss:[0.1827]
Loss:[0.1760]
Loss:[0.1705]
Loss:[0.1630]
Loss:[0.1725]
Loss:[0.1657]
Loss:[0.1576]
Loss:[0.1631]
Loss:[0.1680]
Loss:[0.1610]
Loss:[0.1435]
Loss:[0.1512]
Loss:[0.1431]
Loss:[0.1451]
Loss:[0.1450]
Loss:[0.1425]
Loss:[0.1402]
Loss:[0.1343]
Loss:[0.1367]
Loss:[0.1385]
Loss:[0.1328]
Loss:[0.1307]
Loss:[0.1369]
Loss:[0.1329]
Loss:[0.1289]
Loss:[0.1194]
Loss:[0.1265]
Loss:[0.1213]
Loss:[0.1211]
Loss:[0.1114]
Loss:[0.1164]
Loss:[0.1142]
Loss:[0.1101]
Loss:[0.1087]
Loss:[0.1114]
Loss:[0.1056]
Loss:[0.1066]
Loss:[0.1011]
Loss:[0.1128]
Loss:[0.1047]
Loss:[0.1096]
Loss:[0.1012]
Loss:[0.0994]
Loss:[0.1044]
Loss:[0.1008]
Loss:[0.0953]
Loss:[0.0874]
Loss:[0.0956]
Loss:[0.1062]
Loss:[0.0932]
Loss:[0.1028]
Loss:[0.0961]
Loss:[0.0942]
Loss:[0.0907]
Loss:[0.0901]
Loss:[0.0868]
Loss:[0.0874]
Loss:[0.0871]
Loss:[0.0859]
Loss:[0.0788]
Loss:[0.0772]
Loss:[0.0857]
Loss:[0.0829]
Loss:[0.0752]
Loss:[0.0765]
Loss:[0.0802]
Loss:[0.0812]
Loss:[0.0787]
Loss:[0.0754]
Loss:[0.0771]
Loss:[0.0780]
Loss:[0.0785]
Loss:[0.0688]
Loss:[0.0768]
Loss:[0.0718]
Loss:[0.0699]
Loss:[0.0749]
Loss:[0.0695]
Loss:[0.0681]
Loss:[0.0640]
Loss:[0.0680]
Loss:[0.0719]
Loss:[0.0648]
Loss:[0.0654]
Loss:[0.0670]
Loss:[0.0638]
Loss:[0.0710]
Loss:[0.0633]
Loss:[0.0649]
Loss:[0.0646]
Loss:[0.0576]
Loss:[0.0591]
Loss:[0.0604]
Loss:[0.0659]
Loss:[0.0649]
Loss:[0.0645]
Loss:[0.0597]
Loss:[0.0567]
Loss:[0.0567]
Loss:[0.0522]
Loss:[0.0641]
Loss:[0.0557]
Loss:[0.0667]
Loss:[0.0569]
Loss:[0.0566]
Loss:[0.0597]
Loss:[0.0535]
Loss:[0.0600]
Loss:[0.0596]
Loss:[0.0554]
Loss:[0.0525]
Loss:[0.0570]
Loss:[0.0517]
Loss:[0.0597]
Loss:[0.0604]
Loss:[0.0552]
Loss:[0.0587]
Loss:[0.0512]
Loss:[0.0522]
Loss:[0.0445]
Loss:[0.0524]
Loss:[0.0450]
Loss:[0.0447]
Loss:[0.0473]
Loss:[0.0479]
Loss:[0.0514]
Loss:[0.0534]
Loss:[0.0460]
Loss:[0.0505]
Loss:[0.0492]
Loss:[0.0492]
Loss:[0.0493]
Loss:[0.0488]
Loss:[0.0454]
Loss:[0.0399]
Loss:[0.0486]
Loss:[0.0463]
Loss:[0.0434]
Loss:[0.0421]
Loss:[0.0428]
Loss:[0.0428]
Loss:[0.0461]
Loss:[0.0433]
Loss:[0.0436]
Loss:[0.0399]
Loss:[0.0428]
Loss:[0.0416]
Loss:[0.0426]
Loss:[0.0455]
Loss:[0.0452]
Loss:[0.0441]
Loss:[0.0421]
Loss:[0.0422]
Loss:[0.0408]
Loss:[0.0361]
Loss:[0.0381]
Loss:[0.0368]
Loss:[0.0440]
Loss:[0.0351]
Loss:[0.0429]
Loss:[0.0349]
Loss:[0.0402]
Loss:[0.0317]
Loss:[0.0393]
Loss:[0.0361]
Loss:[0.0338]
Loss:[0.0370]
Loss:[0.0433]
Loss:[0.0415]
Loss:[0.0337]
Loss:[0.0406]
Loss:[0.0400]
Loss:[0.0369]
Loss:[0.0448]
Loss:[0.0355]
Loss:[0.0486]
Loss:[0.0350]
Loss:[0.0371]
Loss:[0.0365]
Loss:[0.0376]
Loss:[0.0342]
Loss:[0.0359]
Loss:[0.0343]
Early stopping!
Loading 224th epoch
acc:[0.8260]
acc:[0.8250]
acc:[0.8280]
acc:[0.8240]
acc:[0.8260]
acc:[0.8260]
acc:[0.8260]
acc:[0.8250]
acc:[0.8260]
acc:[0.8270]
acc:[0.8250]
acc:[0.8260]
acc:[0.8250]
acc:[0.8280]
acc:[0.8260]
acc:[0.8270]
acc:[0.8230]
acc:[0.8250]
acc:[0.8260]
acc:[0.8280]
acc:[0.8260]
acc:[0.8260]
acc:[0.8280]
acc:[0.8260]
acc:[0.8230]
acc:[0.8270]
acc:[0.8260]
acc:[0.8280]
acc:[0.8270]
acc:[0.8260]
acc:[0.8270]
acc:[0.8260]
acc:[0.8260]
acc:[0.8240]
acc:[0.8250]
acc:[0.8250]
acc:[0.8260]
acc:[0.8240]
acc:[0.8270]
acc:[0.8250]
acc:[0.8220]
acc:[0.8280]
acc:[0.8270]
acc:[0.8250]
acc:[0.8250]
acc:[0.8280]
acc:[0.8270]
acc:[0.8250]
acc:[0.8270]
acc:[0.8260]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8259]
Mean:[82.5920]
Std :[0.1383]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='cora', drop_percent=0.1, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6897]
Loss:[0.6840]
Loss:[0.6762]
Loss:[0.6671]
Loss:[0.6573]
Loss:[0.6516]
Loss:[0.6544]
Loss:[0.6375]
Loss:[0.6427]
Loss:[0.6263]
Loss:[0.6279]
Loss:[0.6198]
Loss:[0.6089]
Loss:[0.6063]
Loss:[0.5982]
Loss:[0.5936]
Loss:[0.5801]
Loss:[0.5760]
Loss:[0.5697]
Loss:[0.5539]
Loss:[0.5521]
Loss:[0.5367]
Loss:[0.5339]
Loss:[0.5216]
Loss:[0.5129]
Loss:[0.4974]
Loss:[0.4904]
Loss:[0.4770]
Loss:[0.4683]
Loss:[0.4554]
Loss:[0.4446]
Loss:[0.4349]
Loss:[0.4203]
Loss:[0.4074]
Loss:[0.4016]
Loss:[0.3868]
Loss:[0.3777]
Loss:[0.3671]
Loss:[0.3506]
Loss:[0.3464]
Loss:[0.3346]
Loss:[0.3359]
Loss:[0.3195]
Loss:[0.3125]
Loss:[0.3048]
Loss:[0.2939]
Loss:[0.2882]
Loss:[0.2744]
Loss:[0.2715]
Loss:[0.2660]
Loss:[0.2522]
Loss:[0.2451]
Loss:[0.2525]
Loss:[0.2350]
Loss:[0.2361]
Loss:[0.2280]
Loss:[0.2153]
Loss:[0.2200]
Loss:[0.2048]
Loss:[0.1972]
Loss:[0.2080]
Loss:[0.2000]
Loss:[0.1950]
Loss:[0.1815]
Loss:[0.1776]
Loss:[0.1872]
Loss:[0.1724]
Loss:[0.1694]
Loss:[0.1748]
Loss:[0.1753]
Loss:[0.1652]
Loss:[0.1536]
Loss:[0.1567]
Loss:[0.1514]
Loss:[0.1504]
Loss:[0.1515]
Loss:[0.1520]
Loss:[0.1475]
Loss:[0.1411]
Loss:[0.1484]
Loss:[0.1402]
Loss:[0.1348]
Loss:[0.1383]
Loss:[0.1363]
Loss:[0.1346]
Loss:[0.1354]
Loss:[0.1222]
Loss:[0.1357]
Loss:[0.1252]
Loss:[0.1229]
Loss:[0.1184]
Loss:[0.1178]
Loss:[0.1171]
Loss:[0.1183]
Loss:[0.1097]
Loss:[0.1200]
Loss:[0.1092]
Loss:[0.1063]
Loss:[0.1015]
Loss:[0.1127]
Loss:[0.1049]
Loss:[0.1155]
Loss:[0.1017]
Loss:[0.1015]
Loss:[0.1038]
Loss:[0.1035]
Loss:[0.0988]
Loss:[0.0875]
Loss:[0.0993]
Loss:[0.1006]
Loss:[0.0928]
Loss:[0.1047]
Loss:[0.0897]
Loss:[0.0975]
Loss:[0.0901]
Loss:[0.0911]
Loss:[0.0831]
Loss:[0.0879]
Loss:[0.0860]
Loss:[0.0875]
Loss:[0.0859]
Loss:[0.0745]
Loss:[0.0820]
Loss:[0.0784]
Loss:[0.0789]
Loss:[0.0761]
Loss:[0.0815]
Loss:[0.0802]
Loss:[0.0776]
Loss:[0.0730]
Loss:[0.0790]
Loss:[0.0773]
Loss:[0.0763]
Loss:[0.0706]
Loss:[0.0750]
Loss:[0.0724]
Loss:[0.0665]
Loss:[0.0752]
Loss:[0.0665]
Loss:[0.0711]
Loss:[0.0624]
Loss:[0.0676]
Loss:[0.0649]
Loss:[0.0623]
Loss:[0.0658]
Loss:[0.0643]
Loss:[0.0612]
Loss:[0.0681]
Loss:[0.0616]
Loss:[0.0621]
Loss:[0.0597]
Loss:[0.0555]
Loss:[0.0610]
Loss:[0.0603]
Loss:[0.0616]
Loss:[0.0596]
Loss:[0.0613]
Loss:[0.0614]
Loss:[0.0548]
Loss:[0.0530]
Loss:[0.0526]
Loss:[0.0583]
Loss:[0.0546]
Loss:[0.0673]
Loss:[0.0565]
Loss:[0.0536]
Loss:[0.0552]
Loss:[0.0502]
Loss:[0.0546]
Loss:[0.0559]
Loss:[0.0527]
Loss:[0.0511]
Loss:[0.0538]
Loss:[0.0466]
Loss:[0.0521]
Loss:[0.0527]
Loss:[0.0522]
Loss:[0.0554]
Loss:[0.0481]
Loss:[0.0484]
Loss:[0.0448]
Loss:[0.0494]
Loss:[0.0434]
Loss:[0.0429]
Loss:[0.0412]
Loss:[0.0440]
Loss:[0.0466]
Loss:[0.0495]
Loss:[0.0448]
Loss:[0.0460]
Loss:[0.0456]
Loss:[0.0463]
Loss:[0.0461]
Loss:[0.0459]
Loss:[0.0424]
Loss:[0.0427]
Loss:[0.0435]
Loss:[0.0403]
Loss:[0.0430]
Loss:[0.0414]
Loss:[0.0389]
Loss:[0.0402]
Loss:[0.0421]
Loss:[0.0415]
Loss:[0.0439]
Loss:[0.0385]
Loss:[0.0396]
Loss:[0.0355]
Loss:[0.0366]
Loss:[0.0397]
Loss:[0.0412]
Loss:[0.0379]
Loss:[0.0397]
Loss:[0.0360]
Loss:[0.0378]
Loss:[0.0333]
Loss:[0.0343]
Loss:[0.0358]
Loss:[0.0389]
Loss:[0.0324]
Loss:[0.0393]
Loss:[0.0317]
Loss:[0.0402]
Loss:[0.0303]
Loss:[0.0348]
Loss:[0.0334]
Loss:[0.0306]
Loss:[0.0335]
Loss:[0.0394]
Loss:[0.0343]
Loss:[0.0310]
Loss:[0.0375]
Loss:[0.0345]
Loss:[0.0366]
Loss:[0.0355]
Loss:[0.0324]
Loss:[0.0398]
Loss:[0.0328]
Loss:[0.0290]
Loss:[0.0362]
Loss:[0.0285]
Loss:[0.0315]
Loss:[0.0292]
Loss:[0.0305]
Loss:[0.0309]
Loss:[0.0316]
Loss:[0.0349]
Loss:[0.0284]
Loss:[0.0244]
Loss:[0.0274]
Loss:[0.0304]
Loss:[0.0262]
Loss:[0.0280]
Loss:[0.0283]
Loss:[0.0320]
Loss:[0.0303]
Loss:[0.0307]
Loss:[0.0290]
Loss:[0.0245]
Loss:[0.0278]
Loss:[0.0290]
Loss:[0.0309]
Loss:[0.0266]
Loss:[0.0262]
Loss:[0.0304]
Loss:[0.0253]
Loss:[0.0296]
Loss:[0.0265]
Loss:[0.0289]
Early stopping!
Loading 249th epoch
acc:[0.8160]
acc:[0.8180]
acc:[0.8160]
acc:[0.8160]
acc:[0.8130]
acc:[0.8150]
acc:[0.8140]
acc:[0.8140]
acc:[0.8180]
acc:[0.8160]
acc:[0.8160]
acc:[0.8150]
acc:[0.8150]
acc:[0.8160]
acc:[0.8170]
acc:[0.8140]
acc:[0.8190]
acc:[0.8110]
acc:[0.8180]
acc:[0.8140]
acc:[0.8140]
acc:[0.8170]
acc:[0.8150]
acc:[0.8170]
acc:[0.8150]
acc:[0.8170]
acc:[0.8150]
acc:[0.8140]
acc:[0.8170]
acc:[0.8140]
acc:[0.8160]
acc:[0.8150]
acc:[0.8150]
acc:[0.8120]
acc:[0.8090]
acc:[0.8130]
acc:[0.8150]
acc:[0.8100]
acc:[0.8170]
acc:[0.8130]
acc:[0.8160]
acc:[0.8170]
acc:[0.8180]
acc:[0.8160]
acc:[0.8150]
acc:[0.8170]
acc:[0.8170]
acc:[0.8140]
acc:[0.8180]
acc:[0.8150]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8153]
Mean:[81.5280]
Std :[0.2061]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='cora', drop_percent=0.2, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6888]
Loss:[0.6811]
Loss:[0.6700]
Loss:[0.6553]
Loss:[0.6376]
Loss:[0.6213]
Loss:[0.6110]
Loss:[0.6025]
Loss:[0.5970]
Loss:[0.5915]
Loss:[0.5847]
Loss:[0.5780]
Loss:[0.5711]
Loss:[0.5579]
Loss:[0.5591]
Loss:[0.5434]
Loss:[0.5349]
Loss:[0.5302]
Loss:[0.5250]
Loss:[0.5146]
Loss:[0.5067]
Loss:[0.4987]
Loss:[0.4911]
Loss:[0.4817]
Loss:[0.4733]
Loss:[0.4627]
Loss:[0.4511]
Loss:[0.4433]
Loss:[0.4306]
Loss:[0.4231]
Loss:[0.4095]
Loss:[0.4073]
Loss:[0.3904]
Loss:[0.3878]
Loss:[0.3769]
Loss:[0.3579]
Loss:[0.3536]
Loss:[0.3398]
Loss:[0.3288]
Loss:[0.3273]
Loss:[0.3162]
Loss:[0.3020]
Loss:[0.2946]
Loss:[0.2900]
Loss:[0.2773]
Loss:[0.2640]
Loss:[0.2573]
Loss:[0.2499]
Loss:[0.2416]
Loss:[0.2335]
Loss:[0.2336]
Loss:[0.2245]
Loss:[0.2263]
Loss:[0.2055]
Loss:[0.2010]
Loss:[0.1952]
Loss:[0.1867]
Loss:[0.1845]
Loss:[0.1764]
Loss:[0.1672]
Loss:[0.1694]
Loss:[0.1657]
Loss:[0.1659]
Loss:[0.1568]
Loss:[0.1504]
Loss:[0.1487]
Loss:[0.1480]
Loss:[0.1453]
Loss:[0.1355]
Loss:[0.1516]
Loss:[0.1419]
Loss:[0.1157]
Loss:[0.1333]
Loss:[0.1181]
Loss:[0.1185]
Loss:[0.1120]
Loss:[0.1166]
Loss:[0.1138]
Loss:[0.1077]
Loss:[0.1101]
Loss:[0.1012]
Loss:[0.1026]
Loss:[0.0981]
Loss:[0.0963]
Loss:[0.0923]
Loss:[0.0903]
Loss:[0.0907]
Loss:[0.0877]
Loss:[0.0886]
Loss:[0.0822]
Loss:[0.0778]
Loss:[0.0812]
Loss:[0.0782]
Loss:[0.0805]
Loss:[0.0776]
Loss:[0.0758]
Loss:[0.0717]
Loss:[0.0692]
Loss:[0.0710]
Loss:[0.0720]
Loss:[0.0696]
Loss:[0.0716]
Loss:[0.0667]
Loss:[0.0608]
Loss:[0.0645]
Loss:[0.0634]
Loss:[0.0611]
Loss:[0.0531]
Loss:[0.0622]
Loss:[0.0584]
Loss:[0.0627]
Loss:[0.0573]
Loss:[0.0557]
Loss:[0.0583]
Loss:[0.0555]
Loss:[0.0526]
Loss:[0.0542]
Loss:[0.0565]
Loss:[0.0544]
Loss:[0.0527]
Loss:[0.0508]
Loss:[0.0480]
Loss:[0.0485]
Loss:[0.0469]
Loss:[0.0433]
Loss:[0.0453]
Loss:[0.0434]
Loss:[0.0447]
Loss:[0.0474]
Loss:[0.0426]
Loss:[0.0436]
Loss:[0.0438]
Loss:[0.0430]
Loss:[0.0421]
Loss:[0.0408]
Loss:[0.0421]
Loss:[0.0381]
Loss:[0.0426]
Loss:[0.0412]
Loss:[0.0377]
Loss:[0.0348]
Loss:[0.0400]
Loss:[0.0388]
Loss:[0.0344]
Loss:[0.0371]
Loss:[0.0347]
Loss:[0.0340]
Loss:[0.0366]
Loss:[0.0357]
Loss:[0.0325]
Loss:[0.0340]
Loss:[0.0337]
Loss:[0.0339]
Loss:[0.0325]
Loss:[0.0323]
Loss:[0.0337]
Loss:[0.0325]
Loss:[0.0317]
Loss:[0.0289]
Loss:[0.0289]
Loss:[0.0286]
Loss:[0.0291]
Loss:[0.0291]
Loss:[0.0351]
Loss:[0.0301]
Loss:[0.0293]
Loss:[0.0279]
Loss:[0.0260]
Loss:[0.0302]
Loss:[0.0268]
Loss:[0.0281]
Loss:[0.0251]
Loss:[0.0295]
Loss:[0.0252]
Loss:[0.0252]
Loss:[0.0216]
Loss:[0.0266]
Loss:[0.0274]
Loss:[0.0273]
Loss:[0.0239]
Loss:[0.0264]
Loss:[0.0241]
Loss:[0.0239]
Loss:[0.0235]
Loss:[0.0264]
Loss:[0.0225]
Loss:[0.0245]
Loss:[0.0244]
Loss:[0.0243]
Loss:[0.0237]
Loss:[0.0216]
Loss:[0.0240]
Loss:[0.0212]
Loss:[0.0224]
Loss:[0.0210]
Loss:[0.0192]
Loss:[0.0235]
Loss:[0.0205]
Loss:[0.0245]
Loss:[0.0221]
Loss:[0.0203]
Loss:[0.0216]
Loss:[0.0205]
Loss:[0.0204]
Loss:[0.0196]
Loss:[0.0170]
Loss:[0.0208]
Loss:[0.0171]
Loss:[0.0182]
Loss:[0.0208]
Loss:[0.0193]
Loss:[0.0200]
Loss:[0.0181]
Loss:[0.0188]
Loss:[0.0192]
Loss:[0.0167]
Loss:[0.0175]
Loss:[0.0172]
Loss:[0.0195]
Loss:[0.0161]
Loss:[0.0186]
Loss:[0.0166]
Loss:[0.0168]
Loss:[0.0154]
Loss:[0.0170]
Loss:[0.0158]
Loss:[0.0147]
Loss:[0.0180]
Loss:[0.0161]
Loss:[0.0156]
Loss:[0.0158]
Loss:[0.0204]
Loss:[0.0171]
Loss:[0.0183]
Loss:[0.0175]
Loss:[0.0183]
Loss:[0.0174]
Loss:[0.0153]
Loss:[0.0136]
Loss:[0.0214]
Loss:[0.0137]
Loss:[0.0155]
Loss:[0.0165]
Loss:[0.0138]
Loss:[0.0153]
Loss:[0.0145]
Loss:[0.0168]
Loss:[0.0146]
Loss:[0.0126]
Loss:[0.0156]
Loss:[0.0166]
Loss:[0.0149]
Loss:[0.0139]
Loss:[0.0135]
Loss:[0.0169]
Loss:[0.0141]
Loss:[0.0153]
Loss:[0.0141]
Loss:[0.0152]
Loss:[0.0149]
Loss:[0.0164]
Loss:[0.0162]
Loss:[0.0111]
Loss:[0.0148]
Loss:[0.0141]
Loss:[0.0142]
Loss:[0.0152]
Loss:[0.0115]
Loss:[0.0144]
Loss:[0.0137]
Loss:[0.0121]
Loss:[0.0129]
Loss:[0.0122]
Loss:[0.0153]
Loss:[0.0142]
Loss:[0.0140]
Loss:[0.0128]
Loss:[0.0117]
Loss:[0.0137]
Loss:[0.0138]
Loss:[0.0148]
Loss:[0.0112]
Loss:[0.0104]
Loss:[0.0133]
Loss:[0.0114]
Loss:[0.0117]
Loss:[0.0107]
Loss:[0.0094]
Loss:[0.0133]
Loss:[0.0112]
Loss:[0.0110]
Loss:[0.0130]
Loss:[0.0087]
Loss:[0.0105]
Loss:[0.0110]
Loss:[0.0087]
Loss:[0.0106]
Loss:[0.0101]
Loss:[0.0100]
Loss:[0.0095]
Loss:[0.0103]
Loss:[0.0094]
Loss:[0.0098]
Loss:[0.0097]
Loss:[0.0107]
Loss:[0.0097]
Loss:[0.0103]
Loss:[0.0099]
Loss:[0.0089]
Loss:[0.0112]
Loss:[0.0087]
Loss:[0.0124]
Loss:[0.0092]
Early stopping!
Loading 293th epoch
acc:[0.7910]
acc:[0.7920]
acc:[0.7940]
acc:[0.7940]
acc:[0.7930]
acc:[0.7950]
acc:[0.7920]
acc:[0.7920]
acc:[0.7930]
acc:[0.7910]
acc:[0.7930]
acc:[0.7930]
acc:[0.7920]
acc:[0.7920]
acc:[0.7940]
acc:[0.7940]
acc:[0.7920]
acc:[0.7920]
acc:[0.7940]
acc:[0.7930]
acc:[0.7950]
acc:[0.7930]
acc:[0.7960]
acc:[0.7970]
acc:[0.7940]
acc:[0.7960]
acc:[0.7930]
acc:[0.7960]
acc:[0.7940]
acc:[0.7940]
acc:[0.7940]
acc:[0.7940]
acc:[0.7920]
acc:[0.7960]
acc:[0.7950]
acc:[0.7910]
acc:[0.7910]
acc:[0.7900]
acc:[0.7940]
acc:[0.7920]
acc:[0.7950]
acc:[0.7960]
acc:[0.7930]
acc:[0.7930]
acc:[0.7950]
acc:[0.7930]
acc:[0.7930]
acc:[0.7940]
acc:[0.7950]
acc:[0.7920]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7934]
Mean:[79.3440]
Std :[0.1580]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='cora', drop_percent=0.3, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6860]
Loss:[0.6749]
Loss:[0.6580]
Loss:[0.6360]
Loss:[0.6113]
Loss:[0.5851]
Loss:[0.5621]
Loss:[0.5426]
Loss:[0.5330]
Loss:[0.5241]
Loss:[0.5230]
Loss:[0.5184]
Loss:[0.5129]
Loss:[0.5009]
Loss:[0.4921]
Loss:[0.4855]
Loss:[0.4795]
Loss:[0.4669]
Loss:[0.4628]
Loss:[0.4511]
Loss:[0.4447]
Loss:[0.4396]
Loss:[0.4347]
Loss:[0.4294]
Loss:[0.4244]
Loss:[0.4129]
Loss:[0.4055]
Loss:[0.3925]
Loss:[0.3974]
Loss:[0.3842]
Loss:[0.3742]
Loss:[0.3732]
Loss:[0.3607]
Loss:[0.3546]
Loss:[0.3522]
Loss:[0.3391]
Loss:[0.3325]
Loss:[0.3213]
Loss:[0.3129]
Loss:[0.3155]
Loss:[0.3042]
Loss:[0.2978]
Loss:[0.2851]
Loss:[0.2800]
Loss:[0.2677]
Loss:[0.2577]
Loss:[0.2621]
Loss:[0.2491]
Loss:[0.2393]
Loss:[0.2365]
Loss:[0.2341]
Loss:[0.2169]
Loss:[0.2220]
Loss:[0.2083]
Loss:[0.2106]
Loss:[0.2047]
Loss:[0.1947]
Loss:[0.1897]
Loss:[0.1884]
Loss:[0.1841]
Loss:[0.1701]
Loss:[0.1937]
Loss:[0.1679]
Loss:[0.1671]
Loss:[0.1555]
Loss:[0.1559]
Loss:[0.1506]
Loss:[0.1393]
Loss:[0.1404]
Loss:[0.1400]
Loss:[0.1250]
Loss:[0.1257]
Loss:[0.1278]
Loss:[0.1187]
Loss:[0.1222]
Loss:[0.1197]
Loss:[0.1129]
Loss:[0.1160]
Loss:[0.1128]
Loss:[0.1090]
Loss:[0.0990]
Loss:[0.1048]
Loss:[0.0970]
Loss:[0.0965]
Loss:[0.0935]
Loss:[0.0870]
Loss:[0.0948]
Loss:[0.0910]
Loss:[0.0891]
Loss:[0.0870]
Loss:[0.0802]
Loss:[0.0796]
Loss:[0.0785]
Loss:[0.0753]
Loss:[0.0770]
Loss:[0.0726]
Loss:[0.0709]
Loss:[0.0731]
Loss:[0.0738]
Loss:[0.0691]
Loss:[0.0662]
Loss:[0.0657]
Loss:[0.0680]
Loss:[0.0594]
Loss:[0.0632]
Loss:[0.0592]
Loss:[0.0594]
Loss:[0.0563]
Loss:[0.0597]
Loss:[0.0582]
Loss:[0.0566]
Loss:[0.0576]
Loss:[0.0517]
Loss:[0.0574]
Loss:[0.0509]
Loss:[0.0511]
Loss:[0.0536]
Loss:[0.0516]
Loss:[0.0477]
Loss:[0.0474]
Loss:[0.0477]
Loss:[0.0424]
Loss:[0.0429]
Loss:[0.0453]
Loss:[0.0408]
Loss:[0.0410]
Loss:[0.0415]
Loss:[0.0417]
Loss:[0.0431]
Loss:[0.0408]
Loss:[0.0433]
Loss:[0.0425]
Loss:[0.0394]
Loss:[0.0380]
Loss:[0.0377]
Loss:[0.0412]
Loss:[0.0407]
Loss:[0.0382]
Loss:[0.0387]
Loss:[0.0345]
Loss:[0.0319]
Loss:[0.0384]
Loss:[0.0349]
Loss:[0.0353]
Loss:[0.0370]
Loss:[0.0312]
Loss:[0.0318]
Loss:[0.0304]
Loss:[0.0296]
Loss:[0.0329]
Loss:[0.0292]
Loss:[0.0300]
Loss:[0.0299]
Loss:[0.0285]
Loss:[0.0331]
Loss:[0.0320]
Loss:[0.0288]
Loss:[0.0321]
Loss:[0.0286]
Loss:[0.0295]
Loss:[0.0287]
Loss:[0.0276]
Loss:[0.0274]
Loss:[0.0276]
Loss:[0.0261]
Loss:[0.0235]
Loss:[0.0224]
Loss:[0.0267]
Loss:[0.0260]
Loss:[0.0244]
Loss:[0.0254]
Loss:[0.0285]
Loss:[0.0269]
Loss:[0.0238]
Loss:[0.0235]
Loss:[0.0222]
Loss:[0.0220]
Loss:[0.0237]
Loss:[0.0217]
Loss:[0.0251]
Loss:[0.0222]
Loss:[0.0220]
Loss:[0.0218]
Loss:[0.0224]
Loss:[0.0219]
Loss:[0.0203]
Loss:[0.0204]
Loss:[0.0214]
Loss:[0.0222]
Loss:[0.0204]
Loss:[0.0197]
Loss:[0.0223]
Loss:[0.0177]
Loss:[0.0198]
Loss:[0.0175]
Loss:[0.0198]
Loss:[0.0202]
Loss:[0.0159]
Loss:[0.0223]
Loss:[0.0185]
Loss:[0.0191]
Loss:[0.0177]
Loss:[0.0189]
Loss:[0.0209]
Loss:[0.0177]
Loss:[0.0196]
Loss:[0.0182]
Loss:[0.0168]
Loss:[0.0167]
Loss:[0.0153]
Loss:[0.0183]
Loss:[0.0161]
Loss:[0.0187]
Loss:[0.0187]
Loss:[0.0178]
Loss:[0.0172]
Loss:[0.0164]
Loss:[0.0146]
Loss:[0.0151]
Loss:[0.0158]
Loss:[0.0156]
Loss:[0.0143]
Loss:[0.0134]
Loss:[0.0144]
Loss:[0.0131]
Loss:[0.0134]
Loss:[0.0164]
Loss:[0.0169]
Loss:[0.0133]
Loss:[0.0146]
Loss:[0.0148]
Loss:[0.0136]
Loss:[0.0130]
Loss:[0.0158]
Loss:[0.0140]
Loss:[0.0142]
Loss:[0.0137]
Loss:[0.0129]
Loss:[0.0122]
Loss:[0.0148]
Loss:[0.0137]
Loss:[0.0136]
Loss:[0.0132]
Loss:[0.0113]
Loss:[0.0130]
Loss:[0.0154]
Loss:[0.0149]
Loss:[0.0138]
Loss:[0.0118]
Loss:[0.0117]
Loss:[0.0152]
Loss:[0.0116]
Loss:[0.0134]
Loss:[0.0109]
Loss:[0.0143]
Loss:[0.0140]
Loss:[0.0123]
Loss:[0.0127]
Loss:[0.0119]
Loss:[0.0123]
Loss:[0.0149]
Loss:[0.0133]
Loss:[0.0097]
Loss:[0.0104]
Loss:[0.0161]
Loss:[0.0132]
Loss:[0.0145]
Loss:[0.0130]
Loss:[0.0128]
Loss:[0.0128]
Loss:[0.0100]
Loss:[0.0112]
Loss:[0.0116]
Loss:[0.0140]
Loss:[0.0127]
Loss:[0.0114]
Loss:[0.0119]
Loss:[0.0111]
Loss:[0.0108]
Loss:[0.0136]
Loss:[0.0095]
Loss:[0.0095]
Loss:[0.0098]
Loss:[0.0094]
Loss:[0.0096]
Loss:[0.0086]
Loss:[0.0108]
Loss:[0.0095]
Loss:[0.0090]
Loss:[0.0100]
Loss:[0.0090]
Loss:[0.0104]
Loss:[0.0088]
Loss:[0.0101]
Loss:[0.0106]
Loss:[0.0083]
Loss:[0.0104]
Loss:[0.0092]
Loss:[0.0087]
Loss:[0.0086]
Loss:[0.0109]
Loss:[0.0078]
Loss:[0.0103]
Loss:[0.0097]
Loss:[0.0071]
Loss:[0.0089]
Loss:[0.0094]
Loss:[0.0083]
Loss:[0.0097]
Loss:[0.0106]
Loss:[0.0090]
Loss:[0.0088]
Loss:[0.0080]
Loss:[0.0069]
Loss:[0.0108]
Loss:[0.0083]
Loss:[0.0070]
Loss:[0.0081]
Loss:[0.0096]
Loss:[0.0078]
Loss:[0.0082]
Loss:[0.0073]
Loss:[0.0089]
Loss:[0.0059]
Loss:[0.0074]
Loss:[0.0072]
Loss:[0.0076]
Loss:[0.0096]
Loss:[0.0089]
Loss:[0.0071]
Loss:[0.0072]
Loss:[0.0070]
Loss:[0.0073]
Loss:[0.0077]
Loss:[0.0081]
Loss:[0.0070]
Loss:[0.0081]
Loss:[0.0091]
Loss:[0.0067]
Loss:[0.0080]
Loss:[0.0069]
Loss:[0.0076]
Loss:[0.0062]
Loss:[0.0064]
Early stopping!
Loading 324th epoch
acc:[0.7900]
acc:[0.7860]
acc:[0.7890]
acc:[0.7830]
acc:[0.7880]
acc:[0.7840]
acc:[0.7870]
acc:[0.7860]
acc:[0.7870]
acc:[0.7860]
acc:[0.7870]
acc:[0.7880]
acc:[0.7830]
acc:[0.7890]
acc:[0.7850]
acc:[0.7900]
acc:[0.7860]
acc:[0.7870]
acc:[0.7880]
acc:[0.7810]
acc:[0.7850]
acc:[0.7860]
acc:[0.7880]
acc:[0.7860]
acc:[0.7890]
acc:[0.7860]
acc:[0.7870]
acc:[0.7840]
acc:[0.7900]
acc:[0.7870]
acc:[0.7890]
acc:[0.7860]
acc:[0.7870]
acc:[0.7870]
acc:[0.7870]
acc:[0.7840]
acc:[0.7900]
acc:[0.7860]
acc:[0.7880]
acc:[0.7820]
acc:[0.7860]
acc:[0.7840]
acc:[0.7840]
acc:[0.7860]
acc:[0.7870]
acc:[0.7890]
acc:[0.7840]
acc:[0.7870]
acc:[0.7900]
acc:[0.7840]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7865]
Mean:[78.6500]
Std :[0.2178]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='cora', drop_percent=0.4, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6826]
Loss:[0.6667]
Loss:[0.6443]
Loss:[0.6144]
Loss:[0.5797]
Loss:[0.5463]
Loss:[0.5130]
Loss:[0.4831]
Loss:[0.4640]
Loss:[0.4464]
Loss:[0.4417]
Loss:[0.4354]
Loss:[0.4334]
Loss:[0.4343]
Loss:[0.4243]
Loss:[0.4118]
Loss:[0.4052]
Loss:[0.4005]
Loss:[0.3969]
Loss:[0.3862]
Loss:[0.3789]
Loss:[0.3710]
Loss:[0.3601]
Loss:[0.3613]
Loss:[0.3592]
Loss:[0.3508]
Loss:[0.3475]
Loss:[0.3392]
Loss:[0.3409]
Loss:[0.3344]
Loss:[0.3302]
Loss:[0.3262]
Loss:[0.3188]
Loss:[0.3166]
Loss:[0.3123]
Loss:[0.3052]
Loss:[0.3039]
Loss:[0.2936]
Loss:[0.2905]
Loss:[0.2826]
Loss:[0.2787]
Loss:[0.2801]
Loss:[0.2678]
Loss:[0.2645]
Loss:[0.2559]
Loss:[0.2454]
Loss:[0.2440]
Loss:[0.2501]
Loss:[0.2309]
Loss:[0.2249]
Loss:[0.2255]
Loss:[0.2211]
Loss:[0.2175]
Loss:[0.2060]
Loss:[0.2101]
Loss:[0.2021]
Loss:[0.1865]
Loss:[0.1888]
Loss:[0.1863]
Loss:[0.1784]
Loss:[0.1725]
Loss:[0.1712]
Loss:[0.1583]
Loss:[0.1623]
Loss:[0.1561]
Loss:[0.1538]
Loss:[0.1482]
Loss:[0.1458]
Loss:[0.1434]
Loss:[0.1364]
Loss:[0.1285]
Loss:[0.1338]
Loss:[0.1212]
Loss:[0.1206]
Loss:[0.1157]
Loss:[0.1151]
Loss:[0.1179]
Loss:[0.1088]
Loss:[0.1129]
Loss:[0.1029]
Loss:[0.1043]
Loss:[0.1036]
Loss:[0.0986]
Loss:[0.0927]
Loss:[0.0984]
Loss:[0.0874]
Loss:[0.0934]
Loss:[0.0850]
Loss:[0.0842]
Loss:[0.0758]
Loss:[0.0716]
Loss:[0.0794]
Loss:[0.0737]
Loss:[0.0771]
Loss:[0.0686]
Loss:[0.0660]
Loss:[0.0681]
Loss:[0.0687]
Loss:[0.0612]
Loss:[0.0649]
Loss:[0.0624]
Loss:[0.0643]
Loss:[0.0600]
Loss:[0.0581]
Loss:[0.0587]
Loss:[0.0589]
Loss:[0.0583]
Loss:[0.0572]
Loss:[0.0520]
Loss:[0.0468]
Loss:[0.0569]
Loss:[0.0517]
Loss:[0.0497]
Loss:[0.0503]
Loss:[0.0532]
Loss:[0.0465]
Loss:[0.0469]
Loss:[0.0476]
Loss:[0.0454]
Loss:[0.0449]
Loss:[0.0437]
Loss:[0.0433]
Loss:[0.0409]
Loss:[0.0413]
Loss:[0.0388]
Loss:[0.0412]
Loss:[0.0409]
Loss:[0.0396]
Loss:[0.0372]
Loss:[0.0362]
Loss:[0.0352]
Loss:[0.0413]
Loss:[0.0378]
Loss:[0.0321]
Loss:[0.0410]
Loss:[0.0335]
Loss:[0.0362]
Loss:[0.0388]
Loss:[0.0343]
Loss:[0.0308]
Loss:[0.0300]
Loss:[0.0298]
Loss:[0.0321]
Loss:[0.0326]
Loss:[0.0311]
Loss:[0.0310]
Loss:[0.0292]
Loss:[0.0286]
Loss:[0.0301]
Loss:[0.0270]
Loss:[0.0284]
Loss:[0.0302]
Loss:[0.0287]
Loss:[0.0242]
Loss:[0.0282]
Loss:[0.0252]
Loss:[0.0259]
Loss:[0.0272]
Loss:[0.0238]
Loss:[0.0246]
Loss:[0.0263]
Loss:[0.0278]
Loss:[0.0236]
Loss:[0.0255]
Loss:[0.0235]
Loss:[0.0222]
Loss:[0.0209]
Loss:[0.0262]
Loss:[0.0225]
Loss:[0.0197]
Loss:[0.0222]
Loss:[0.0215]
Loss:[0.0223]
Loss:[0.0202]
Loss:[0.0212]
Loss:[0.0183]
Loss:[0.0244]
Loss:[0.0226]
Loss:[0.0190]
Loss:[0.0218]
Loss:[0.0176]
Loss:[0.0173]
Loss:[0.0178]
Loss:[0.0202]
Loss:[0.0203]
Loss:[0.0175]
Loss:[0.0192]
Loss:[0.0185]
Loss:[0.0192]
Loss:[0.0181]
Loss:[0.0157]
Loss:[0.0179]
Loss:[0.0160]
Loss:[0.0181]
Loss:[0.0157]
Loss:[0.0155]
Loss:[0.0207]
Loss:[0.0151]
Loss:[0.0189]
Loss:[0.0195]
Loss:[0.0167]
Loss:[0.0157]
Loss:[0.0185]
Loss:[0.0176]
Loss:[0.0167]
Loss:[0.0145]
Loss:[0.0164]
Loss:[0.0169]
Loss:[0.0166]
Loss:[0.0157]
Loss:[0.0167]
Loss:[0.0130]
Loss:[0.0177]
Loss:[0.0195]
Loss:[0.0164]
Loss:[0.0125]
Loss:[0.0180]
Loss:[0.0120]
Loss:[0.0153]
Loss:[0.0162]
Loss:[0.0130]
Loss:[0.0142]
Loss:[0.0138]
Loss:[0.0131]
Loss:[0.0150]
Loss:[0.0127]
Loss:[0.0146]
Loss:[0.0162]
Loss:[0.0138]
Loss:[0.0123]
Loss:[0.0120]
Loss:[0.0141]
Loss:[0.0147]
Loss:[0.0151]
Loss:[0.0160]
Loss:[0.0159]
Loss:[0.0129]
Loss:[0.0145]
Early stopping!
Loading 218th epoch
acc:[0.7390]
acc:[0.7360]
acc:[0.7430]
acc:[0.7390]
acc:[0.7380]
acc:[0.7400]
acc:[0.7350]
acc:[0.7360]
acc:[0.7380]
acc:[0.7350]
acc:[0.7410]
acc:[0.7390]
acc:[0.7350]
acc:[0.7350]
acc:[0.7370]
acc:[0.7380]
acc:[0.7410]
acc:[0.7370]
acc:[0.7400]
acc:[0.7370]
acc:[0.7370]
acc:[0.7380]
acc:[0.7410]
acc:[0.7360]
acc:[0.7360]
acc:[0.7360]
acc:[0.7370]
acc:[0.7400]
acc:[0.7350]
acc:[0.7340]
acc:[0.7340]
acc:[0.7340]
acc:[0.7330]
acc:[0.7370]
acc:[0.7350]
acc:[0.7430]
acc:[0.7370]
acc:[0.7350]
acc:[0.7340]
acc:[0.7350]
acc:[0.7400]
acc:[0.7380]
acc:[0.7400]
acc:[0.7390]
acc:[0.7370]
acc:[0.7360]
acc:[0.7360]
acc:[0.7360]
acc:[0.7390]
acc:[0.7370]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7373]
Mean:[73.7280]
Std :[0.2382]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='cora', drop_percent=0.5, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6783]
Loss:[0.6562]
Loss:[0.6259]
Loss:[0.5875]
Loss:[0.5427]
Loss:[0.4968]
Loss:[0.4550]
Loss:[0.4144]
Loss:[0.3855]
Loss:[0.3622]
Loss:[0.3519]
Loss:[0.3415]
Loss:[0.3399]
Loss:[0.3327]
Loss:[0.3333]
Loss:[0.3282]
Loss:[0.3208]
Loss:[0.3217]
Loss:[0.3141]
Loss:[0.3065]
Loss:[0.3061]
Loss:[0.2866]
Loss:[0.2979]
Loss:[0.2900]
Loss:[0.2853]
Loss:[0.2688]
Loss:[0.2719]
Loss:[0.2665]
Loss:[0.2640]
Loss:[0.2627]
Loss:[0.2597]
Loss:[0.2522]
Loss:[0.2567]
Loss:[0.2544]
Loss:[0.2515]
Loss:[0.2411]
Loss:[0.2413]
Loss:[0.2441]
Loss:[0.2286]
Loss:[0.2307]
Loss:[0.2326]
Loss:[0.2317]
Loss:[0.2232]
Loss:[0.2212]
Loss:[0.2158]
Loss:[0.2157]
Loss:[0.2142]
Loss:[0.2095]
Loss:[0.1979]
Loss:[0.2052]
Loss:[0.1976]
Loss:[0.1931]
Loss:[0.1985]
Loss:[0.1896]
Loss:[0.1881]
Loss:[0.1894]
Loss:[0.1790]
Loss:[0.1731]
Loss:[0.1726]
Loss:[0.1744]
Loss:[0.1700]
Loss:[0.1684]
Loss:[0.1621]
Loss:[0.1590]
Loss:[0.1518]
Loss:[0.1481]
Loss:[0.1471]
Loss:[0.1437]
Loss:[0.1427]
Loss:[0.1406]
Loss:[0.1356]
Loss:[0.1420]
Loss:[0.1348]
Loss:[0.1240]
Loss:[0.1199]
Loss:[0.1256]
Loss:[0.1157]
Loss:[0.1184]
Loss:[0.1130]
Loss:[0.1163]
Loss:[0.1093]
Loss:[0.1067]
Loss:[0.1093]
Loss:[0.1094]
Loss:[0.1040]
Loss:[0.1056]
Loss:[0.0958]
Loss:[0.0898]
Loss:[0.0921]
Loss:[0.0895]
Loss:[0.0853]
Loss:[0.0890]
Loss:[0.0824]
Loss:[0.0885]
Loss:[0.0850]
Loss:[0.0794]
Loss:[0.0871]
Loss:[0.0739]
Loss:[0.0829]
Loss:[0.0761]
Loss:[0.0684]
Loss:[0.0706]
Loss:[0.0692]
Loss:[0.0702]
Loss:[0.0664]
Loss:[0.0648]
Loss:[0.0583]
Loss:[0.0595]
Loss:[0.0567]
Loss:[0.0620]
Loss:[0.0552]
Loss:[0.0559]
Loss:[0.0513]
Loss:[0.0521]
Loss:[0.0533]
Loss:[0.0514]
Loss:[0.0481]
Loss:[0.0518]
Loss:[0.0486]
Loss:[0.0496]
Loss:[0.0489]
Loss:[0.0407]
Loss:[0.0487]
Loss:[0.0413]
Loss:[0.0423]
Loss:[0.0406]
Loss:[0.0430]
Loss:[0.0382]
Loss:[0.0447]
Loss:[0.0367]
Loss:[0.0401]
Loss:[0.0416]
Loss:[0.0368]
Loss:[0.0385]
Loss:[0.0381]
Loss:[0.0356]
Loss:[0.0410]
Loss:[0.0356]
Loss:[0.0381]
Loss:[0.0319]
Loss:[0.0331]
Loss:[0.0331]
Loss:[0.0307]
Loss:[0.0349]
Loss:[0.0298]
Loss:[0.0317]
Loss:[0.0326]
Loss:[0.0293]
Loss:[0.0316]
Loss:[0.0311]
Loss:[0.0278]
Loss:[0.0274]
Loss:[0.0265]
Loss:[0.0297]
Loss:[0.0297]
Loss:[0.0316]
Loss:[0.0269]
Loss:[0.0283]
Loss:[0.0241]
Loss:[0.0254]
Loss:[0.0237]
Loss:[0.0231]
Loss:[0.0275]
Loss:[0.0253]
Loss:[0.0235]
Loss:[0.0208]
Loss:[0.0243]
Loss:[0.0223]
Loss:[0.0233]
Loss:[0.0202]
Loss:[0.0213]
Loss:[0.0209]
Loss:[0.0223]
Loss:[0.0221]
Loss:[0.0212]
Loss:[0.0229]
Loss:[0.0236]
Loss:[0.0225]
Loss:[0.0223]
Loss:[0.0213]
Loss:[0.0204]
Loss:[0.0205]
Loss:[0.0203]
Loss:[0.0182]
Loss:[0.0182]
Loss:[0.0171]
Loss:[0.0196]
Loss:[0.0194]
Loss:[0.0206]
Loss:[0.0197]
Loss:[0.0168]
Loss:[0.0168]
Loss:[0.0176]
Loss:[0.0178]
Loss:[0.0159]
Loss:[0.0210]
Loss:[0.0170]
Loss:[0.0153]
Loss:[0.0191]
Loss:[0.0201]
Loss:[0.0193]
Loss:[0.0140]
Loss:[0.0148]
Loss:[0.0200]
Loss:[0.0131]
Loss:[0.0163]
Loss:[0.0169]
Loss:[0.0156]
Loss:[0.0151]
Loss:[0.0134]
Loss:[0.0148]
Loss:[0.0160]
Loss:[0.0191]
Loss:[0.0152]
Loss:[0.0146]
Loss:[0.0141]
Loss:[0.0126]
Loss:[0.0104]
Loss:[0.0179]
Loss:[0.0154]
Loss:[0.0161]
Loss:[0.0135]
Loss:[0.0121]
Loss:[0.0125]
Loss:[0.0121]
Loss:[0.0132]
Loss:[0.0125]
Loss:[0.0129]
Loss:[0.0142]
Loss:[0.0119]
Loss:[0.0155]
Loss:[0.0114]
Loss:[0.0147]
Loss:[0.0136]
Loss:[0.0134]
Loss:[0.0139]
Loss:[0.0119]
Loss:[0.0129]
Early stopping!
Loading 218th epoch
acc:[0.6780]
acc:[0.6740]
acc:[0.6780]
acc:[0.6720]
acc:[0.6820]
acc:[0.6740]
acc:[0.6760]
acc:[0.6800]
acc:[0.6780]
acc:[0.6740]
acc:[0.6770]
acc:[0.6770]
acc:[0.6760]
acc:[0.6770]
acc:[0.6810]
acc:[0.6770]
acc:[0.6810]
acc:[0.6730]
acc:[0.6760]
acc:[0.6710]
acc:[0.6780]
acc:[0.6810]
acc:[0.6760]
acc:[0.6790]
acc:[0.6790]
acc:[0.6730]
acc:[0.6790]
acc:[0.6790]
acc:[0.6740]
acc:[0.6760]
acc:[0.6770]
acc:[0.6790]
acc:[0.6760]
acc:[0.6770]
acc:[0.6760]
acc:[0.6770]
acc:[0.6770]
acc:[0.6720]
acc:[0.6780]
acc:[0.6750]
acc:[0.6810]
acc:[0.6740]
acc:[0.6750]
acc:[0.6790]
acc:[0.6790]
acc:[0.6750]
acc:[0.6760]
acc:[0.6780]
acc:[0.6770]
acc:[0.6730]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.6767]
Mean:[67.6740]
Std :[0.2586]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='cora', drop_percent=0.6, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6743]
Loss:[0.6455]
Loss:[0.6074]
Loss:[0.5605]
Loss:[0.5069]
Loss:[0.4507]
Loss:[0.3980]
Loss:[0.3486]
Loss:[0.3080]
Loss:[0.2774]
Loss:[0.2566]
Loss:[0.2442]
Loss:[0.2334]
Loss:[0.2264]
Loss:[0.2317]
Loss:[0.2233]
Loss:[0.2169]
Loss:[0.2181]
Loss:[0.2134]
Loss:[0.2078]
Loss:[0.2101]
Loss:[0.2079]
Loss:[0.1946]
Loss:[0.1883]
Loss:[0.1889]
Loss:[0.1858]
Loss:[0.1794]
Loss:[0.1736]
Loss:[0.1717]
Loss:[0.1681]
Loss:[0.1712]
Loss:[0.1676]
Loss:[0.1615]
Loss:[0.1701]
Loss:[0.1608]
Loss:[0.1605]
Loss:[0.1499]
Loss:[0.1586]
Loss:[0.1499]
Loss:[0.1553]
Loss:[0.1528]
Loss:[0.1481]
Loss:[0.1471]
Loss:[0.1473]
Loss:[0.1443]
Loss:[0.1398]
Loss:[0.1426]
Loss:[0.1407]
Loss:[0.1316]
Loss:[0.1348]
Loss:[0.1362]
Loss:[0.1382]
Loss:[0.1339]
Loss:[0.1302]
Loss:[0.1392]
Loss:[0.1348]
Loss:[0.1238]
Loss:[0.1254]
Loss:[0.1259]
Loss:[0.1170]
Loss:[0.1233]
Loss:[0.1186]
Loss:[0.1255]
Loss:[0.1144]
Loss:[0.1209]
Loss:[0.1220]
Loss:[0.1161]
Loss:[0.1144]
Loss:[0.1148]
Loss:[0.1154]
Loss:[0.1171]
Loss:[0.1137]
Loss:[0.1075]
Loss:[0.1059]
Loss:[0.1109]
Loss:[0.1050]
Loss:[0.1049]
Loss:[0.1034]
Loss:[0.1063]
Loss:[0.1013]
Loss:[0.1012]
Loss:[0.0983]
Loss:[0.1062]
Loss:[0.0917]
Loss:[0.0949]
Loss:[0.0920]
Loss:[0.0940]
Loss:[0.0900]
Loss:[0.0892]
Loss:[0.0913]
Loss:[0.0945]
Loss:[0.0801]
Loss:[0.0864]
Loss:[0.0889]
Loss:[0.0827]
Loss:[0.0794]
Loss:[0.0849]
Loss:[0.0795]
Loss:[0.0856]
Loss:[0.0800]
Loss:[0.0722]
Loss:[0.0840]
Loss:[0.0696]
Loss:[0.0706]
Loss:[0.0707]
Loss:[0.0714]
Loss:[0.0699]
Loss:[0.0671]
Loss:[0.0676]
Loss:[0.0672]
Loss:[0.0624]
Loss:[0.0578]
Loss:[0.0700]
Loss:[0.0605]
Loss:[0.0652]
Loss:[0.0573]
Loss:[0.0572]
Loss:[0.0606]
Loss:[0.0645]
Loss:[0.0576]
Loss:[0.0552]
Loss:[0.0632]
Loss:[0.0528]
Loss:[0.0567]
Loss:[0.0460]
Loss:[0.0558]
Loss:[0.0521]
Loss:[0.0474]
Loss:[0.0568]
Loss:[0.0497]
Loss:[0.0500]
Loss:[0.0548]
Loss:[0.0480]
Loss:[0.0477]
Loss:[0.0460]
Loss:[0.0423]
Loss:[0.0372]
Loss:[0.0438]
Loss:[0.0508]
Loss:[0.0430]
Loss:[0.0424]
Loss:[0.0405]
Loss:[0.0387]
Loss:[0.0387]
Loss:[0.0374]
Loss:[0.0421]
Loss:[0.0378]
Loss:[0.0342]
Loss:[0.0416]
Loss:[0.0359]
Loss:[0.0348]
Loss:[0.0393]
Loss:[0.0386]
Loss:[0.0399]
Loss:[0.0355]
Loss:[0.0332]
Loss:[0.0349]
Loss:[0.0291]
Loss:[0.0342]
Loss:[0.0317]
Loss:[0.0307]
Loss:[0.0305]
Loss:[0.0311]
Loss:[0.0299]
Loss:[0.0389]
Loss:[0.0287]
Loss:[0.0304]
Loss:[0.0272]
Loss:[0.0339]
Loss:[0.0334]
Loss:[0.0280]
Loss:[0.0250]
Loss:[0.0296]
Loss:[0.0309]
Loss:[0.0324]
Loss:[0.0272]
Loss:[0.0281]
Loss:[0.0249]
Loss:[0.0213]
Loss:[0.0218]
Loss:[0.0299]
Loss:[0.0319]
Loss:[0.0246]
Loss:[0.0243]
Loss:[0.0266]
Loss:[0.0149]
Loss:[0.0246]
Loss:[0.0230]
Loss:[0.0258]
Loss:[0.0195]
Loss:[0.0228]
Loss:[0.0237]
Loss:[0.0231]
Loss:[0.0206]
Loss:[0.0198]
Loss:[0.0220]
Loss:[0.0234]
Loss:[0.0214]
Loss:[0.0257]
Loss:[0.0206]
Loss:[0.0170]
Loss:[0.0208]
Loss:[0.0187]
Loss:[0.0185]
Loss:[0.0223]
Loss:[0.0151]
Early stopping!
Loading 186th epoch
acc:[0.6380]
acc:[0.6390]
acc:[0.6360]
acc:[0.6420]
acc:[0.6440]
acc:[0.6410]
acc:[0.6400]
acc:[0.6410]
acc:[0.6410]
acc:[0.6410]
acc:[0.6410]
acc:[0.6390]
acc:[0.6400]
acc:[0.6410]
acc:[0.6430]
acc:[0.6450]
acc:[0.6410]
acc:[0.6420]
acc:[0.6340]
acc:[0.6380]
acc:[0.6380]
acc:[0.6400]
acc:[0.6370]
acc:[0.6410]
acc:[0.6410]
acc:[0.6410]
acc:[0.6380]
acc:[0.6420]
acc:[0.6430]
acc:[0.6380]
acc:[0.6430]
acc:[0.6440]
acc:[0.6360]
acc:[0.6390]
acc:[0.6340]
acc:[0.6390]
acc:[0.6420]
acc:[0.6400]
acc:[0.6420]
acc:[0.6380]
acc:[0.6400]
acc:[0.6420]
acc:[0.6430]
acc:[0.6400]
acc:[0.6380]
acc:[0.6400]
acc:[0.6360]
acc:[0.6370]
acc:[0.6380]
acc:[0.6390]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.6399]
Mean:[63.9920]
Std :[0.2481]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='citeseer', drop_percent=0.02, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6923]
Loss:[0.6897]
Loss:[0.6866]
Loss:[0.6820]
Loss:[0.6783]
Loss:[0.6713]
Loss:[0.6652]
Loss:[0.6565]
Loss:[0.6488]
Loss:[0.6386]
Loss:[0.6274]
Loss:[0.6176]
Loss:[0.6037]
Loss:[0.5919]
Loss:[0.5790]
Loss:[0.5616]
Loss:[0.5478]
Loss:[0.5331]
Loss:[0.5190]
Loss:[0.5000]
Loss:[0.4856]
Loss:[0.4717]
Loss:[0.4556]
Loss:[0.4391]
Loss:[0.4222]
Loss:[0.4108]
Loss:[0.3967]
Loss:[0.3752]
Loss:[0.3625]
Loss:[0.3559]
Loss:[0.3393]
Loss:[0.3189]
Loss:[0.3129]
Loss:[0.3041]
Loss:[0.2926]
Loss:[0.2816]
Loss:[0.2709]
Loss:[0.2629]
Loss:[0.2588]
Loss:[0.2520]
Loss:[0.2391]
Loss:[0.2362]
Loss:[0.2249]
Loss:[0.2230]
Loss:[0.2161]
Loss:[0.2112]
Loss:[0.1971]
Loss:[0.1910]
Loss:[0.1895]
Loss:[0.1899]
Loss:[0.1801]
Loss:[0.1819]
Loss:[0.1770]
Loss:[0.1764]
Loss:[0.1652]
Loss:[0.1588]
Loss:[0.1714]
Loss:[0.1601]
Loss:[0.1488]
Loss:[0.1475]
Loss:[0.1571]
Loss:[0.1478]
Loss:[0.1453]
Loss:[0.1389]
Loss:[0.1438]
Loss:[0.1368]
Loss:[0.1329]
Loss:[0.1362]
Loss:[0.1360]
Loss:[0.1391]
Loss:[0.1356]
Loss:[0.1300]
Loss:[0.1353]
Loss:[0.1309]
Loss:[0.1308]
Loss:[0.1331]
Loss:[0.1285]
Loss:[0.1229]
Loss:[0.1221]
Loss:[0.1272]
Loss:[0.1293]
Loss:[0.1224]
Loss:[0.1218]
Loss:[0.1186]
Loss:[0.1149]
Loss:[0.1212]
Loss:[0.1207]
Loss:[0.1086]
Loss:[0.1132]
Loss:[0.1099]
Loss:[0.1141]
Loss:[0.1162]
Loss:[0.1117]
Loss:[0.1112]
Loss:[0.1118]
Loss:[0.1066]
Loss:[0.1187]
Loss:[0.1109]
Loss:[0.1081]
Loss:[0.1034]
Loss:[0.1059]
Loss:[0.1034]
Loss:[0.1062]
Loss:[0.1006]
Loss:[0.1104]
Loss:[0.1045]
Loss:[0.1029]
Loss:[0.1070]
Loss:[0.1004]
Loss:[0.0945]
Loss:[0.1001]
Loss:[0.1049]
Loss:[0.0991]
Loss:[0.0957]
Loss:[0.0991]
Loss:[0.0996]
Loss:[0.0949]
Loss:[0.1045]
Loss:[0.1002]
Loss:[0.1052]
Loss:[0.0996]
Loss:[0.0953]
Loss:[0.0969]
Loss:[0.1016]
Loss:[0.1044]
Loss:[0.0926]
Loss:[0.0998]
Loss:[0.0996]
Loss:[0.0988]
Loss:[0.0954]
Loss:[0.1018]
Loss:[0.0950]
Loss:[0.0946]
Loss:[0.0983]
Loss:[0.0931]
Loss:[0.0956]
Loss:[0.1010]
Loss:[0.0897]
Loss:[0.1025]
Loss:[0.0955]
Loss:[0.0995]
Loss:[0.0936]
Loss:[0.0909]
Loss:[0.0913]
Loss:[0.0968]
Loss:[0.0910]
Loss:[0.0887]
Loss:[0.0978]
Loss:[0.0922]
Loss:[0.0947]
Loss:[0.0904]
Loss:[0.0964]
Loss:[0.0895]
Loss:[0.0907]
Loss:[0.0899]
Loss:[0.0947]
Loss:[0.0917]
Loss:[0.0904]
Loss:[0.0967]
Loss:[0.0922]
Loss:[0.0856]
Loss:[0.0880]
Loss:[0.0862]
Loss:[0.0908]
Loss:[0.0836]
Loss:[0.0920]
Loss:[0.0844]
Loss:[0.0971]
Loss:[0.0897]
Loss:[0.0929]
Loss:[0.0950]
Loss:[0.0887]
Loss:[0.0859]
Loss:[0.0902]
Loss:[0.0860]
Loss:[0.0912]
Loss:[0.0850]
Loss:[0.0875]
Loss:[0.0854]
Loss:[0.0814]
Loss:[0.0826]
Loss:[0.0859]
Loss:[0.0819]
Loss:[0.0850]
Loss:[0.0829]
Loss:[0.0817]
Loss:[0.0830]
Loss:[0.0883]
Loss:[0.0835]
Loss:[0.0883]
Loss:[0.0849]
Loss:[0.0864]
Loss:[0.0932]
Loss:[0.0905]
Loss:[0.0823]
Loss:[0.0899]
Loss:[0.0836]
Loss:[0.0881]
Loss:[0.0847]
Loss:[0.0866]
Early stopping!
Loading 180th epoch
acc:[0.7190]
acc:[0.7170]
acc:[0.7190]
acc:[0.7180]
acc:[0.7190]
acc:[0.7120]
acc:[0.7190]
acc:[0.7210]
acc:[0.7180]
acc:[0.7210]
acc:[0.7150]
acc:[0.7220]
acc:[0.7160]
acc:[0.7220]
acc:[0.7230]
acc:[0.7170]
acc:[0.7200]
acc:[0.7220]
acc:[0.7190]
acc:[0.7180]
acc:[0.7130]
acc:[0.7180]
acc:[0.7150]
acc:[0.7210]
acc:[0.7170]
acc:[0.7190]
acc:[0.7200]
acc:[0.7220]
acc:[0.7170]
acc:[0.7200]
acc:[0.7170]
acc:[0.7200]
acc:[0.7190]
acc:[0.7170]
acc:[0.7160]
acc:[0.7190]
acc:[0.7200]
acc:[0.7190]
acc:[0.7190]
acc:[0.7220]
acc:[0.7140]
acc:[0.7190]
acc:[0.7180]
acc:[0.7210]
acc:[0.7180]
acc:[0.7130]
acc:[0.7200]
acc:[0.7190]
acc:[0.7170]
acc:[0.7160]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7184]
Mean:[71.8440]
Std :[0.2500]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='citeseer', drop_percent=0.05, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6922]
Loss:[0.6900]
Loss:[0.6861]
Loss:[0.6819]
Loss:[0.6765]
Loss:[0.6702]
Loss:[0.6623]
Loss:[0.6546]
Loss:[0.6442]
Loss:[0.6347]
Loss:[0.6230]
Loss:[0.6103]
Loss:[0.5994]
Loss:[0.5857]
Loss:[0.5713]
Loss:[0.5577]
Loss:[0.5447]
Loss:[0.5312]
Loss:[0.5137]
Loss:[0.4991]
Loss:[0.4853]
Loss:[0.4720]
Loss:[0.4542]
Loss:[0.4404]
Loss:[0.4252]
Loss:[0.4108]
Loss:[0.3974]
Loss:[0.3749]
Loss:[0.3626]
Loss:[0.3559]
Loss:[0.3403]
Loss:[0.3280]
Loss:[0.3177]
Loss:[0.3022]
Loss:[0.2969]
Loss:[0.2878]
Loss:[0.2777]
Loss:[0.2629]
Loss:[0.2611]
Loss:[0.2554]
Loss:[0.2428]
Loss:[0.2395]
Loss:[0.2275]
Loss:[0.2205]
Loss:[0.2167]
Loss:[0.2111]
Loss:[0.2008]
Loss:[0.1945]
Loss:[0.1902]
Loss:[0.1864]
Loss:[0.1801]
Loss:[0.1837]
Loss:[0.1775]
Loss:[0.1710]
Loss:[0.1642]
Loss:[0.1602]
Loss:[0.1666]
Loss:[0.1577]
Loss:[0.1504]
Loss:[0.1498]
Loss:[0.1539]
Loss:[0.1460]
Loss:[0.1454]
Loss:[0.1398]
Loss:[0.1425]
Loss:[0.1372]
Loss:[0.1338]
Loss:[0.1360]
Loss:[0.1362]
Loss:[0.1394]
Loss:[0.1373]
Loss:[0.1319]
Loss:[0.1313]
Loss:[0.1310]
Loss:[0.1350]
Loss:[0.1288]
Loss:[0.1286]
Loss:[0.1265]
Loss:[0.1185]
Loss:[0.1340]
Loss:[0.1394]
Loss:[0.1165]
Loss:[0.1319]
Loss:[0.1169]
Loss:[0.1202]
Loss:[0.1207]
Loss:[0.1108]
Loss:[0.1112]
Loss:[0.1095]
Loss:[0.1152]
Loss:[0.1135]
Loss:[0.1118]
Loss:[0.1106]
Loss:[0.1070]
Loss:[0.1101]
Loss:[0.1006]
Loss:[0.1132]
Loss:[0.1060]
Loss:[0.1064]
Loss:[0.0987]
Loss:[0.0999]
Loss:[0.1018]
Loss:[0.0976]
Loss:[0.0978]
Loss:[0.1055]
Loss:[0.0997]
Loss:[0.0991]
Loss:[0.1012]
Loss:[0.0984]
Loss:[0.0912]
Loss:[0.0939]
Loss:[0.1022]
Loss:[0.0923]
Loss:[0.0914]
Loss:[0.0972]
Loss:[0.0974]
Loss:[0.0925]
Loss:[0.1003]
Loss:[0.0921]
Loss:[0.1029]
Loss:[0.0963]
Loss:[0.0889]
Loss:[0.0944]
Loss:[0.0975]
Loss:[0.0979]
Loss:[0.0883]
Loss:[0.0973]
Loss:[0.0936]
Loss:[0.0944]
Loss:[0.0912]
Loss:[0.0956]
Loss:[0.0887]
Loss:[0.0880]
Loss:[0.0880]
Loss:[0.0896]
Loss:[0.0912]
Loss:[0.0959]
Loss:[0.0870]
Loss:[0.0908]
Loss:[0.0917]
Loss:[0.0945]
Loss:[0.0872]
Loss:[0.0851]
Loss:[0.0826]
Loss:[0.0883]
Loss:[0.0899]
Loss:[0.0793]
Loss:[0.0907]
Loss:[0.0881]
Loss:[0.0905]
Loss:[0.0829]
Loss:[0.0899]
Loss:[0.0799]
Loss:[0.0875]
Loss:[0.0880]
Loss:[0.0861]
Loss:[0.0861]
Loss:[0.0834]
Loss:[0.0893]
Loss:[0.0833]
Loss:[0.0776]
Loss:[0.0819]
Loss:[0.0839]
Loss:[0.0831]
Loss:[0.0831]
Loss:[0.0829]
Loss:[0.0786]
Loss:[0.0883]
Loss:[0.0843]
Loss:[0.0878]
Loss:[0.0892]
Loss:[0.0777]
Loss:[0.0832]
Loss:[0.0806]
Loss:[0.0804]
Loss:[0.0859]
Loss:[0.0807]
Loss:[0.0797]
Loss:[0.0794]
Loss:[0.0745]
Loss:[0.0781]
Loss:[0.0777]
Loss:[0.0782]
Loss:[0.0803]
Loss:[0.0773]
Loss:[0.0767]
Loss:[0.0778]
Loss:[0.0842]
Loss:[0.0770]
Loss:[0.0785]
Loss:[0.0792]
Loss:[0.0769]
Loss:[0.0889]
Loss:[0.0814]
Loss:[0.0757]
Loss:[0.0845]
Loss:[0.0757]
Loss:[0.0820]
Loss:[0.0783]
Loss:[0.0807]
Early stopping!
Loading 180th epoch
acc:[0.7130]
acc:[0.7180]
acc:[0.7140]
acc:[0.7160]
acc:[0.7140]
acc:[0.7140]
acc:[0.7150]
acc:[0.7150]
acc:[0.7160]
acc:[0.7150]
acc:[0.7140]
acc:[0.7120]
acc:[0.7140]
acc:[0.7170]
acc:[0.7140]
acc:[0.7130]
acc:[0.7150]
acc:[0.7170]
acc:[0.7130]
acc:[0.7150]
acc:[0.7140]
acc:[0.7150]
acc:[0.7170]
acc:[0.7160]
acc:[0.7150]
acc:[0.7140]
acc:[0.7140]
acc:[0.7140]
acc:[0.7160]
acc:[0.7120]
acc:[0.7150]
acc:[0.7160]
acc:[0.7140]
acc:[0.7160]
acc:[0.7150]
acc:[0.7130]
acc:[0.7140]
acc:[0.7130]
acc:[0.7140]
acc:[0.7160]
acc:[0.7130]
acc:[0.7170]
acc:[0.7140]
acc:[0.7120]
acc:[0.7150]
acc:[0.7170]
acc:[0.7170]
acc:[0.7140]
acc:[0.7140]
acc:[0.7140]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7147]
Mean:[71.4680]
Std :[0.1449]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='citeseer', drop_percent=0.08, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6916]
Loss:[0.6894]
Loss:[0.6840]
Loss:[0.6803]
Loss:[0.6720]
Loss:[0.6660]
Loss:[0.6552]
Loss:[0.6487]
Loss:[0.6359]
Loss:[0.6263]
Loss:[0.6163]
Loss:[0.6027]
Loss:[0.5952]
Loss:[0.5832]
Loss:[0.5706]
Loss:[0.5593]
Loss:[0.5487]
Loss:[0.5323]
Loss:[0.5207]
Loss:[0.5089]
Loss:[0.4937]
Loss:[0.4777]
Loss:[0.4678]
Loss:[0.4553]
Loss:[0.4375]
Loss:[0.4216]
Loss:[0.4075]
Loss:[0.3952]
Loss:[0.3828]
Loss:[0.3682]
Loss:[0.3522]
Loss:[0.3396]
Loss:[0.3347]
Loss:[0.3253]
Loss:[0.3176]
Loss:[0.2998]
Loss:[0.2891]
Loss:[0.2833]
Loss:[0.2777]
Loss:[0.2628]
Loss:[0.2548]
Loss:[0.2505]
Loss:[0.2438]
Loss:[0.2351]
Loss:[0.2248]
Loss:[0.2252]
Loss:[0.2145]
Loss:[0.2086]
Loss:[0.1996]
Loss:[0.1958]
Loss:[0.1925]
Loss:[0.1906]
Loss:[0.1822]
Loss:[0.1797]
Loss:[0.1719]
Loss:[0.1678]
Loss:[0.1706]
Loss:[0.1650]
Loss:[0.1570]
Loss:[0.1553]
Loss:[0.1561]
Loss:[0.1539]
Loss:[0.1496]
Loss:[0.1460]
Loss:[0.1454]
Loss:[0.1403]
Loss:[0.1384]
Loss:[0.1378]
Loss:[0.1370]
Loss:[0.1404]
Loss:[0.1372]
Loss:[0.1292]
Loss:[0.1383]
Loss:[0.1343]
Loss:[0.1304]
Loss:[0.1296]
Loss:[0.1329]
Loss:[0.1261]
Loss:[0.1225]
Loss:[0.1341]
Loss:[0.1363]
Loss:[0.1152]
Loss:[0.1309]
Loss:[0.1209]
Loss:[0.1177]
Loss:[0.1265]
Loss:[0.1138]
Loss:[0.1137]
Loss:[0.1133]
Loss:[0.1113]
Loss:[0.1138]
Loss:[0.1075]
Loss:[0.1140]
Loss:[0.1073]
Loss:[0.1037]
Loss:[0.1017]
Loss:[0.1102]
Loss:[0.1077]
Loss:[0.1015]
Loss:[0.0994]
Loss:[0.1032]
Loss:[0.0969]
Loss:[0.0989]
Loss:[0.0980]
Loss:[0.1069]
Loss:[0.0950]
Loss:[0.0981]
Loss:[0.0995]
Loss:[0.0960]
Loss:[0.0893]
Loss:[0.0977]
Loss:[0.0965]
Loss:[0.0940]
Loss:[0.0893]
Loss:[0.1002]
Loss:[0.0968]
Loss:[0.0900]
Loss:[0.1022]
Loss:[0.0925]
Loss:[0.1020]
Loss:[0.0950]
Loss:[0.0956]
Loss:[0.0928]
Loss:[0.0949]
Loss:[0.0999]
Loss:[0.0834]
Loss:[0.0923]
Loss:[0.0887]
Loss:[0.0903]
Loss:[0.0884]
Loss:[0.0901]
Loss:[0.0839]
Loss:[0.0854]
Loss:[0.0896]
Loss:[0.0877]
Loss:[0.0859]
Loss:[0.0905]
Loss:[0.0830]
Loss:[0.0938]
Loss:[0.0859]
Loss:[0.0891]
Loss:[0.0874]
Loss:[0.0845]
Loss:[0.0805]
Loss:[0.0869]
Loss:[0.0865]
Loss:[0.0807]
Loss:[0.0823]
Loss:[0.0865]
Loss:[0.0877]
Loss:[0.0831]
Loss:[0.0850]
Loss:[0.0807]
Loss:[0.0868]
Loss:[0.0844]
Loss:[0.0837]
Loss:[0.0788]
Loss:[0.0811]
Loss:[0.0880]
Loss:[0.0846]
Loss:[0.0773]
Loss:[0.0798]
Loss:[0.0749]
Loss:[0.0803]
Loss:[0.0772]
Loss:[0.0772]
Loss:[0.0788]
Loss:[0.0835]
Loss:[0.0809]
Loss:[0.0813]
Loss:[0.0828]
Loss:[0.0757]
Loss:[0.0781]
Loss:[0.0767]
Loss:[0.0726]
Loss:[0.0844]
Loss:[0.0739]
Loss:[0.0771]
Loss:[0.0763]
Loss:[0.0727]
Loss:[0.0717]
Loss:[0.0771]
Loss:[0.0742]
Loss:[0.0749]
Loss:[0.0754]
Loss:[0.0725]
Loss:[0.0723]
Loss:[0.0800]
Loss:[0.0729]
Loss:[0.0760]
Loss:[0.0765]
Loss:[0.0728]
Loss:[0.0825]
Loss:[0.0752]
Loss:[0.0672]
Loss:[0.0795]
Loss:[0.0739]
Loss:[0.0788]
Loss:[0.0719]
Loss:[0.0773]
Loss:[0.0747]
Loss:[0.0755]
Loss:[0.0706]
Loss:[0.0736]
Loss:[0.0789]
Loss:[0.0698]
Loss:[0.0703]
Loss:[0.0731]
Loss:[0.0727]
Loss:[0.0687]
Loss:[0.0649]
Loss:[0.0695]
Loss:[0.0682]
Loss:[0.0733]
Loss:[0.0714]
Loss:[0.0759]
Loss:[0.0698]
Loss:[0.0693]
Loss:[0.0721]
Loss:[0.0726]
Loss:[0.0702]
Loss:[0.0663]
Loss:[0.0717]
Loss:[0.0660]
Loss:[0.0783]
Loss:[0.0666]
Loss:[0.0668]
Loss:[0.0753]
Loss:[0.0720]
Loss:[0.0665]
Loss:[0.0678]
Early stopping!
Loading 211th epoch
acc:[0.7100]
acc:[0.7140]
acc:[0.7140]
acc:[0.7100]
acc:[0.7080]
acc:[0.7090]
acc:[0.7100]
acc:[0.7100]
acc:[0.7120]
acc:[0.7110]
acc:[0.7070]
acc:[0.7130]
acc:[0.7100]
acc:[0.7120]
acc:[0.7090]
acc:[0.7100]
acc:[0.7100]
acc:[0.7140]
acc:[0.7100]
acc:[0.7100]
acc:[0.7130]
acc:[0.7080]
acc:[0.7110]
acc:[0.7120]
acc:[0.7100]
acc:[0.7130]
acc:[0.7090]
acc:[0.7140]
acc:[0.7100]
acc:[0.7110]
acc:[0.7100]
acc:[0.7110]
acc:[0.7140]
acc:[0.7110]
acc:[0.7120]
acc:[0.7140]
acc:[0.7090]
acc:[0.7110]
acc:[0.7100]
acc:[0.7140]
acc:[0.7110]
acc:[0.7100]
acc:[0.7120]
acc:[0.7090]
acc:[0.7100]
acc:[0.7100]
acc:[0.7120]
acc:[0.7130]
acc:[0.7140]
acc:[0.7110]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7110]
Mean:[71.1040]
Std :[0.1829]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='citeseer', drop_percent=0.1, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6913]
Loss:[0.6877]
Loss:[0.6817]
Loss:[0.6753]
Loss:[0.6663]
Loss:[0.6562]
Loss:[0.6461]
Loss:[0.6351]
Loss:[0.6248]
Loss:[0.6152]
Loss:[0.6091]
Loss:[0.5948]
Loss:[0.5880]
Loss:[0.5806]
Loss:[0.5666]
Loss:[0.5574]
Loss:[0.5469]
Loss:[0.5344]
Loss:[0.5245]
Loss:[0.5120]
Loss:[0.4963]
Loss:[0.4856]
Loss:[0.4743]
Loss:[0.4590]
Loss:[0.4463]
Loss:[0.4343]
Loss:[0.4210]
Loss:[0.4036]
Loss:[0.3901]
Loss:[0.3804]
Loss:[0.3693]
Loss:[0.3585]
Loss:[0.3453]
Loss:[0.3292]
Loss:[0.3201]
Loss:[0.3129]
Loss:[0.3071]
Loss:[0.2927]
Loss:[0.2852]
Loss:[0.2733]
Loss:[0.2656]
Loss:[0.2594]
Loss:[0.2497]
Loss:[0.2427]
Loss:[0.2317]
Loss:[0.2292]
Loss:[0.2169]
Loss:[0.2108]
Loss:[0.2082]
Loss:[0.2020]
Loss:[0.1917]
Loss:[0.1959]
Loss:[0.1906]
Loss:[0.1901]
Loss:[0.1748]
Loss:[0.1707]
Loss:[0.1747]
Loss:[0.1711]
Loss:[0.1584]
Loss:[0.1586]
Loss:[0.1606]
Loss:[0.1637]
Loss:[0.1489]
Loss:[0.1442]
Loss:[0.1476]
Loss:[0.1423]
Loss:[0.1400]
Loss:[0.1354]
Loss:[0.1385]
Loss:[0.1411]
Loss:[0.1345]
Loss:[0.1279]
Loss:[0.1375]
Loss:[0.1316]
Loss:[0.1247]
Loss:[0.1289]
Loss:[0.1271]
Loss:[0.1182]
Loss:[0.1192]
Loss:[0.1242]
Loss:[0.1267]
Loss:[0.1168]
Loss:[0.1208]
Loss:[0.1169]
Loss:[0.1116]
Loss:[0.1159]
Loss:[0.1105]
Loss:[0.1079]
Loss:[0.1115]
Loss:[0.1065]
Loss:[0.1067]
Loss:[0.1045]
Loss:[0.1058]
Loss:[0.1024]
Loss:[0.0983]
Loss:[0.0984]
Loss:[0.1071]
Loss:[0.1031]
Loss:[0.0971]
Loss:[0.0956]
Loss:[0.0968]
Loss:[0.0958]
Loss:[0.0955]
Loss:[0.0952]
Loss:[0.0990]
Loss:[0.0905]
Loss:[0.0931]
Loss:[0.0971]
Loss:[0.0907]
Loss:[0.0854]
Loss:[0.0887]
Loss:[0.0932]
Loss:[0.0890]
Loss:[0.0831]
Loss:[0.0878]
Loss:[0.0919]
Loss:[0.0860]
Loss:[0.0926]
Loss:[0.0837]
Loss:[0.0926]
Loss:[0.0893]
Loss:[0.0862]
Loss:[0.0877]
Loss:[0.0852]
Loss:[0.0920]
Loss:[0.0773]
Loss:[0.0877]
Loss:[0.0810]
Loss:[0.0845]
Loss:[0.0801]
Loss:[0.0865]
Loss:[0.0794]
Loss:[0.0812]
Loss:[0.0845]
Loss:[0.0830]
Loss:[0.0829]
Loss:[0.0814]
Loss:[0.0846]
Loss:[0.0943]
Loss:[0.0794]
Loss:[0.0889]
Loss:[0.0834]
Loss:[0.0768]
Loss:[0.0802]
Loss:[0.0807]
Loss:[0.0806]
Loss:[0.0776]
Loss:[0.0773]
Loss:[0.0794]
Loss:[0.0813]
Loss:[0.0749]
Loss:[0.0786]
Loss:[0.0746]
Loss:[0.0766]
Loss:[0.0742]
Loss:[0.0771]
Loss:[0.0766]
Loss:[0.0730]
Loss:[0.0815]
Loss:[0.0757]
Loss:[0.0685]
Loss:[0.0708]
Loss:[0.0678]
Loss:[0.0723]
Loss:[0.0721]
Loss:[0.0670]
Loss:[0.0681]
Loss:[0.0748]
Loss:[0.0679]
Loss:[0.0708]
Loss:[0.0738]
Loss:[0.0675]
Loss:[0.0705]
Loss:[0.0664]
Loss:[0.0665]
Loss:[0.0750]
Loss:[0.0657]
Loss:[0.0701]
Loss:[0.0662]
Loss:[0.0612]
Loss:[0.0628]
Loss:[0.0611]
Loss:[0.0650]
Loss:[0.0661]
Loss:[0.0675]
Loss:[0.0601]
Loss:[0.0647]
Loss:[0.0677]
Loss:[0.0630]
Loss:[0.0638]
Loss:[0.0638]
Loss:[0.0632]
Loss:[0.0709]
Loss:[0.0645]
Loss:[0.0604]
Loss:[0.0680]
Loss:[0.0627]
Loss:[0.0674]
Loss:[0.0591]
Loss:[0.0643]
Loss:[0.0629]
Loss:[0.0631]
Loss:[0.0619]
Loss:[0.0628]
Loss:[0.0688]
Loss:[0.0584]
Loss:[0.0627]
Loss:[0.0599]
Loss:[0.0636]
Loss:[0.0581]
Loss:[0.0565]
Loss:[0.0590]
Loss:[0.0600]
Loss:[0.0622]
Loss:[0.0592]
Loss:[0.0602]
Loss:[0.0589]
Loss:[0.0588]
Loss:[0.0585]
Loss:[0.0605]
Loss:[0.0578]
Loss:[0.0541]
Loss:[0.0605]
Loss:[0.0532]
Loss:[0.0657]
Loss:[0.0524]
Loss:[0.0573]
Loss:[0.0633]
Loss:[0.0595]
Loss:[0.0592]
Loss:[0.0559]
Loss:[0.0553]
Loss:[0.0568]
Loss:[0.0537]
Loss:[0.0554]
Loss:[0.0501]
Loss:[0.0476]
Loss:[0.0558]
Loss:[0.0524]
Loss:[0.0516]
Loss:[0.0532]
Loss:[0.0570]
Loss:[0.0597]
Loss:[0.0549]
Loss:[0.0497]
Loss:[0.0468]
Loss:[0.0500]
Loss:[0.0555]
Loss:[0.0538]
Loss:[0.0460]
Loss:[0.0520]
Loss:[0.0493]
Loss:[0.0541]
Loss:[0.0502]
Loss:[0.0472]
Loss:[0.0536]
Loss:[0.0524]
Loss:[0.0466]
Loss:[0.0489]
Loss:[0.0470]
Loss:[0.0544]
Loss:[0.0534]
Loss:[0.0542]
Loss:[0.0490]
Loss:[0.0452]
Loss:[0.0436]
Loss:[0.0475]
Loss:[0.0461]
Loss:[0.0511]
Loss:[0.0513]
Loss:[0.0501]
Loss:[0.0471]
Loss:[0.0427]
Loss:[0.0481]
Loss:[0.0494]
Loss:[0.0437]
Loss:[0.0437]
Loss:[0.0495]
Loss:[0.0477]
Loss:[0.0457]
Loss:[0.0481]
Loss:[0.0455]
Loss:[0.0460]
Loss:[0.0446]
Loss:[0.0445]
Loss:[0.0466]
Loss:[0.0466]
Loss:[0.0419]
Loss:[0.0478]
Loss:[0.0419]
Loss:[0.0440]
Loss:[0.0383]
Loss:[0.0340]
Loss:[0.0451]
Loss:[0.0400]
Loss:[0.0416]
Loss:[0.0410]
Loss:[0.0407]
Loss:[0.0425]
Loss:[0.0407]
Loss:[0.0420]
Loss:[0.0475]
Loss:[0.0380]
Loss:[0.0397]
Loss:[0.0424]
Loss:[0.0410]
Loss:[0.0397]
Loss:[0.0374]
Loss:[0.0439]
Loss:[0.0382]
Loss:[0.0372]
Loss:[0.0488]
Loss:[0.0419]
Early stopping!
Loading 293th epoch
acc:[0.7100]
acc:[0.7060]
acc:[0.7090]
acc:[0.7090]
acc:[0.7090]
acc:[0.7070]
acc:[0.7090]
acc:[0.7080]
acc:[0.7100]
acc:[0.7080]
acc:[0.7100]
acc:[0.7090]
acc:[0.7100]
acc:[0.7090]
acc:[0.7050]
acc:[0.7080]
acc:[0.7080]
acc:[0.7070]
acc:[0.7070]
acc:[0.7090]
acc:[0.7070]
acc:[0.7110]
acc:[0.7080]
acc:[0.7100]
acc:[0.7080]
acc:[0.7080]
acc:[0.7060]
acc:[0.7070]
acc:[0.7080]
acc:[0.7100]
acc:[0.7090]
acc:[0.7090]
acc:[0.7080]
acc:[0.7070]
acc:[0.7080]
acc:[0.7090]
acc:[0.7070]
acc:[0.7080]
acc:[0.7100]
acc:[0.7090]
acc:[0.7100]
acc:[0.7060]
acc:[0.7110]
acc:[0.7090]
acc:[0.7090]
acc:[0.7110]
acc:[0.7080]
acc:[0.7060]
acc:[0.7090]
acc:[0.7070]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7084]
Mean:[70.8400]
Std :[0.1414]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='citeseer', drop_percent=0.6, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6821]
Loss:[0.6571]
Loss:[0.6227]
Loss:[0.5802]
Loss:[0.5313]
Loss:[0.4786]
Loss:[0.4254]
Loss:[0.3761]
Loss:[0.3328]
Loss:[0.2982]
Loss:[0.2719]
Loss:[0.2538]
Loss:[0.2405]
Loss:[0.2305]
Loss:[0.2216]
Loss:[0.2148]
Loss:[0.2067]
Loss:[0.2028]
Loss:[0.1945]
Loss:[0.1963]
Loss:[0.1845]
Loss:[0.1807]
Loss:[0.1767]
Loss:[0.1726]
Loss:[0.1680]
Loss:[0.1708]
Loss:[0.1585]
Loss:[0.1640]
Loss:[0.1619]
Loss:[0.1521]
Loss:[0.1520]
Loss:[0.1482]
Loss:[0.1509]
Loss:[0.1403]
Loss:[0.1419]
Loss:[0.1395]
Loss:[0.1364]
Loss:[0.1357]
Loss:[0.1343]
Loss:[0.1255]
Loss:[0.1292]
Loss:[0.1283]
Loss:[0.1278]
Loss:[0.1228]
Loss:[0.1295]
Loss:[0.1243]
Loss:[0.1194]
Loss:[0.1183]
Loss:[0.1237]
Loss:[0.1186]
Loss:[0.1214]
Loss:[0.1127]
Loss:[0.1178]
Loss:[0.1125]
Loss:[0.1095]
Loss:[0.1142]
Loss:[0.1085]
Loss:[0.1102]
Loss:[0.1105]
Loss:[0.1086]
Loss:[0.1080]
Loss:[0.1068]
Loss:[0.1108]
Loss:[0.1038]
Loss:[0.1022]
Loss:[0.1068]
Loss:[0.1042]
Loss:[0.0984]
Loss:[0.1033]
Loss:[0.0994]
Loss:[0.0953]
Loss:[0.0946]
Loss:[0.0973]
Loss:[0.0927]
Loss:[0.0996]
Loss:[0.0916]
Loss:[0.0925]
Loss:[0.0905]
Loss:[0.0843]
Loss:[0.1008]
Loss:[0.0919]
Loss:[0.0944]
Loss:[0.0919]
Loss:[0.0885]
Loss:[0.0911]
Loss:[0.0889]
Loss:[0.0913]
Loss:[0.0916]
Loss:[0.0856]
Loss:[0.0785]
Loss:[0.0874]
Loss:[0.0840]
Loss:[0.0886]
Loss:[0.0798]
Loss:[0.0760]
Loss:[0.0800]
Loss:[0.0837]
Loss:[0.0786]
Loss:[0.0850]
Loss:[0.0810]
Loss:[0.0794]
Loss:[0.0791]
Loss:[0.0778]
Loss:[0.0821]
Loss:[0.0760]
Loss:[0.0744]
Loss:[0.0717]
Loss:[0.0705]
Loss:[0.0809]
Loss:[0.0783]
Loss:[0.0743]
Loss:[0.0733]
Loss:[0.0765]
Loss:[0.0735]
Loss:[0.0734]
Loss:[0.0746]
Loss:[0.0719]
Loss:[0.0745]
Loss:[0.0695]
Loss:[0.0738]
Loss:[0.0712]
Loss:[0.0661]
Loss:[0.0624]
Loss:[0.0590]
Loss:[0.0661]
Loss:[0.0630]
Loss:[0.0682]
Loss:[0.0653]
Loss:[0.0630]
Loss:[0.0635]
Loss:[0.0695]
Loss:[0.0556]
Loss:[0.0589]
Loss:[0.0638]
Loss:[0.0627]
Loss:[0.0597]
Loss:[0.0664]
Loss:[0.0619]
Loss:[0.0557]
Loss:[0.0644]
Loss:[0.0605]
Loss:[0.0587]
Loss:[0.0489]
Loss:[0.0542]
Loss:[0.0560]
Loss:[0.0542]
Loss:[0.0538]
Loss:[0.0558]
Loss:[0.0575]
Loss:[0.0523]
Loss:[0.0595]
Loss:[0.0518]
Loss:[0.0498]
Loss:[0.0503]
Loss:[0.0489]
Loss:[0.0516]
Loss:[0.0469]
Loss:[0.0483]
Loss:[0.0461]
Loss:[0.0481]
Loss:[0.0421]
Loss:[0.0438]
Loss:[0.0453]
Loss:[0.0430]
Loss:[0.0491]
Loss:[0.0459]
Loss:[0.0476]
Loss:[0.0422]
Loss:[0.0412]
Loss:[0.0388]
Loss:[0.0405]
Loss:[0.0446]
Loss:[0.0445]
Loss:[0.0404]
Loss:[0.0374]
Loss:[0.0373]
Loss:[0.0400]
Loss:[0.0379]
Loss:[0.0398]
Loss:[0.0371]
Loss:[0.0402]
Loss:[0.0421]
Loss:[0.0444]
Loss:[0.0369]
Loss:[0.0332]
Loss:[0.0353]
Loss:[0.0353]
Loss:[0.0326]
Loss:[0.0423]
Loss:[0.0318]
Loss:[0.0354]
Loss:[0.0353]
Loss:[0.0334]
Loss:[0.0390]
Loss:[0.0337]
Loss:[0.0370]
Loss:[0.0334]
Loss:[0.0307]
Loss:[0.0314]
Loss:[0.0321]
Loss:[0.0349]
Loss:[0.0326]
Loss:[0.0328]
Loss:[0.0315]
Loss:[0.0332]
Loss:[0.0283]
Loss:[0.0303]
Loss:[0.0328]
Loss:[0.0352]
Loss:[0.0279]
Loss:[0.0289]
Loss:[0.0327]
Loss:[0.0270]
Loss:[0.0302]
Loss:[0.0296]
Loss:[0.0340]
Loss:[0.0330]
Loss:[0.0283]
Loss:[0.0272]
Loss:[0.0302]
Loss:[0.0249]
Loss:[0.0296]
Loss:[0.0321]
Loss:[0.0273]
Loss:[0.0317]
Loss:[0.0258]
Loss:[0.0246]
Loss:[0.0265]
Loss:[0.0263]
Loss:[0.0280]
Loss:[0.0270]
Loss:[0.0286]
Loss:[0.0260]
Loss:[0.0235]
Loss:[0.0262]
Loss:[0.0226]
Loss:[0.0281]
Loss:[0.0242]
Loss:[0.0253]
Loss:[0.0250]
Loss:[0.0254]
Loss:[0.0246]
Loss:[0.0255]
Loss:[0.0299]
Loss:[0.0243]
Loss:[0.0299]
Loss:[0.0232]
Loss:[0.0265]
Loss:[0.0241]
Loss:[0.0267]
Loss:[0.0199]
Loss:[0.0212]
Loss:[0.0257]
Loss:[0.0251]
Loss:[0.0270]
Loss:[0.0255]
Loss:[0.0233]
Loss:[0.0207]
Loss:[0.0266]
Loss:[0.0232]
Loss:[0.0290]
Loss:[0.0235]
Loss:[0.0213]
Loss:[0.0250]
Loss:[0.0229]
Loss:[0.0205]
Loss:[0.0202]
Loss:[0.0209]
Loss:[0.0260]
Loss:[0.0226]
Loss:[0.0195]
Loss:[0.0220]
Loss:[0.0204]
Loss:[0.0261]
Loss:[0.0195]
Loss:[0.0163]
Loss:[0.0274]
Loss:[0.0211]
Loss:[0.0243]
Loss:[0.0224]
Loss:[0.0182]
Loss:[0.0212]
Loss:[0.0217]
Loss:[0.0226]
Loss:[0.0170]
Loss:[0.0216]
Loss:[0.0199]
Loss:[0.0207]
Loss:[0.0201]
Loss:[0.0245]
Loss:[0.0164]
Loss:[0.0243]
Loss:[0.0224]
Loss:[0.0188]
Loss:[0.0181]
Loss:[0.0172]
Early stopping!
Loading 276th epoch
acc:[0.3170]
acc:[0.3170]
acc:[0.3250]
acc:[0.3230]
acc:[0.3200]
acc:[0.3240]
acc:[0.3280]
acc:[0.3220]
acc:[0.3200]
acc:[0.3220]
acc:[0.3220]
acc:[0.3220]
acc:[0.3160]
acc:[0.3240]
acc:[0.3200]
acc:[0.3220]
acc:[0.3230]
acc:[0.3220]
acc:[0.3260]
acc:[0.3260]
acc:[0.3200]
acc:[0.3210]
acc:[0.3250]
acc:[0.3220]
acc:[0.3210]
acc:[0.3220]
acc:[0.3280]
acc:[0.3280]
acc:[0.3240]
acc:[0.3180]
acc:[0.3220]
acc:[0.3240]
acc:[0.3240]
acc:[0.3210]
acc:[0.3200]
acc:[0.3250]
acc:[0.3280]
acc:[0.3270]
acc:[0.3240]
acc:[0.3160]
acc:[0.3190]
acc:[0.3260]
acc:[0.3180]
acc:[0.3250]
acc:[0.3260]
acc:[0.3240]
acc:[0.3220]
acc:[0.3240]
acc:[0.3290]
acc:[0.3220]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.3227]
Mean:[32.2720]
Std :[0.3233]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='citeseer', drop_percent=0.6, gpu=3, seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6926]
Loss:[0.6905]
Loss:[0.6886]
Loss:[0.6849]
Loss:[0.6830]
Loss:[0.6769]
Loss:[0.6733]
Loss:[0.6657]
Loss:[0.6618]
Loss:[0.6517]
Loss:[0.6464]
Loss:[0.6346]
Loss:[0.6276]
Loss:[0.6165]
Loss:[0.6053]
Loss:[0.5953]
Loss:[0.5802]
Loss:[0.5703]
Loss:[0.5596]
Loss:[0.5432]
Loss:[0.5304]
Loss:[0.5207]
Loss:[0.5052]
Loss:[0.4907]
Loss:[0.4775]
Loss:[0.4670]
Loss:[0.4526]
Loss:[0.4352]
Loss:[0.4228]
Loss:[0.4146]
Loss:[0.4046]
Loss:[0.3863]
Loss:[0.3751]
Loss:[0.3661]
Loss:[0.3645]
Loss:[0.3501]
Loss:[0.3341]
Loss:[0.3279]
Loss:[0.3281]
Loss:[0.3152]
Loss:[0.3023]
Loss:[0.3023]
Loss:[0.2911]
Loss:[0.2833]
Loss:[0.2721]
Loss:[0.2706]
Loss:[0.2627]
Loss:[0.2580]
Loss:[0.2488]
Loss:[0.2457]
Loss:[0.2412]
Loss:[0.2410]
Loss:[0.2326]
Loss:[0.2281]
Loss:[0.2216]
Loss:[0.2133]
Loss:[0.2181]
Loss:[0.2112]
Loss:[0.2041]
Loss:[0.2027]
Loss:[0.2071]
Loss:[0.2008]
Loss:[0.1923]
Loss:[0.1888]
Loss:[0.1998]
Loss:[0.1834]
Loss:[0.1797]
Loss:[0.1809]
Loss:[0.1846]
Loss:[0.1813]
Loss:[0.1824]
Loss:[0.1728]
Loss:[0.1750]
Loss:[0.1659]
Loss:[0.1714]
Loss:[0.1689]
Loss:[0.1627]
Loss:[0.1580]
Loss:[0.1615]
Loss:[0.1626]
Loss:[0.1635]
Loss:[0.1580]
Loss:[0.1600]
Loss:[0.1497]
Loss:[0.1526]
Loss:[0.1494]
Loss:[0.1484]
Loss:[0.1488]
Loss:[0.1513]
Loss:[0.1475]
Loss:[0.1476]
Loss:[0.1470]
Loss:[0.1432]
Loss:[0.1441]
Loss:[0.1433]
Loss:[0.1402]
Loss:[0.1480]
Loss:[0.1366]
Loss:[0.1442]
Loss:[0.1350]
Loss:[0.1438]
Loss:[0.1384]
Loss:[0.1351]
Loss:[0.1278]
Loss:[0.1368]
Loss:[0.1341]
Loss:[0.1311]
Loss:[0.1346]
Loss:[0.1326]
Loss:[0.1291]
Loss:[0.1260]
Loss:[0.1294]
Loss:[0.1273]
Loss:[0.1223]
Loss:[0.1241]
Loss:[0.1293]
Loss:[0.1275]
Loss:[0.1308]
Loss:[0.1246]
Loss:[0.1353]
Loss:[0.1303]
Loss:[0.1215]
Loss:[0.1203]
Loss:[0.1299]
Loss:[0.1372]
Loss:[0.1155]
Loss:[0.1287]
Loss:[0.1298]
Loss:[0.1280]
Loss:[0.1234]
Loss:[0.1383]
Loss:[0.1166]
Loss:[0.1194]
Loss:[0.1295]
Loss:[0.1145]
Loss:[0.1159]
Loss:[0.1271]
Loss:[0.1145]
Loss:[0.1177]
Loss:[0.1185]
Loss:[0.1169]
Loss:[0.1152]
Loss:[0.1128]
Loss:[0.1165]
Loss:[0.1142]
Loss:[0.1126]
Loss:[0.1103]
Loss:[0.1156]
Loss:[0.1160]
Loss:[0.1165]
Loss:[0.1102]
Loss:[0.1177]
Loss:[0.1109]
Loss:[0.1147]
Loss:[0.1096]
Loss:[0.1146]
Loss:[0.1121]
Loss:[0.1069]
Loss:[0.1096]
Loss:[0.1105]
Loss:[0.1025]
Loss:[0.1085]
Loss:[0.1038]
Loss:[0.1081]
Loss:[0.1060]
Loss:[0.1139]
Loss:[0.1018]
Loss:[0.1110]
Loss:[0.1057]
Loss:[0.1088]
Loss:[0.1132]
Loss:[0.1046]
Loss:[0.1028]
Loss:[0.1054]
Loss:[0.1039]
Loss:[0.1133]
Loss:[0.1027]
Loss:[0.1116]
Loss:[0.1029]
Loss:[0.1018]
Loss:[0.0998]
Loss:[0.1025]
Loss:[0.1034]
Loss:[0.1008]
Loss:[0.1012]
Loss:[0.1009]
Loss:[0.1016]
Loss:[0.1040]
Loss:[0.0986]
Loss:[0.1068]
Loss:[0.1007]
Loss:[0.1016]
Loss:[0.1119]
Loss:[0.1103]
Loss:[0.0995]
Loss:[0.1015]
Loss:[0.1035]
Loss:[0.1061]
Loss:[0.1027]
Loss:[0.1045]
Loss:[0.1022]
Loss:[0.1014]
Loss:[0.0928]
Loss:[0.1048]
Loss:[0.1067]
Loss:[0.1000]
Loss:[0.1041]
Loss:[0.1025]
Loss:[0.0980]
Loss:[0.0988]
Loss:[0.0918]
Loss:[0.1059]
Loss:[0.1012]
Loss:[0.1038]
Loss:[0.1086]
Loss:[0.0992]
Loss:[0.1016]
Loss:[0.1054]
Loss:[0.1038]
Loss:[0.1058]
Loss:[0.1075]
Loss:[0.0963]
Loss:[0.1088]
Loss:[0.1085]
Loss:[0.1029]
Loss:[0.1090]
Loss:[0.1062]
Loss:[0.1054]
Loss:[0.1237]
Loss:[0.1171]
Loss:[0.1006]
Early stopping!
Loading 211th epoch
acc:[0.6990]
acc:[0.6980]
acc:[0.6950]
acc:[0.6970]
acc:[0.6980]
acc:[0.6930]
acc:[0.6970]
acc:[0.6970]
acc:[0.6970]
acc:[0.6970]
acc:[0.6970]
acc:[0.6960]
acc:[0.6950]
acc:[0.6980]
acc:[0.6940]
acc:[0.7000]
acc:[0.6950]
acc:[0.6940]
acc:[0.6980]
acc:[0.6950]
acc:[0.6950]
acc:[0.6950]
acc:[0.7000]
acc:[0.6970]
acc:[0.6940]
acc:[0.6960]
acc:[0.6970]
acc:[0.6960]
acc:[0.6960]
acc:[0.6980]
acc:[0.6970]
acc:[0.6960]
acc:[0.7000]
acc:[0.6950]
acc:[0.6940]
acc:[0.6940]
acc:[0.6980]
acc:[0.6940]
acc:[0.6980]
acc:[0.6960]
acc:[0.6970]
acc:[0.6960]
acc:[0.6970]
acc:[0.6960]
acc:[0.6950]
acc:[0.6950]
acc:[0.6950]
acc:[0.6940]
acc:[0.6960]
acc:[0.6950]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.6962]
Mean:[69.6240]
Std :[0.1697]
----------------------------------------------------------------------------------------------------
