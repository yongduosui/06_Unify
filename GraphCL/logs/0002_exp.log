----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='cora', drop_percent=0.02, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6922]
Loss:[0.6874]
Loss:[0.6859]
Loss:[0.6814]
Loss:[0.6766]
Loss:[0.6733]
Loss:[0.6650]
Loss:[0.6599]
Loss:[0.6510]
Loss:[0.6432]
Loss:[0.6336]
Loss:[0.6223]
Loss:[0.6135]
Loss:[0.5990]
Loss:[0.5887]
Loss:[0.5750]
Loss:[0.5635]
Loss:[0.5491]
Loss:[0.5336]
Loss:[0.5187]
Loss:[0.5042]
Loss:[0.4912]
Loss:[0.4748]
Loss:[0.4629]
Loss:[0.4493]
Loss:[0.4311]
Loss:[0.4233]
Loss:[0.4026]
Loss:[0.3881]
Loss:[0.3761]
Loss:[0.3749]
Loss:[0.3666]
Loss:[0.3486]
Loss:[0.3319]
Loss:[0.3235]
Loss:[0.3173]
Loss:[0.2974]
Loss:[0.2938]
Loss:[0.2875]
Loss:[0.2791]
Loss:[0.2699]
Loss:[0.2669]
Loss:[0.2607]
Loss:[0.2572]
Loss:[0.2462]
Loss:[0.2396]
Loss:[0.2448]
Loss:[0.2360]
Loss:[0.2261]
Loss:[0.2208]
Loss:[0.2143]
Loss:[0.2170]
Loss:[0.2237]
Loss:[0.2055]
Loss:[0.1974]
Loss:[0.2119]
Loss:[0.1887]
Loss:[0.2021]
Loss:[0.1948]
Loss:[0.1850]
Loss:[0.1891]
Loss:[0.1778]
Loss:[0.1697]
Loss:[0.1774]
Loss:[0.1839]
Loss:[0.1768]
Loss:[0.1765]
Loss:[0.1663]
Loss:[0.1710]
Loss:[0.1497]
Loss:[0.1603]
Loss:[0.1581]
Loss:[0.1605]
Loss:[0.1561]
Loss:[0.1541]
Loss:[0.1583]
Loss:[0.1600]
Loss:[0.1431]
Loss:[0.1457]
Loss:[0.1414]
Loss:[0.1447]
Loss:[0.1353]
Loss:[0.1352]
Loss:[0.1318]
Loss:[0.1338]
Loss:[0.1368]
Loss:[0.1264]
Loss:[0.1346]
Loss:[0.1194]
Loss:[0.1315]
Loss:[0.1182]
Loss:[0.1274]
Loss:[0.1310]
Loss:[0.1206]
Loss:[0.1306]
Loss:[0.1225]
Loss:[0.1141]
Loss:[0.1165]
Loss:[0.1198]
Loss:[0.1209]
Loss:[0.1108]
Loss:[0.1101]
Loss:[0.1156]
Loss:[0.1078]
Loss:[0.1149]
Loss:[0.1153]
Loss:[0.1156]
Loss:[0.1054]
Loss:[0.1112]
Loss:[0.1034]
Loss:[0.1150]
Loss:[0.1062]
Loss:[0.1049]
Loss:[0.1028]
Loss:[0.0927]
Loss:[0.1015]
Loss:[0.0974]
Loss:[0.0989]
Loss:[0.1011]
Loss:[0.0868]
Loss:[0.1077]
Loss:[0.1012]
Loss:[0.0954]
Loss:[0.0996]
Loss:[0.0880]
Loss:[0.0981]
Loss:[0.0906]
Loss:[0.1009]
Loss:[0.1050]
Loss:[0.1005]
Loss:[0.0904]
Loss:[0.0995]
Loss:[0.1053]
Loss:[0.0855]
Loss:[0.0902]
Loss:[0.0826]
Loss:[0.0936]
Loss:[0.0904]
Loss:[0.0856]
Loss:[0.0972]
Loss:[0.0902]
Loss:[0.0712]
Loss:[0.0835]
Loss:[0.0811]
Loss:[0.0868]
Loss:[0.0858]
Loss:[0.0906]
Loss:[0.0841]
Loss:[0.0742]
Loss:[0.0704]
Loss:[0.0820]
Loss:[0.0745]
Loss:[0.0758]
Loss:[0.0753]
Loss:[0.0759]
Loss:[0.0941]
Loss:[0.0806]
Loss:[0.0823]
Loss:[0.0807]
Loss:[0.0759]
Loss:[0.0772]
Loss:[0.0752]
Loss:[0.0680]
Loss:[0.0724]
Loss:[0.0717]
Loss:[0.0649]
Loss:[0.0682]
Loss:[0.0752]
Loss:[0.0775]
Loss:[0.0767]
Loss:[0.0749]
Loss:[0.0657]
Loss:[0.0708]
Loss:[0.0740]
Loss:[0.0766]
Loss:[0.0746]
Loss:[0.0650]
Loss:[0.0704]
Loss:[0.0692]
Loss:[0.0639]
Loss:[0.0741]
Loss:[0.0686]
Loss:[0.0629]
Loss:[0.0636]
Loss:[0.0660]
Loss:[0.0630]
Loss:[0.0634]
Loss:[0.0647]
Loss:[0.0665]
Loss:[0.0612]
Loss:[0.0691]
Loss:[0.0559]
Loss:[0.0621]
Loss:[0.0736]
Loss:[0.0758]
Loss:[0.0630]
Loss:[0.0658]
Loss:[0.0703]
Loss:[0.0583]
Loss:[0.0589]
Loss:[0.0614]
Loss:[0.0588]
Loss:[0.0653]
Loss:[0.0508]
Loss:[0.0630]
Loss:[0.0599]
Loss:[0.0586]
Loss:[0.0564]
Loss:[0.0545]
Loss:[0.0638]
Loss:[0.0568]
Loss:[0.0556]
Loss:[0.0569]
Loss:[0.0551]
Loss:[0.0518]
Loss:[0.0544]
Loss:[0.0564]
Loss:[0.0572]
Loss:[0.0503]
Loss:[0.0537]
Loss:[0.0585]
Loss:[0.0572]
Loss:[0.0614]
Loss:[0.0537]
Loss:[0.0556]
Loss:[0.0496]
Loss:[0.0518]
Loss:[0.0534]
Loss:[0.0505]
Loss:[0.0462]
Loss:[0.0424]
Loss:[0.0512]
Loss:[0.0503]
Loss:[0.0539]
Loss:[0.0527]
Loss:[0.0655]
Loss:[0.0512]
Loss:[0.0596]
Loss:[0.0610]
Loss:[0.0503]
Loss:[0.0502]
Loss:[0.0588]
Loss:[0.0522]
Loss:[0.0519]
Loss:[0.0388]
Loss:[0.0544]
Loss:[0.0479]
Loss:[0.0429]
Loss:[0.0489]
Loss:[0.0460]
Loss:[0.0494]
Loss:[0.0462]
Loss:[0.0466]
Loss:[0.0525]
Loss:[0.0532]
Loss:[0.0437]
Loss:[0.0495]
Loss:[0.0510]
Loss:[0.0510]
Loss:[0.0483]
Loss:[0.0463]
Loss:[0.0424]
Loss:[0.0386]
Loss:[0.0425]
Loss:[0.0489]
Loss:[0.0442]
Loss:[0.0469]
Loss:[0.0480]
Loss:[0.0442]
Loss:[0.0403]
Loss:[0.0435]
Loss:[0.0490]
Loss:[0.0416]
Loss:[0.0498]
Loss:[0.0489]
Loss:[0.0539]
Loss:[0.0403]
Loss:[0.0530]
Loss:[0.0394]
Loss:[0.0480]
Loss:[0.0506]
Loss:[0.0598]
Loss:[0.0472]
Early stopping!
Loading 263th epoch
acc:[0.8170]
acc:[0.8160]
acc:[0.8160]
acc:[0.8140]
acc:[0.8160]
acc:[0.8180]
acc:[0.8170]
acc:[0.8180]
acc:[0.8160]
acc:[0.8170]
acc:[0.8150]
acc:[0.8170]
acc:[0.8160]
acc:[0.8160]
acc:[0.8160]
acc:[0.8170]
acc:[0.8180]
acc:[0.8170]
acc:[0.8140]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
acc:[0.8160]
acc:[0.8140]
acc:[0.8160]
acc:[0.8170]
acc:[0.8150]
acc:[0.8170]
acc:[0.8150]
acc:[0.8160]
acc:[0.8150]
acc:[0.8170]
acc:[0.8170]
acc:[0.8170]
acc:[0.8150]
acc:[0.8130]
acc:[0.8160]
acc:[0.8150]
acc:[0.8130]
acc:[0.8170]
acc:[0.8170]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
acc:[0.8140]
acc:[0.8160]
acc:[0.8140]
acc:[0.8180]
acc:[0.8140]
acc:[0.8170]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8160]
Mean:[81.6000]
Std :[0.1278]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='cora', drop_percent=0.05, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6923]
Loss:[0.6875]
Loss:[0.6861]
Loss:[0.6818]
Loss:[0.6770]
Loss:[0.6739]
Loss:[0.6657]
Loss:[0.6606]
Loss:[0.6521]
Loss:[0.6440]
Loss:[0.6352]
Loss:[0.6235]
Loss:[0.6156]
Loss:[0.6005]
Loss:[0.5911]
Loss:[0.5770]
Loss:[0.5660]
Loss:[0.5526]
Loss:[0.5362]
Loss:[0.5219]
Loss:[0.5085]
Loss:[0.4950]
Loss:[0.4775]
Loss:[0.4666]
Loss:[0.4543]
Loss:[0.4366]
Loss:[0.4275]
Loss:[0.4068]
Loss:[0.3939]
Loss:[0.3827]
Loss:[0.3820]
Loss:[0.3693]
Loss:[0.3500]
Loss:[0.3395]
Loss:[0.3295]
Loss:[0.3185]
Loss:[0.3022]
Loss:[0.2989]
Loss:[0.2912]
Loss:[0.2835]
Loss:[0.2732]
Loss:[0.2708]
Loss:[0.2661]
Loss:[0.2582]
Loss:[0.2501]
Loss:[0.2436]
Loss:[0.2447]
Loss:[0.2396]
Loss:[0.2298]
Loss:[0.2199]
Loss:[0.2189]
Loss:[0.2199]
Loss:[0.2210]
Loss:[0.2086]
Loss:[0.2002]
Loss:[0.2121]
Loss:[0.1917]
Loss:[0.2010]
Loss:[0.1947]
Loss:[0.1872]
Loss:[0.1879]
Loss:[0.1810]
Loss:[0.1697]
Loss:[0.1770]
Loss:[0.1861]
Loss:[0.1707]
Loss:[0.1771]
Loss:[0.1708]
Loss:[0.1661]
Loss:[0.1491]
Loss:[0.1575]
Loss:[0.1591]
Loss:[0.1596]
Loss:[0.1572]
Loss:[0.1618]
Loss:[0.1560]
Loss:[0.1603]
Loss:[0.1451]
Loss:[0.1453]
Loss:[0.1436]
Loss:[0.1463]
Loss:[0.1371]
Loss:[0.1358]
Loss:[0.1332]
Loss:[0.1330]
Loss:[0.1395]
Loss:[0.1270]
Loss:[0.1346]
Loss:[0.1196]
Loss:[0.1334]
Loss:[0.1194]
Loss:[0.1272]
Loss:[0.1325]
Loss:[0.1230]
Loss:[0.1339]
Loss:[0.1227]
Loss:[0.1160]
Loss:[0.1183]
Loss:[0.1193]
Loss:[0.1212]
Loss:[0.1126]
Loss:[0.1101]
Loss:[0.1195]
Loss:[0.1112]
Loss:[0.1145]
Loss:[0.1156]
Loss:[0.1158]
Loss:[0.1061]
Loss:[0.1099]
Loss:[0.1029]
Loss:[0.1163]
Loss:[0.1066]
Loss:[0.1060]
Loss:[0.1031]
Loss:[0.0943]
Loss:[0.1069]
Loss:[0.0969]
Loss:[0.1003]
Loss:[0.1038]
Loss:[0.0874]
Loss:[0.1073]
Loss:[0.1024]
Loss:[0.0961]
Loss:[0.0993]
Loss:[0.0890]
Loss:[0.0989]
Loss:[0.0940]
Loss:[0.1041]
Loss:[0.1058]
Loss:[0.1002]
Loss:[0.0902]
Loss:[0.1013]
Loss:[0.1077]
Loss:[0.0858]
Loss:[0.0911]
Loss:[0.0835]
Loss:[0.0926]
Loss:[0.0924]
Loss:[0.0861]
Loss:[0.0961]
Loss:[0.0921]
Loss:[0.0743]
Loss:[0.0819]
Loss:[0.0819]
Loss:[0.0879]
Loss:[0.0860]
Loss:[0.0924]
Loss:[0.0877]
Loss:[0.0748]
Loss:[0.0715]
Loss:[0.0838]
Loss:[0.0764]
Loss:[0.0763]
Loss:[0.0759]
Loss:[0.0775]
Loss:[0.0959]
Loss:[0.0817]
Loss:[0.0830]
Loss:[0.0824]
Loss:[0.0787]
Loss:[0.0786]
Loss:[0.0779]
Loss:[0.0699]
Loss:[0.0739]
Loss:[0.0711]
Loss:[0.0670]
Loss:[0.0690]
Loss:[0.0778]
Loss:[0.0782]
Loss:[0.0792]
Loss:[0.0763]
Loss:[0.0675]
Loss:[0.0711]
Loss:[0.0755]
Loss:[0.0807]
Loss:[0.0792]
Loss:[0.0650]
Loss:[0.0751]
Loss:[0.0723]
Loss:[0.0665]
Loss:[0.0767]
Loss:[0.0695]
Loss:[0.0624]
Loss:[0.0652]
Loss:[0.0687]
Loss:[0.0638]
Loss:[0.0643]
Loss:[0.0667]
Loss:[0.0696]
Loss:[0.0638]
Loss:[0.0711]
Loss:[0.0565]
Loss:[0.0621]
Loss:[0.0728]
Loss:[0.0766]
Loss:[0.0650]
Loss:[0.0680]
Loss:[0.0735]
Loss:[0.0619]
Loss:[0.0596]
Loss:[0.0635]
Loss:[0.0584]
Loss:[0.0655]
Loss:[0.0531]
Loss:[0.0653]
Loss:[0.0612]
Loss:[0.0618]
Loss:[0.0580]
Loss:[0.0547]
Loss:[0.0651]
Loss:[0.0574]
Loss:[0.0575]
Loss:[0.0556]
Loss:[0.0565]
Loss:[0.0529]
Loss:[0.0567]
Loss:[0.0590]
Loss:[0.0578]
Loss:[0.0521]
Loss:[0.0556]
Loss:[0.0599]
Loss:[0.0574]
Loss:[0.0615]
Loss:[0.0554]
Loss:[0.0569]
Loss:[0.0496]
Loss:[0.0532]
Loss:[0.0547]
Loss:[0.0527]
Loss:[0.0480]
Loss:[0.0432]
Loss:[0.0530]
Loss:[0.0499]
Loss:[0.0551]
Loss:[0.0532]
Loss:[0.0676]
Loss:[0.0493]
Loss:[0.0623]
Loss:[0.0609]
Loss:[0.0516]
Loss:[0.0518]
Loss:[0.0607]
Loss:[0.0534]
Loss:[0.0530]
Loss:[0.0404]
Loss:[0.0562]
Loss:[0.0502]
Loss:[0.0442]
Loss:[0.0511]
Loss:[0.0477]
Loss:[0.0517]
Loss:[0.0468]
Loss:[0.0501]
Loss:[0.0543]
Loss:[0.0545]
Loss:[0.0479]
Loss:[0.0518]
Loss:[0.0548]
Loss:[0.0529]
Loss:[0.0498]
Loss:[0.0484]
Loss:[0.0441]
Loss:[0.0400]
Loss:[0.0434]
Loss:[0.0511]
Loss:[0.0462]
Loss:[0.0466]
Loss:[0.0516]
Loss:[0.0468]
Loss:[0.0406]
Loss:[0.0446]
Loss:[0.0506]
Loss:[0.0466]
Loss:[0.0511]
Loss:[0.0535]
Loss:[0.0554]
Loss:[0.0408]
Loss:[0.0514]
Loss:[0.0385]
Loss:[0.0491]
Loss:[0.0487]
Loss:[0.0589]
Loss:[0.0475]
Loss:[0.0442]
Loss:[0.0532]
Loss:[0.0509]
Loss:[0.0443]
Loss:[0.0529]
Loss:[0.0364]
Loss:[0.0589]
Loss:[0.0439]
Loss:[0.0541]
Loss:[0.0505]
Loss:[0.0462]
Loss:[0.0489]
Loss:[0.0450]
Loss:[0.0412]
Loss:[0.0440]
Loss:[0.0355]
Loss:[0.0538]
Loss:[0.0428]
Loss:[0.0604]
Loss:[0.0292]
Loss:[0.0412]
Loss:[0.0411]
Loss:[0.0497]
Loss:[0.0348]
Loss:[0.0436]
Loss:[0.0352]
Loss:[0.0364]
Loss:[0.0367]
Loss:[0.0318]
Loss:[0.0393]
Loss:[0.0293]
Loss:[0.0364]
Loss:[0.0379]
Loss:[0.0373]
Loss:[0.0368]
Loss:[0.0391]
Loss:[0.0430]
Loss:[0.0439]
Loss:[0.0343]
Loss:[0.0417]
Early stopping!
Loading 303th epoch
acc:[0.8200]
acc:[0.8210]
acc:[0.8210]
acc:[0.8200]
acc:[0.8230]
acc:[0.8240]
acc:[0.8190]
acc:[0.8220]
acc:[0.8210]
acc:[0.8230]
acc:[0.8210]
acc:[0.8240]
acc:[0.8210]
acc:[0.8220]
acc:[0.8200]
acc:[0.8230]
acc:[0.8230]
acc:[0.8210]
acc:[0.8220]
acc:[0.8190]
acc:[0.8220]
acc:[0.8210]
acc:[0.8240]
acc:[0.8210]
acc:[0.8230]
acc:[0.8210]
acc:[0.8210]
acc:[0.8240]
acc:[0.8230]
acc:[0.8230]
acc:[0.8230]
acc:[0.8220]
acc:[0.8200]
acc:[0.8220]
acc:[0.8190]
acc:[0.8210]
acc:[0.8220]
acc:[0.8220]
acc:[0.8200]
acc:[0.8220]
acc:[0.8220]
acc:[0.8230]
acc:[0.8220]
acc:[0.8250]
acc:[0.8220]
acc:[0.8190]
acc:[0.8210]
acc:[0.8210]
acc:[0.8230]
acc:[0.8230]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8217]
Mean:[82.1740]
Std :[0.1440]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='cora', drop_percent=0.08, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6925]
Loss:[0.6876]
Loss:[0.6864]
Loss:[0.6822]
Loss:[0.6774]
Loss:[0.6746]
Loss:[0.6666]
Loss:[0.6615]
Loss:[0.6535]
Loss:[0.6453]
Loss:[0.6372]
Loss:[0.6252]
Loss:[0.6181]
Loss:[0.6027]
Loss:[0.5945]
Loss:[0.5795]
Loss:[0.5697]
Loss:[0.5564]
Loss:[0.5391]
Loss:[0.5262]
Loss:[0.5139]
Loss:[0.4989]
Loss:[0.4806]
Loss:[0.4721]
Loss:[0.4604]
Loss:[0.4414]
Loss:[0.4305]
Loss:[0.4125]
Loss:[0.4028]
Loss:[0.3895]
Loss:[0.3840]
Loss:[0.3687]
Loss:[0.3558]
Loss:[0.3454]
Loss:[0.3307]
Loss:[0.3220]
Loss:[0.3065]
Loss:[0.3000]
Loss:[0.2930]
Loss:[0.2882]
Loss:[0.2732]
Loss:[0.2701]
Loss:[0.2680]
Loss:[0.2606]
Loss:[0.2518]
Loss:[0.2462]
Loss:[0.2461]
Loss:[0.2389]
Loss:[0.2297]
Loss:[0.2220]
Loss:[0.2183]
Loss:[0.2207]
Loss:[0.2218]
Loss:[0.2081]
Loss:[0.2005]
Loss:[0.2148]
Loss:[0.1951]
Loss:[0.2037]
Loss:[0.1961]
Loss:[0.1884]
Loss:[0.1931]
Loss:[0.1831]
Loss:[0.1681]
Loss:[0.1775]
Loss:[0.1886]
Loss:[0.1715]
Loss:[0.1751]
Loss:[0.1713]
Loss:[0.1657]
Loss:[0.1530]
Loss:[0.1594]
Loss:[0.1585]
Loss:[0.1614]
Loss:[0.1621]
Loss:[0.1606]
Loss:[0.1555]
Loss:[0.1631]
Loss:[0.1455]
Loss:[0.1465]
Loss:[0.1466]
Loss:[0.1455]
Loss:[0.1384]
Loss:[0.1383]
Loss:[0.1332]
Loss:[0.1346]
Loss:[0.1396]
Loss:[0.1302]
Loss:[0.1351]
Loss:[0.1202]
Loss:[0.1320]
Loss:[0.1203]
Loss:[0.1282]
Loss:[0.1328]
Loss:[0.1237]
Loss:[0.1326]
Loss:[0.1232]
Loss:[0.1153]
Loss:[0.1182]
Loss:[0.1196]
Loss:[0.1203]
Loss:[0.1126]
Loss:[0.1105]
Loss:[0.1180]
Loss:[0.1107]
Loss:[0.1171]
Loss:[0.1155]
Loss:[0.1171]
Loss:[0.1077]
Loss:[0.1125]
Loss:[0.1052]
Loss:[0.1169]
Loss:[0.1066]
Loss:[0.1055]
Loss:[0.1054]
Loss:[0.0959]
Loss:[0.1062]
Loss:[0.0984]
Loss:[0.0999]
Loss:[0.1023]
Loss:[0.0892]
Loss:[0.1054]
Loss:[0.1011]
Loss:[0.0973]
Loss:[0.0997]
Loss:[0.0888]
Loss:[0.1001]
Loss:[0.0938]
Loss:[0.1033]
Loss:[0.1059]
Loss:[0.1018]
Loss:[0.0927]
Loss:[0.0999]
Loss:[0.1072]
Loss:[0.0848]
Loss:[0.0910]
Loss:[0.0852]
Loss:[0.0929]
Loss:[0.0941]
Loss:[0.0864]
Loss:[0.0968]
Loss:[0.0940]
Loss:[0.0740]
Loss:[0.0850]
Loss:[0.0817]
Loss:[0.0891]
Loss:[0.0866]
Loss:[0.0959]
Loss:[0.0878]
Loss:[0.0769]
Loss:[0.0718]
Loss:[0.0848]
Loss:[0.0766]
Loss:[0.0758]
Loss:[0.0770]
Loss:[0.0773]
Loss:[0.0961]
Loss:[0.0823]
Loss:[0.0825]
Loss:[0.0821]
Loss:[0.0780]
Loss:[0.0762]
Loss:[0.0774]
Loss:[0.0702]
Loss:[0.0738]
Loss:[0.0722]
Loss:[0.0674]
Loss:[0.0673]
Loss:[0.0768]
Loss:[0.0772]
Loss:[0.0776]
Loss:[0.0766]
Loss:[0.0678]
Loss:[0.0708]
Loss:[0.0738]
Loss:[0.0744]
Loss:[0.0761]
Loss:[0.0674]
Loss:[0.0699]
Loss:[0.0726]
Loss:[0.0661]
Loss:[0.0746]
Loss:[0.0687]
Loss:[0.0614]
Loss:[0.0651]
Loss:[0.0667]
Loss:[0.0652]
Loss:[0.0623]
Loss:[0.0646]
Loss:[0.0691]
Loss:[0.0623]
Loss:[0.0699]
Loss:[0.0554]
Loss:[0.0608]
Loss:[0.0727]
Loss:[0.0744]
Loss:[0.0648]
Loss:[0.0641]
Loss:[0.0690]
Loss:[0.0585]
Loss:[0.0576]
Loss:[0.0650]
Loss:[0.0563]
Loss:[0.0654]
Loss:[0.0515]
Loss:[0.0628]
Loss:[0.0606]
Loss:[0.0596]
Loss:[0.0587]
Loss:[0.0531]
Loss:[0.0634]
Loss:[0.0595]
Loss:[0.0556]
Loss:[0.0567]
Loss:[0.0545]
Loss:[0.0512]
Loss:[0.0562]
Loss:[0.0557]
Loss:[0.0588]
Loss:[0.0512]
Loss:[0.0570]
Loss:[0.0623]
Loss:[0.0563]
Loss:[0.0630]
Loss:[0.0599]
Loss:[0.0552]
Loss:[0.0508]
Loss:[0.0522]
Loss:[0.0544]
Loss:[0.0565]
Loss:[0.0472]
Loss:[0.0430]
Loss:[0.0544]
Loss:[0.0497]
Loss:[0.0533]
Loss:[0.0539]
Loss:[0.0672]
Loss:[0.0514]
Loss:[0.0613]
Loss:[0.0598]
Loss:[0.0515]
Loss:[0.0519]
Loss:[0.0604]
Loss:[0.0533]
Loss:[0.0522]
Loss:[0.0386]
Loss:[0.0558]
Loss:[0.0501]
Loss:[0.0433]
Loss:[0.0524]
Loss:[0.0495]
Loss:[0.0488]
Loss:[0.0478]
Loss:[0.0478]
Loss:[0.0540]
Loss:[0.0563]
Loss:[0.0431]
Loss:[0.0502]
Loss:[0.0513]
Loss:[0.0526]
Loss:[0.0474]
Loss:[0.0486]
Loss:[0.0432]
Loss:[0.0407]
Loss:[0.0411]
Loss:[0.0487]
Early stopping!
Loading 245th epoch
acc:[0.8240]
acc:[0.8260]
acc:[0.8240]
acc:[0.8220]
acc:[0.8230]
acc:[0.8260]
acc:[0.8240]
acc:[0.8220]
acc:[0.8200]
acc:[0.8240]
acc:[0.8220]
acc:[0.8250]
acc:[0.8240]
acc:[0.8240]
acc:[0.8230]
acc:[0.8250]
acc:[0.8230]
acc:[0.8220]
acc:[0.8240]
acc:[0.8240]
acc:[0.8240]
acc:[0.8230]
acc:[0.8240]
acc:[0.8230]
acc:[0.8230]
acc:[0.8240]
acc:[0.8240]
acc:[0.8220]
acc:[0.8230]
acc:[0.8230]
acc:[0.8250]
acc:[0.8230]
acc:[0.8240]
acc:[0.8240]
acc:[0.8240]
acc:[0.8240]
acc:[0.8240]
acc:[0.8240]
acc:[0.8240]
acc:[0.8230]
acc:[0.8220]
acc:[0.8240]
acc:[0.8240]
acc:[0.8220]
acc:[0.8220]
acc:[0.8240]
acc:[0.8230]
acc:[0.8240]
acc:[0.8230]
acc:[0.8240]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8235]
Mean:[82.3500]
Std :[0.1093]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='cora', drop_percent=0.1, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6924]
Loss:[0.6877]
Loss:[0.6863]
Loss:[0.6821]
Loss:[0.6774]
Loss:[0.6744]
Loss:[0.6664]
Loss:[0.6615]
Loss:[0.6532]
Loss:[0.6453]
Loss:[0.6366]
Loss:[0.6252]
Loss:[0.6175]
Loss:[0.6028]
Loss:[0.5936]
Loss:[0.5796]
Loss:[0.5690]
Loss:[0.5556]
Loss:[0.5396]
Loss:[0.5255]
Loss:[0.5121]
Loss:[0.4990]
Loss:[0.4826]
Loss:[0.4714]
Loss:[0.4589]
Loss:[0.4410]
Loss:[0.4327]
Loss:[0.4124]
Loss:[0.4000]
Loss:[0.3878]
Loss:[0.3880]
Loss:[0.3769]
Loss:[0.3570]
Loss:[0.3454]
Loss:[0.3362]
Loss:[0.3256]
Loss:[0.3093]
Loss:[0.3059]
Loss:[0.2973]
Loss:[0.2911]
Loss:[0.2802]
Loss:[0.2785]
Loss:[0.2730]
Loss:[0.2680]
Loss:[0.2586]
Loss:[0.2496]
Loss:[0.2552]
Loss:[0.2474]
Loss:[0.2372]
Loss:[0.2301]
Loss:[0.2293]
Loss:[0.2254]
Loss:[0.2333]
Loss:[0.2198]
Loss:[0.2070]
Loss:[0.2220]
Loss:[0.2012]
Loss:[0.2096]
Loss:[0.2076]
Loss:[0.1963]
Loss:[0.1979]
Loss:[0.1918]
Loss:[0.1758]
Loss:[0.1863]
Loss:[0.1927]
Loss:[0.1784]
Loss:[0.1886]
Loss:[0.1742]
Loss:[0.1764]
Loss:[0.1644]
Loss:[0.1645]
Loss:[0.1711]
Loss:[0.1694]
Loss:[0.1622]
Loss:[0.1626]
Loss:[0.1637]
Loss:[0.1697]
Loss:[0.1540]
Loss:[0.1523]
Loss:[0.1517]
Loss:[0.1527]
Loss:[0.1487]
Loss:[0.1463]
Loss:[0.1405]
Loss:[0.1432]
Loss:[0.1489]
Loss:[0.1337]
Loss:[0.1425]
Loss:[0.1274]
Loss:[0.1391]
Loss:[0.1272]
Loss:[0.1332]
Loss:[0.1407]
Loss:[0.1299]
Loss:[0.1369]
Loss:[0.1301]
Loss:[0.1228]
Loss:[0.1262]
Loss:[0.1298]
Loss:[0.1252]
Loss:[0.1176]
Loss:[0.1178]
Loss:[0.1237]
Loss:[0.1166]
Loss:[0.1245]
Loss:[0.1219]
Loss:[0.1244]
Loss:[0.1139]
Loss:[0.1195]
Loss:[0.1123]
Loss:[0.1220]
Loss:[0.1134]
Loss:[0.1112]
Loss:[0.1103]
Loss:[0.1000]
Loss:[0.1127]
Loss:[0.1045]
Loss:[0.1059]
Loss:[0.1090]
Loss:[0.0952]
Loss:[0.1137]
Loss:[0.1077]
Loss:[0.1032]
Loss:[0.1070]
Loss:[0.0950]
Loss:[0.1059]
Loss:[0.0987]
Loss:[0.1090]
Loss:[0.1128]
Loss:[0.1094]
Loss:[0.0985]
Loss:[0.1046]
Loss:[0.1130]
Loss:[0.0890]
Loss:[0.0961]
Loss:[0.0902]
Loss:[0.0978]
Loss:[0.0985]
Loss:[0.0914]
Loss:[0.1036]
Loss:[0.0993]
Loss:[0.0779]
Loss:[0.0929]
Loss:[0.0864]
Loss:[0.0951]
Loss:[0.0933]
Loss:[0.1001]
Loss:[0.0904]
Loss:[0.0814]
Loss:[0.0767]
Loss:[0.0885]
Loss:[0.0811]
Loss:[0.0798]
Loss:[0.0818]
Loss:[0.0823]
Loss:[0.1005]
Loss:[0.0866]
Loss:[0.0877]
Loss:[0.0876]
Loss:[0.0817]
Loss:[0.0824]
Loss:[0.0817]
Loss:[0.0741]
Loss:[0.0788]
Loss:[0.0772]
Loss:[0.0716]
Loss:[0.0729]
Loss:[0.0812]
Loss:[0.0827]
Loss:[0.0812]
Loss:[0.0814]
Loss:[0.0721]
Loss:[0.0743]
Loss:[0.0801]
Loss:[0.0807]
Loss:[0.0820]
Loss:[0.0734]
Loss:[0.0770]
Loss:[0.0766]
Loss:[0.0713]
Loss:[0.0794]
Loss:[0.0721]
Loss:[0.0657]
Loss:[0.0676]
Loss:[0.0715]
Loss:[0.0679]
Loss:[0.0678]
Loss:[0.0706]
Loss:[0.0736]
Loss:[0.0660]
Loss:[0.0758]
Loss:[0.0604]
Loss:[0.0649]
Loss:[0.0757]
Loss:[0.0770]
Loss:[0.0704]
Loss:[0.0700]
Loss:[0.0742]
Loss:[0.0636]
Loss:[0.0589]
Loss:[0.0685]
Loss:[0.0593]
Loss:[0.0707]
Loss:[0.0562]
Loss:[0.0672]
Loss:[0.0655]
Loss:[0.0641]
Loss:[0.0634]
Loss:[0.0572]
Loss:[0.0690]
Loss:[0.0634]
Loss:[0.0580]
Loss:[0.0613]
Loss:[0.0597]
Loss:[0.0564]
Loss:[0.0611]
Loss:[0.0625]
Loss:[0.0648]
Loss:[0.0541]
Loss:[0.0634]
Loss:[0.0659]
Loss:[0.0603]
Loss:[0.0678]
Loss:[0.0619]
Loss:[0.0603]
Loss:[0.0549]
Loss:[0.0561]
Loss:[0.0601]
Loss:[0.0606]
Loss:[0.0511]
Loss:[0.0502]
Loss:[0.0574]
Loss:[0.0539]
Loss:[0.0568]
Loss:[0.0567]
Loss:[0.0716]
Loss:[0.0539]
Loss:[0.0655]
Loss:[0.0647]
Loss:[0.0548]
Loss:[0.0545]
Loss:[0.0645]
Loss:[0.0584]
Loss:[0.0560]
Loss:[0.0422]
Loss:[0.0604]
Loss:[0.0537]
Loss:[0.0457]
Loss:[0.0541]
Loss:[0.0506]
Loss:[0.0548]
Loss:[0.0514]
Loss:[0.0529]
Loss:[0.0577]
Loss:[0.0610]
Loss:[0.0484]
Loss:[0.0540]
Loss:[0.0562]
Loss:[0.0553]
Loss:[0.0493]
Loss:[0.0515]
Loss:[0.0482]
Loss:[0.0438]
Loss:[0.0468]
Loss:[0.0525]
Early stopping!
Loading 245th epoch
acc:[0.8170]
acc:[0.8180]
acc:[0.8180]
acc:[0.8180]
acc:[0.8190]
acc:[0.8170]
acc:[0.8180]
acc:[0.8170]
acc:[0.8180]
acc:[0.8160]
acc:[0.8180]
acc:[0.8160]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
acc:[0.8170]
acc:[0.8170]
acc:[0.8160]
acc:[0.8190]
acc:[0.8160]
acc:[0.8180]
acc:[0.8140]
acc:[0.8180]
acc:[0.8160]
acc:[0.8170]
acc:[0.8170]
acc:[0.8170]
acc:[0.8170]
acc:[0.8170]
acc:[0.8140]
acc:[0.8160]
acc:[0.8190]
acc:[0.8160]
acc:[0.8160]
acc:[0.8190]
acc:[0.8160]
acc:[0.8160]
acc:[0.8140]
acc:[0.8160]
acc:[0.8170]
acc:[0.8150]
acc:[0.8170]
acc:[0.8150]
acc:[0.8180]
acc:[0.8170]
acc:[0.8180]
acc:[0.8160]
acc:[0.8180]
acc:[0.8160]
acc:[0.8160]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8168]
Mean:[81.6800]
Std :[0.1245]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='cora', drop_percent=0.2, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6924]
Loss:[0.6879]
Loss:[0.6867]
Loss:[0.6827]
Loss:[0.6783]
Loss:[0.6755]
Loss:[0.6677]
Loss:[0.6632]
Loss:[0.6553]
Loss:[0.6476]
Loss:[0.6396]
Loss:[0.6286]
Loss:[0.6213]
Loss:[0.6072]
Loss:[0.5986]
Loss:[0.5853]
Loss:[0.5748]
Loss:[0.5625]
Loss:[0.5476]
Loss:[0.5335]
Loss:[0.5208]
Loss:[0.5088]
Loss:[0.4923]
Loss:[0.4811]
Loss:[0.4693]
Loss:[0.4522]
Loss:[0.4451]
Loss:[0.4243]
Loss:[0.4140]
Loss:[0.4027]
Loss:[0.4017]
Loss:[0.3866]
Loss:[0.3706]
Loss:[0.3592]
Loss:[0.3493]
Loss:[0.3376]
Loss:[0.3228]
Loss:[0.3183]
Loss:[0.3143]
Loss:[0.3049]
Loss:[0.2948]
Loss:[0.2899]
Loss:[0.2871]
Loss:[0.2765]
Loss:[0.2702]
Loss:[0.2651]
Loss:[0.2651]
Loss:[0.2548]
Loss:[0.2490]
Loss:[0.2427]
Loss:[0.2351]
Loss:[0.2369]
Loss:[0.2402]
Loss:[0.2272]
Loss:[0.2193]
Loss:[0.2308]
Loss:[0.2101]
Loss:[0.2220]
Loss:[0.2138]
Loss:[0.2059]
Loss:[0.2061]
Loss:[0.1989]
Loss:[0.1899]
Loss:[0.1900]
Loss:[0.1982]
Loss:[0.1863]
Loss:[0.1907]
Loss:[0.1817]
Loss:[0.1818]
Loss:[0.1708]
Loss:[0.1741]
Loss:[0.1719]
Loss:[0.1742]
Loss:[0.1705]
Loss:[0.1699]
Loss:[0.1726]
Loss:[0.1790]
Loss:[0.1626]
Loss:[0.1602]
Loss:[0.1540]
Loss:[0.1558]
Loss:[0.1508]
Loss:[0.1511]
Loss:[0.1460]
Loss:[0.1493]
Loss:[0.1511]
Loss:[0.1397]
Loss:[0.1517]
Loss:[0.1326]
Loss:[0.1429]
Loss:[0.1306]
Loss:[0.1352]
Loss:[0.1434]
Loss:[0.1322]
Loss:[0.1450]
Loss:[0.1363]
Loss:[0.1220]
Loss:[0.1265]
Loss:[0.1326]
Loss:[0.1299]
Loss:[0.1251]
Loss:[0.1192]
Loss:[0.1286]
Loss:[0.1177]
Loss:[0.1232]
Loss:[0.1234]
Loss:[0.1268]
Loss:[0.1126]
Loss:[0.1211]
Loss:[0.1098]
Loss:[0.1229]
Loss:[0.1168]
Loss:[0.1159]
Loss:[0.1118]
Loss:[0.0996]
Loss:[0.1082]
Loss:[0.1073]
Loss:[0.1072]
Loss:[0.1108]
Loss:[0.0959]
Loss:[0.1162]
Loss:[0.1097]
Loss:[0.1036]
Loss:[0.1056]
Loss:[0.0961]
Loss:[0.1088]
Loss:[0.1005]
Loss:[0.1086]
Loss:[0.1134]
Loss:[0.1074]
Loss:[0.1008]
Loss:[0.1039]
Loss:[0.1122]
Loss:[0.0895]
Loss:[0.0977]
Loss:[0.0940]
Loss:[0.1012]
Loss:[0.0957]
Loss:[0.0941]
Loss:[0.1022]
Loss:[0.0930]
Loss:[0.0803]
Loss:[0.0959]
Loss:[0.0882]
Loss:[0.0961]
Loss:[0.0942]
Loss:[0.1005]
Loss:[0.0998]
Loss:[0.0926]
Loss:[0.0770]
Loss:[0.1040]
Loss:[0.0931]
Loss:[0.0852]
Loss:[0.0913]
Loss:[0.0806]
Loss:[0.1090]
Loss:[0.1002]
Loss:[0.0866]
Loss:[0.0948]
Loss:[0.0829]
Loss:[0.0848]
Loss:[0.0810]
Loss:[0.0783]
Loss:[0.0830]
Loss:[0.0759]
Loss:[0.0729]
Loss:[0.0715]
Loss:[0.0782]
Loss:[0.0818]
Loss:[0.0825]
Loss:[0.0786]
Loss:[0.0698]
Loss:[0.0758]
Loss:[0.0766]
Loss:[0.0793]
Loss:[0.0782]
Loss:[0.0693]
Loss:[0.0722]
Loss:[0.0757]
Loss:[0.0697]
Loss:[0.0793]
Loss:[0.0713]
Loss:[0.0652]
Loss:[0.0695]
Loss:[0.0706]
Loss:[0.0668]
Loss:[0.0651]
Loss:[0.0690]
Loss:[0.0692]
Loss:[0.0654]
Loss:[0.0703]
Loss:[0.0608]
Loss:[0.0618]
Loss:[0.0740]
Loss:[0.0753]
Loss:[0.0676]
Loss:[0.0676]
Loss:[0.0695]
Loss:[0.0636]
Loss:[0.0601]
Loss:[0.0655]
Loss:[0.0580]
Loss:[0.0675]
Loss:[0.0554]
Loss:[0.0636]
Loss:[0.0611]
Loss:[0.0672]
Loss:[0.0602]
Loss:[0.0574]
Loss:[0.0632]
Loss:[0.0589]
Loss:[0.0584]
Loss:[0.0570]
Loss:[0.0590]
Loss:[0.0554]
Loss:[0.0584]
Loss:[0.0588]
Loss:[0.0631]
Loss:[0.0519]
Loss:[0.0597]
Loss:[0.0651]
Loss:[0.0596]
Loss:[0.0636]
Loss:[0.0582]
Loss:[0.0572]
Loss:[0.0544]
Loss:[0.0530]
Loss:[0.0569]
Loss:[0.0555]
Loss:[0.0494]
Loss:[0.0466]
Loss:[0.0546]
Loss:[0.0522]
Loss:[0.0564]
Loss:[0.0558]
Loss:[0.0678]
Loss:[0.0522]
Loss:[0.0638]
Loss:[0.0623]
Loss:[0.0528]
Loss:[0.0527]
Loss:[0.0623]
Loss:[0.0537]
Loss:[0.0555]
Loss:[0.0400]
Loss:[0.0566]
Loss:[0.0509]
Loss:[0.0462]
Loss:[0.0537]
Loss:[0.0463]
Loss:[0.0531]
Loss:[0.0480]
Loss:[0.0483]
Loss:[0.0543]
Loss:[0.0553]
Loss:[0.0481]
Loss:[0.0505]
Loss:[0.0547]
Loss:[0.0549]
Loss:[0.0495]
Loss:[0.0473]
Loss:[0.0446]
Loss:[0.0415]
Loss:[0.0431]
Loss:[0.0497]
Early stopping!
Loading 245th epoch
acc:[0.8150]
acc:[0.8150]
acc:[0.8200]
acc:[0.8140]
acc:[0.8170]
acc:[0.8200]
acc:[0.8160]
acc:[0.8140]
acc:[0.8140]
acc:[0.8170]
acc:[0.8160]
acc:[0.8170]
acc:[0.8180]
acc:[0.8150]
acc:[0.8140]
acc:[0.8160]
acc:[0.8150]
acc:[0.8200]
acc:[0.8170]
acc:[0.8160]
acc:[0.8190]
acc:[0.8150]
acc:[0.8170]
acc:[0.8160]
acc:[0.8140]
acc:[0.8160]
acc:[0.8160]
acc:[0.8160]
acc:[0.8180]
acc:[0.8160]
acc:[0.8180]
acc:[0.8150]
acc:[0.8150]
acc:[0.8140]
acc:[0.8170]
acc:[0.8130]
acc:[0.8170]
acc:[0.8150]
acc:[0.8130]
acc:[0.8160]
acc:[0.8160]
acc:[0.8170]
acc:[0.8170]
acc:[0.8160]
acc:[0.8150]
acc:[0.8170]
acc:[0.8190]
acc:[0.8180]
acc:[0.8130]
acc:[0.8220]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8162]
Mean:[81.6240]
Std :[0.1944]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='cora', drop_percent=0.3, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6925]
Loss:[0.6881]
Loss:[0.6870]
Loss:[0.6831]
Loss:[0.6789]
Loss:[0.6763]
Loss:[0.6687]
Loss:[0.6644]
Loss:[0.6569]
Loss:[0.6496]
Loss:[0.6417]
Loss:[0.6311]
Loss:[0.6246]
Loss:[0.6106]
Loss:[0.6028]
Loss:[0.5896]
Loss:[0.5803]
Loss:[0.5679]
Loss:[0.5534]
Loss:[0.5401]
Loss:[0.5277]
Loss:[0.5162]
Loss:[0.5002]
Loss:[0.4898]
Loss:[0.4782]
Loss:[0.4617]
Loss:[0.4547]
Loss:[0.4357]
Loss:[0.4222]
Loss:[0.4103]
Loss:[0.4088]
Loss:[0.4023]
Loss:[0.3863]
Loss:[0.3688]
Loss:[0.3628]
Loss:[0.3524]
Loss:[0.3347]
Loss:[0.3333]
Loss:[0.3265]
Loss:[0.3177]
Loss:[0.3108]
Loss:[0.3044]
Loss:[0.3003]
Loss:[0.2969]
Loss:[0.2863]
Loss:[0.2771]
Loss:[0.2824]
Loss:[0.2752]
Loss:[0.2631]
Loss:[0.2583]
Loss:[0.2564]
Loss:[0.2526]
Loss:[0.2599]
Loss:[0.2453]
Loss:[0.2339]
Loss:[0.2473]
Loss:[0.2272]
Loss:[0.2364]
Loss:[0.2248]
Loss:[0.2216]
Loss:[0.2209]
Loss:[0.2178]
Loss:[0.2024]
Loss:[0.2072]
Loss:[0.2215]
Loss:[0.2062]
Loss:[0.2078]
Loss:[0.2041]
Loss:[0.1986]
Loss:[0.1824]
Loss:[0.1919]
Loss:[0.1868]
Loss:[0.1936]
Loss:[0.1895]
Loss:[0.1848]
Loss:[0.1856]
Loss:[0.1887]
Loss:[0.1776]
Loss:[0.1765]
Loss:[0.1719]
Loss:[0.1754]
Loss:[0.1663]
Loss:[0.1662]
Loss:[0.1632]
Loss:[0.1601]
Loss:[0.1666]
Loss:[0.1548]
Loss:[0.1656]
Loss:[0.1472]
Loss:[0.1544]
Loss:[0.1466]
Loss:[0.1518]
Loss:[0.1575]
Loss:[0.1480]
Loss:[0.1604]
Loss:[0.1494]
Loss:[0.1420]
Loss:[0.1434]
Loss:[0.1479]
Loss:[0.1478]
Loss:[0.1402]
Loss:[0.1350]
Loss:[0.1435]
Loss:[0.1301]
Loss:[0.1378]
Loss:[0.1383]
Loss:[0.1383]
Loss:[0.1298]
Loss:[0.1296]
Loss:[0.1243]
Loss:[0.1382]
Loss:[0.1303]
Loss:[0.1258]
Loss:[0.1249]
Loss:[0.1139]
Loss:[0.1233]
Loss:[0.1205]
Loss:[0.1199]
Loss:[0.1232]
Loss:[0.1085]
Loss:[0.1279]
Loss:[0.1222]
Loss:[0.1170]
Loss:[0.1186]
Loss:[0.1079]
Loss:[0.1214]
Loss:[0.1125]
Loss:[0.1217]
Loss:[0.1236]
Loss:[0.1206]
Loss:[0.1130]
Loss:[0.1192]
Loss:[0.1255]
Loss:[0.1083]
Loss:[0.1115]
Loss:[0.1014]
Loss:[0.1211]
Loss:[0.1109]
Loss:[0.1022]
Loss:[0.1162]
Loss:[0.1070]
Loss:[0.0879]
Loss:[0.0990]
Loss:[0.1005]
Loss:[0.1037]
Loss:[0.0991]
Loss:[0.1063]
Loss:[0.1017]
Loss:[0.0913]
Loss:[0.0874]
Loss:[0.0983]
Loss:[0.0915]
Loss:[0.0877]
Loss:[0.0899]
Loss:[0.0906]
Loss:[0.1076]
Loss:[0.0955]
Loss:[0.0963]
Loss:[0.0938]
Loss:[0.0922]
Loss:[0.0900]
Loss:[0.0888]
Loss:[0.0841]
Loss:[0.0884]
Loss:[0.0861]
Loss:[0.0795]
Loss:[0.0798]
Loss:[0.0899]
Loss:[0.0852]
Loss:[0.0900]
Loss:[0.0873]
Loss:[0.0783]
Loss:[0.0843]
Loss:[0.0864]
Loss:[0.0863]
Loss:[0.0869]
Loss:[0.0750]
Loss:[0.0825]
Loss:[0.0843]
Loss:[0.0761]
Loss:[0.0872]
Loss:[0.0807]
Loss:[0.0713]
Loss:[0.0802]
Loss:[0.0779]
Loss:[0.0736]
Loss:[0.0745]
Loss:[0.0761]
Loss:[0.0740]
Loss:[0.0724]
Loss:[0.0756]
Loss:[0.0681]
Loss:[0.0673]
Loss:[0.0833]
Loss:[0.0844]
Loss:[0.0737]
Loss:[0.0750]
Loss:[0.0756]
Loss:[0.0711]
Loss:[0.0686]
Loss:[0.0699]
Loss:[0.0662]
Loss:[0.0737]
Loss:[0.0613]
Loss:[0.0695]
Loss:[0.0678]
Loss:[0.0676]
Loss:[0.0663]
Loss:[0.0642]
Loss:[0.0675]
Loss:[0.0691]
Loss:[0.0634]
Loss:[0.0620]
Loss:[0.0629]
Loss:[0.0608]
Loss:[0.0656]
Loss:[0.0675]
Loss:[0.0679]
Loss:[0.0587]
Loss:[0.0615]
Loss:[0.0693]
Loss:[0.0633]
Loss:[0.0702]
Loss:[0.0663]
Loss:[0.0628]
Loss:[0.0576]
Loss:[0.0605]
Loss:[0.0604]
Loss:[0.0651]
Loss:[0.0546]
Loss:[0.0525]
Loss:[0.0587]
Loss:[0.0582]
Loss:[0.0610]
Loss:[0.0594]
Loss:[0.0746]
Loss:[0.0593]
Loss:[0.0679]
Loss:[0.0650]
Loss:[0.0596]
Loss:[0.0553]
Loss:[0.0682]
Loss:[0.0622]
Loss:[0.0563]
Loss:[0.0487]
Loss:[0.0607]
Loss:[0.0577]
Loss:[0.0521]
Loss:[0.0572]
Loss:[0.0559]
Loss:[0.0586]
Loss:[0.0543]
Loss:[0.0545]
Loss:[0.0594]
Loss:[0.0605]
Loss:[0.0511]
Loss:[0.0567]
Loss:[0.0559]
Loss:[0.0571]
Loss:[0.0520]
Loss:[0.0504]
Loss:[0.0532]
Loss:[0.0457]
Loss:[0.0483]
Loss:[0.0540]
Loss:[0.0509]
Loss:[0.0529]
Loss:[0.0519]
Loss:[0.0492]
Loss:[0.0446]
Loss:[0.0464]
Loss:[0.0537]
Loss:[0.0447]
Loss:[0.0523]
Loss:[0.0528]
Loss:[0.0526]
Loss:[0.0414]
Loss:[0.0463]
Loss:[0.0419]
Loss:[0.0432]
Loss:[0.0466]
Loss:[0.0497]
Loss:[0.0495]
Loss:[0.0379]
Loss:[0.0503]
Loss:[0.0460]
Loss:[0.0460]
Loss:[0.0515]
Loss:[0.0367]
Loss:[0.0448]
Loss:[0.0459]
Loss:[0.0554]
Loss:[0.0467]
Loss:[0.0485]
Loss:[0.0506]
Loss:[0.0359]
Loss:[0.0377]
Loss:[0.0362]
Loss:[0.0380]
Loss:[0.0438]
Loss:[0.0443]
Loss:[0.0457]
Loss:[0.0320]
Loss:[0.0399]
Loss:[0.0419]
Loss:[0.0451]
Loss:[0.0360]
Loss:[0.0444]
Loss:[0.0375]
Loss:[0.0367]
Loss:[0.0356]
Loss:[0.0334]
Loss:[0.0390]
Loss:[0.0371]
Loss:[0.0345]
Loss:[0.0375]
Loss:[0.0384]
Loss:[0.0385]
Loss:[0.0393]
Loss:[0.0459]
Loss:[0.0440]
Loss:[0.0358]
Loss:[0.0391]
Early stopping!
Loading 303th epoch
acc:[0.8200]
acc:[0.8210]
acc:[0.8230]
acc:[0.8200]
acc:[0.8220]
acc:[0.8250]
acc:[0.8200]
acc:[0.8190]
acc:[0.8180]
acc:[0.8210]
acc:[0.8220]
acc:[0.8240]
acc:[0.8180]
acc:[0.8210]
acc:[0.8180]
acc:[0.8190]
acc:[0.8190]
acc:[0.8220]
acc:[0.8200]
acc:[0.8190]
acc:[0.8240]
acc:[0.8150]
acc:[0.8220]
acc:[0.8200]
acc:[0.8240]
acc:[0.8220]
acc:[0.8200]
acc:[0.8200]
acc:[0.8200]
acc:[0.8190]
acc:[0.8230]
acc:[0.8190]
acc:[0.8220]
acc:[0.8210]
acc:[0.8220]
acc:[0.8210]
acc:[0.8200]
acc:[0.8210]
acc:[0.8200]
acc:[0.8170]
acc:[0.8190]
acc:[0.8220]
acc:[0.8210]
acc:[0.8210]
acc:[0.8220]
acc:[0.8250]
acc:[0.8200]
acc:[0.8220]
acc:[0.8210]
acc:[0.8220]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8208]
Mean:[82.0760]
Std :[0.1985]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='cora', drop_percent=0.4, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6928]
Loss:[0.6886]
Loss:[0.6877]
Loss:[0.6841]
Loss:[0.6802]
Loss:[0.6779]
Loss:[0.6709]
Loss:[0.6667]
Loss:[0.6600]
Loss:[0.6529]
Loss:[0.6461]
Loss:[0.6357]
Loss:[0.6299]
Loss:[0.6160]
Loss:[0.6100]
Loss:[0.5963]
Loss:[0.5888]
Loss:[0.5770]
Loss:[0.5615]
Loss:[0.5502]
Loss:[0.5397]
Loss:[0.5258]
Loss:[0.5110]
Loss:[0.5033]
Loss:[0.4929]
Loss:[0.4753]
Loss:[0.4659]
Loss:[0.4498]
Loss:[0.4379]
Loss:[0.4275]
Loss:[0.4237]
Loss:[0.4101]
Loss:[0.3948]
Loss:[0.3873]
Loss:[0.3738]
Loss:[0.3664]
Loss:[0.3502]
Loss:[0.3445]
Loss:[0.3368]
Loss:[0.3320]
Loss:[0.3197]
Loss:[0.3132]
Loss:[0.3127]
Loss:[0.3033]
Loss:[0.2967]
Loss:[0.2891]
Loss:[0.2916]
Loss:[0.2869]
Loss:[0.2776]
Loss:[0.2651]
Loss:[0.2609]
Loss:[0.2627]
Loss:[0.2672]
Loss:[0.2578]
Loss:[0.2477]
Loss:[0.2538]
Loss:[0.2312]
Loss:[0.2459]
Loss:[0.2342]
Loss:[0.2245]
Loss:[0.2277]
Loss:[0.2229]
Loss:[0.2083]
Loss:[0.2130]
Loss:[0.2222]
Loss:[0.2086]
Loss:[0.2119]
Loss:[0.2049]
Loss:[0.2085]
Loss:[0.1902]
Loss:[0.1952]
Loss:[0.1900]
Loss:[0.1957]
Loss:[0.1925]
Loss:[0.1936]
Loss:[0.1940]
Loss:[0.1965]
Loss:[0.1850]
Loss:[0.1798]
Loss:[0.1773]
Loss:[0.1767]
Loss:[0.1677]
Loss:[0.1694]
Loss:[0.1688]
Loss:[0.1694]
Loss:[0.1663]
Loss:[0.1586]
Loss:[0.1711]
Loss:[0.1521]
Loss:[0.1613]
Loss:[0.1514]
Loss:[0.1575]
Loss:[0.1653]
Loss:[0.1515]
Loss:[0.1599]
Loss:[0.1537]
Loss:[0.1460]
Loss:[0.1473]
Loss:[0.1516]
Loss:[0.1505]
Loss:[0.1442]
Loss:[0.1379]
Loss:[0.1430]
Loss:[0.1334]
Loss:[0.1425]
Loss:[0.1399]
Loss:[0.1371]
Loss:[0.1309]
Loss:[0.1339]
Loss:[0.1296]
Loss:[0.1377]
Loss:[0.1334]
Loss:[0.1298]
Loss:[0.1259]
Loss:[0.1168]
Loss:[0.1249]
Loss:[0.1229]
Loss:[0.1243]
Loss:[0.1285]
Loss:[0.1117]
Loss:[0.1335]
Loss:[0.1266]
Loss:[0.1269]
Loss:[0.1258]
Loss:[0.1125]
Loss:[0.1316]
Loss:[0.1203]
Loss:[0.1270]
Loss:[0.1258]
Loss:[0.1189]
Loss:[0.1153]
Loss:[0.1227]
Loss:[0.1300]
Loss:[0.1031]
Loss:[0.1109]
Loss:[0.1104]
Loss:[0.1175]
Loss:[0.1102]
Loss:[0.1094]
Loss:[0.1180]
Loss:[0.1088]
Loss:[0.1052]
Loss:[0.1240]
Loss:[0.0988]
Loss:[0.1340]
Loss:[0.1179]
Loss:[0.1260]
Loss:[0.1246]
Loss:[0.0950]
Loss:[0.1049]
Loss:[0.0990]
Loss:[0.1097]
Loss:[0.0908]
Loss:[0.1066]
Loss:[0.0928]
Loss:[0.1213]
Loss:[0.1002]
Loss:[0.1027]
Loss:[0.0988]
Loss:[0.0973]
Loss:[0.0964]
Loss:[0.0919]
Loss:[0.0876]
Loss:[0.0934]
Loss:[0.0891]
Loss:[0.0813]
Loss:[0.0866]
Loss:[0.0932]
Loss:[0.0909]
Loss:[0.0906]
Loss:[0.0887]
Loss:[0.0812]
Loss:[0.0828]
Loss:[0.0871]
Loss:[0.0867]
Loss:[0.0856]
Loss:[0.0763]
Loss:[0.0831]
Loss:[0.0859]
Loss:[0.0805]
Loss:[0.0895]
Loss:[0.0771]
Loss:[0.0749]
Loss:[0.0805]
Loss:[0.0794]
Loss:[0.0753]
Loss:[0.0735]
Loss:[0.0766]
Loss:[0.0788]
Loss:[0.0735]
Loss:[0.0778]
Loss:[0.0714]
Loss:[0.0706]
Loss:[0.0816]
Loss:[0.0814]
Loss:[0.0732]
Loss:[0.0730]
Loss:[0.0727]
Loss:[0.0728]
Loss:[0.0706]
Loss:[0.0721]
Loss:[0.0657]
Loss:[0.0733]
Loss:[0.0662]
Loss:[0.0699]
Loss:[0.0713]
Loss:[0.0681]
Loss:[0.0692]
Loss:[0.0662]
Loss:[0.0710]
Loss:[0.0699]
Loss:[0.0682]
Loss:[0.0647]
Loss:[0.0679]
Loss:[0.0623]
Loss:[0.0659]
Loss:[0.0670]
Loss:[0.0677]
Loss:[0.0606]
Loss:[0.0653]
Loss:[0.0682]
Loss:[0.0670]
Loss:[0.0683]
Loss:[0.0633]
Loss:[0.0629]
Loss:[0.0589]
Loss:[0.0644]
Loss:[0.0621]
Loss:[0.0620]
Loss:[0.0548]
Loss:[0.0536]
Loss:[0.0632]
Loss:[0.0592]
Loss:[0.0633]
Loss:[0.0625]
Loss:[0.0724]
Loss:[0.0588]
Loss:[0.0709]
Loss:[0.0654]
Loss:[0.0612]
Loss:[0.0582]
Loss:[0.0700]
Loss:[0.0599]
Loss:[0.0612]
Loss:[0.0471]
Loss:[0.0634]
Loss:[0.0552]
Loss:[0.0526]
Loss:[0.0596]
Loss:[0.0492]
Loss:[0.0584]
Loss:[0.0540]
Loss:[0.0558]
Loss:[0.0613]
Loss:[0.0604]
Loss:[0.0535]
Loss:[0.0596]
Loss:[0.0585]
Loss:[0.0604]
Loss:[0.0547]
Loss:[0.0567]
Loss:[0.0539]
Loss:[0.0498]
Loss:[0.0528]
Loss:[0.0561]
Early stopping!
Loading 245th epoch
acc:[0.8150]
acc:[0.8160]
acc:[0.8180]
acc:[0.8160]
acc:[0.8120]
acc:[0.8160]
acc:[0.8160]
acc:[0.8150]
acc:[0.8130]
acc:[0.8190]
acc:[0.8120]
acc:[0.8150]
acc:[0.8130]
acc:[0.8150]
acc:[0.8140]
acc:[0.8140]
acc:[0.8140]
acc:[0.8190]
acc:[0.8170]
acc:[0.8170]
acc:[0.8170]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8180]
acc:[0.8180]
acc:[0.8170]
acc:[0.8160]
acc:[0.8170]
acc:[0.8120]
acc:[0.8140]
acc:[0.8170]
acc:[0.8150]
acc:[0.8160]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8170]
acc:[0.8160]
acc:[0.8150]
acc:[0.8180]
acc:[0.8160]
acc:[0.8160]
acc:[0.8170]
acc:[0.8190]
acc:[0.8180]
acc:[0.8150]
acc:[0.8170]
acc:[0.8170]
acc:[0.8160]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8157]
Mean:[81.5720]
Std :[0.1807]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='cora', drop_percent=0.5, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6890]
Loss:[0.6883]
Loss:[0.6851]
Loss:[0.6813]
Loss:[0.6796]
Loss:[0.6730]
Loss:[0.6688]
Loss:[0.6632]
Loss:[0.6556]
Loss:[0.6501]
Loss:[0.6397]
Loss:[0.6352]
Loss:[0.6212]
Loss:[0.6175]
Loss:[0.6023]
Loss:[0.5982]
Loss:[0.5842]
Loss:[0.5716]
Loss:[0.5620]
Loss:[0.5490]
Loss:[0.5374]
Loss:[0.5257]
Loss:[0.5150]
Loss:[0.5042]
Loss:[0.4882]
Loss:[0.4853]
Loss:[0.4662]
Loss:[0.4503]
Loss:[0.4426]
Loss:[0.4400]
Loss:[0.4252]
Loss:[0.4156]
Loss:[0.4091]
Loss:[0.3896]
Loss:[0.3822]
Loss:[0.3728]
Loss:[0.3675]
Loss:[0.3580]
Loss:[0.3505]
Loss:[0.3427]
Loss:[0.3321]
Loss:[0.3320]
Loss:[0.3240]
Loss:[0.3179]
Loss:[0.3126]
Loss:[0.3075]
Loss:[0.3015]
Loss:[0.2959]
Loss:[0.2855]
Loss:[0.2824]
Loss:[0.2795]
Loss:[0.2836]
Loss:[0.2744]
Loss:[0.2653]
Loss:[0.2739]
Loss:[0.2475]
Loss:[0.2597]
Loss:[0.2530]
Loss:[0.2448]
Loss:[0.2456]
Loss:[0.2405]
Loss:[0.2242]
Loss:[0.2296]
Loss:[0.2378]
Loss:[0.2247]
Loss:[0.2271]
Loss:[0.2199]
Loss:[0.2226]
Loss:[0.2061]
Loss:[0.2121]
Loss:[0.2047]
Loss:[0.2097]
Loss:[0.2075]
Loss:[0.2065]
Loss:[0.2046]
Loss:[0.2079]
Loss:[0.1974]
Loss:[0.1919]
Loss:[0.1901]
Loss:[0.1880]
Loss:[0.1816]
Loss:[0.1809]
Loss:[0.1845]
Loss:[0.1782]
Loss:[0.1804]
Loss:[0.1688]
Loss:[0.1799]
Loss:[0.1600]
Loss:[0.1697]
Loss:[0.1607]
Loss:[0.1693]
Loss:[0.1705]
Loss:[0.1617]
Loss:[0.1665]
Loss:[0.1620]
Loss:[0.1569]
Loss:[0.1561]
Loss:[0.1632]
Loss:[0.1599]
Loss:[0.1509]
Loss:[0.1484]
Loss:[0.1535]
Loss:[0.1409]
Loss:[0.1510]
Loss:[0.1480]
Loss:[0.1493]
Loss:[0.1387]
Loss:[0.1438]
Loss:[0.1367]
Loss:[0.1468]
Loss:[0.1406]
Loss:[0.1397]
Loss:[0.1332]
Loss:[0.1238]
Loss:[0.1316]
Loss:[0.1306]
Loss:[0.1356]
Loss:[0.1349]
Loss:[0.1187]
Loss:[0.1401]
Loss:[0.1389]
Loss:[0.1373]
Loss:[0.1312]
Loss:[0.1253]
Loss:[0.1447]
Loss:[0.1217]
Loss:[0.1335]
Loss:[0.1394]
Loss:[0.1256]
Loss:[0.1264]
Loss:[0.1372]
Loss:[0.1347]
Loss:[0.1306]
Loss:[0.1399]
Loss:[0.1127]
Loss:[0.1463]
Loss:[0.1262]
Loss:[0.1237]
Loss:[0.1290]
Loss:[0.1141]
Loss:[0.1021]
Loss:[0.1098]
Loss:[0.1146]
Loss:[0.1120]
Loss:[0.1149]
Loss:[0.1181]
Loss:[0.1175]
Loss:[0.1026]
Loss:[0.1047]
Loss:[0.1042]
Loss:[0.1007]
Loss:[0.0973]
Loss:[0.0978]
Loss:[0.1009]
Loss:[0.1153]
Loss:[0.1041]
Loss:[0.1015]
Loss:[0.1015]
Loss:[0.0990]
Loss:[0.1023]
Loss:[0.0942]
Loss:[0.0910]
Loss:[0.0986]
Loss:[0.0880]
Loss:[0.0879]
Loss:[0.0885]
Loss:[0.0956]
Loss:[0.0962]
Loss:[0.0982]
Loss:[0.0920]
Loss:[0.0825]
Loss:[0.0849]
Loss:[0.0934]
Loss:[0.0869]
Loss:[0.0904]
Loss:[0.0811]
Loss:[0.0844]
Loss:[0.0874]
Loss:[0.0855]
Loss:[0.0924]
Loss:[0.0808]
Loss:[0.0814]
Loss:[0.0822]
Loss:[0.0799]
Loss:[0.0793]
Loss:[0.0763]
Loss:[0.0794]
Loss:[0.0860]
Loss:[0.0792]
Loss:[0.0804]
Loss:[0.0773]
Loss:[0.0754]
Loss:[0.0824]
Loss:[0.0868]
Loss:[0.0774]
Loss:[0.0793]
Loss:[0.0757]
Loss:[0.0744]
Loss:[0.0709]
Loss:[0.0746]
Loss:[0.0672]
Loss:[0.0726]
Loss:[0.0704]
Loss:[0.0735]
Loss:[0.0769]
Loss:[0.0705]
Loss:[0.0711]
Loss:[0.0658]
Loss:[0.0706]
Loss:[0.0708]
Loss:[0.0710]
Loss:[0.0692]
Loss:[0.0725]
Loss:[0.0651]
Loss:[0.0702]
Loss:[0.0674]
Loss:[0.0691]
Loss:[0.0626]
Loss:[0.0695]
Loss:[0.0702]
Loss:[0.0715]
Loss:[0.0707]
Loss:[0.0636]
Loss:[0.0630]
Loss:[0.0653]
Loss:[0.0660]
Loss:[0.0633]
Loss:[0.0661]
Loss:[0.0557]
Loss:[0.0535]
Loss:[0.0644]
Loss:[0.0617]
Loss:[0.0650]
Loss:[0.0628]
Loss:[0.0734]
Loss:[0.0597]
Loss:[0.0728]
Loss:[0.0645]
Loss:[0.0615]
Loss:[0.0573]
Loss:[0.0702]
Loss:[0.0593]
Loss:[0.0633]
Loss:[0.0473]
Loss:[0.0624]
Loss:[0.0561]
Loss:[0.0564]
Loss:[0.0615]
Loss:[0.0522]
Loss:[0.0580]
Loss:[0.0546]
Loss:[0.0559]
Loss:[0.0609]
Loss:[0.0618]
Loss:[0.0542]
Loss:[0.0588]
Loss:[0.0585]
Loss:[0.0616]
Loss:[0.0534]
Loss:[0.0551]
Loss:[0.0513]
Loss:[0.0511]
Loss:[0.0500]
Loss:[0.0570]
Early stopping!
Loading 245th epoch
acc:[0.8050]
acc:[0.8010]
acc:[0.8070]
acc:[0.8020]
acc:[0.8070]
acc:[0.8070]
acc:[0.8050]
acc:[0.8050]
acc:[0.8030]
acc:[0.8070]
acc:[0.8050]
acc:[0.8060]
acc:[0.8040]
acc:[0.8050]
acc:[0.8000]
acc:[0.8060]
acc:[0.8050]
acc:[0.8060]
acc:[0.8070]
acc:[0.8060]
acc:[0.8090]
acc:[0.8050]
acc:[0.8070]
acc:[0.8050]
acc:[0.8080]
acc:[0.8060]
acc:[0.8060]
acc:[0.8080]
acc:[0.8050]
acc:[0.8080]
acc:[0.8060]
acc:[0.8060]
acc:[0.8040]
acc:[0.8060]
acc:[0.8070]
acc:[0.8040]
acc:[0.8070]
acc:[0.8070]
acc:[0.8050]
acc:[0.8040]
acc:[0.8050]
acc:[0.8060]
acc:[0.8060]
acc:[0.8040]
acc:[0.8060]
acc:[0.8060]
acc:[0.8040]
acc:[0.8090]
acc:[0.8070]
acc:[0.8050]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8056]
Mean:[80.5600]
Std :[0.1773]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='cora', drop_percent=0.6, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6931]
Loss:[0.6892]
Loss:[0.6886]
Loss:[0.6856]
Loss:[0.6820]
Loss:[0.6804]
Loss:[0.6743]
Loss:[0.6701]
Loss:[0.6648]
Loss:[0.6576]
Loss:[0.6524]
Loss:[0.6424]
Loss:[0.6382]
Loss:[0.6252]
Loss:[0.6214]
Loss:[0.6074]
Loss:[0.6030]
Loss:[0.5896]
Loss:[0.5780]
Loss:[0.5686]
Loss:[0.5557]
Loss:[0.5461]
Loss:[0.5342]
Loss:[0.5228]
Loss:[0.5135]
Loss:[0.4989]
Loss:[0.4938]
Loss:[0.4747]
Loss:[0.4609]
Loss:[0.4525]
Loss:[0.4493]
Loss:[0.4357]
Loss:[0.4275]
Loss:[0.4197]
Loss:[0.4013]
Loss:[0.3946]
Loss:[0.3833]
Loss:[0.3776]
Loss:[0.3685]
Loss:[0.3617]
Loss:[0.3530]
Loss:[0.3458]
Loss:[0.3433]
Loss:[0.3350]
Loss:[0.3274]
Loss:[0.3225]
Loss:[0.3173]
Loss:[0.3082]
Loss:[0.3060]
Loss:[0.2969]
Loss:[0.2933]
Loss:[0.2902]
Loss:[0.2916]
Loss:[0.2817]
Loss:[0.2759]
Loss:[0.2852]
Loss:[0.2571]
Loss:[0.2687]
Loss:[0.2569]
Loss:[0.2534]
Loss:[0.2507]
Loss:[0.2506]
Loss:[0.2285]
Loss:[0.2391]
Loss:[0.2503]
Loss:[0.2368]
Loss:[0.2314]
Loss:[0.2256]
Loss:[0.2327]
Loss:[0.2146]
Loss:[0.2184]
Loss:[0.2141]
Loss:[0.2184]
Loss:[0.2098]
Loss:[0.2113]
Loss:[0.2104]
Loss:[0.2221]
Loss:[0.2036]
Loss:[0.1963]
Loss:[0.1990]
Loss:[0.1984]
Loss:[0.1889]
Loss:[0.1867]
Loss:[0.1904]
Loss:[0.1847]
Loss:[0.1868]
Loss:[0.1797]
Loss:[0.1885]
Loss:[0.1664]
Loss:[0.1767]
Loss:[0.1700]
Loss:[0.1727]
Loss:[0.1791]
Loss:[0.1683]
Loss:[0.1695]
Loss:[0.1752]
Loss:[0.1700]
Loss:[0.1582]
Loss:[0.1675]
Loss:[0.1674]
Loss:[0.1562]
Loss:[0.1541]
Loss:[0.1593]
Loss:[0.1481]
Loss:[0.1540]
Loss:[0.1599]
Loss:[0.1535]
Loss:[0.1530]
Loss:[0.1512]
Loss:[0.1410]
Loss:[0.1585]
Loss:[0.1473]
Loss:[0.1463]
Loss:[0.1439]
Loss:[0.1283]
Loss:[0.1358]
Loss:[0.1393]
Loss:[0.1318]
Loss:[0.1396]
Loss:[0.1260]
Loss:[0.1418]
Loss:[0.1387]
Loss:[0.1288]
Loss:[0.1329]
Loss:[0.1249]
Loss:[0.1369]
Loss:[0.1234]
Loss:[0.1342]
Loss:[0.1356]
Loss:[0.1312]
Loss:[0.1256]
Loss:[0.1257]
Loss:[0.1405]
Loss:[0.1121]
Loss:[0.1191]
Loss:[0.1119]
Loss:[0.1225]
Loss:[0.1171]
Loss:[0.1133]
Loss:[0.1192]
Loss:[0.1180]
Loss:[0.1015]
Loss:[0.1154]
Loss:[0.1050]
Loss:[0.1123]
Loss:[0.1116]
Loss:[0.1215]
Loss:[0.1116]
Loss:[0.1023]
Loss:[0.1017]
Loss:[0.1090]
Loss:[0.1012]
Loss:[0.1020]
Loss:[0.1007]
Loss:[0.1052]
Loss:[0.1132]
Loss:[0.1020]
Loss:[0.1067]
Loss:[0.1013]
Loss:[0.1009]
Loss:[0.1024]
Loss:[0.0962]
Loss:[0.0923]
Loss:[0.0979]
Loss:[0.0919]
Loss:[0.0920]
Loss:[0.0894]
Loss:[0.0971]
Loss:[0.0984]
Loss:[0.0957]
Loss:[0.0954]
Loss:[0.0857]
Loss:[0.0886]
Loss:[0.0941]
Loss:[0.0925]
Loss:[0.0928]
Loss:[0.0832]
Loss:[0.0849]
Loss:[0.0892]
Loss:[0.0855]
Loss:[0.0951]
Loss:[0.0833]
Loss:[0.0795]
Loss:[0.0812]
Loss:[0.0793]
Loss:[0.0805]
Loss:[0.0769]
Loss:[0.0788]
Loss:[0.0892]
Loss:[0.0774]
Loss:[0.0796]
Loss:[0.0768]
Loss:[0.0759]
Loss:[0.0822]
Loss:[0.0877]
Loss:[0.0751]
Loss:[0.0803]
Loss:[0.0782]
Loss:[0.0745]
Loss:[0.0695]
Loss:[0.0752]
Loss:[0.0693]
Loss:[0.0708]
Loss:[0.0700]
Loss:[0.0720]
Loss:[0.0727]
Loss:[0.0684]
Loss:[0.0723]
Loss:[0.0643]
Loss:[0.0660]
Loss:[0.0674]
Loss:[0.0666]
Loss:[0.0695]
Loss:[0.0692]
Loss:[0.0658]
Loss:[0.0696]
Loss:[0.0644]
Loss:[0.0715]
Loss:[0.0606]
Loss:[0.0734]
Loss:[0.0711]
Loss:[0.0663]
Loss:[0.0712]
Loss:[0.0635]
Loss:[0.0591]
Loss:[0.0603]
Loss:[0.0626]
Loss:[0.0610]
Loss:[0.0633]
Loss:[0.0564]
Loss:[0.0546]
Loss:[0.0609]
Loss:[0.0611]
Loss:[0.0646]
Loss:[0.0622]
Loss:[0.0678]
Loss:[0.0577]
Loss:[0.0648]
Loss:[0.0609]
Loss:[0.0604]
Loss:[0.0530]
Loss:[0.0651]
Loss:[0.0547]
Loss:[0.0561]
Loss:[0.0455]
Loss:[0.0628]
Loss:[0.0525]
Loss:[0.0519]
Loss:[0.0560]
Loss:[0.0504]
Loss:[0.0566]
Loss:[0.0527]
Loss:[0.0519]
Loss:[0.0567]
Loss:[0.0560]
Loss:[0.0505]
Loss:[0.0576]
Loss:[0.0537]
Loss:[0.0600]
Loss:[0.0486]
Loss:[0.0526]
Loss:[0.0484]
Loss:[0.0460]
Loss:[0.0500]
Loss:[0.0529]
Early stopping!
Loading 245th epoch
acc:[0.8050]
acc:[0.8080]
acc:[0.8110]
acc:[0.8060]
acc:[0.8100]
acc:[0.8100]
acc:[0.8080]
acc:[0.8100]
acc:[0.8070]
acc:[0.8060]
acc:[0.8090]
acc:[0.8100]
acc:[0.8080]
acc:[0.8100]
acc:[0.8050]
acc:[0.8100]
acc:[0.8110]
acc:[0.8080]
acc:[0.8090]
acc:[0.8070]
acc:[0.8110]
acc:[0.8070]
acc:[0.8060]
acc:[0.8070]
acc:[0.8100]
acc:[0.8070]
acc:[0.8090]
acc:[0.8100]
acc:[0.8090]
acc:[0.8050]
acc:[0.8070]
acc:[0.8090]
acc:[0.8120]
acc:[0.8080]
acc:[0.8080]
acc:[0.8080]
acc:[0.8050]
acc:[0.8080]
acc:[0.8090]
acc:[0.8090]
acc:[0.8090]
acc:[0.8070]
acc:[0.8080]
acc:[0.8090]
acc:[0.8110]
acc:[0.8080]
acc:[0.8080]
acc:[0.8120]
acc:[0.8080]
acc:[0.8080]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8084]
Mean:[80.8400]
Std :[0.1784]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='cora', drop_percent=0.02, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6913]
Loss:[0.6873]
Loss:[0.6844]
Loss:[0.6788]
Loss:[0.6761]
Loss:[0.6685]
Loss:[0.6638]
Loss:[0.6547]
Loss:[0.6478]
Loss:[0.6380]
Loss:[0.6294]
Loss:[0.6164]
Loss:[0.6095]
Loss:[0.5924]
Loss:[0.5837]
Loss:[0.5689]
Loss:[0.5568]
Loss:[0.5447]
Loss:[0.5272]
Loss:[0.5139]
Loss:[0.5029]
Loss:[0.4872]
Loss:[0.4685]
Loss:[0.4609]
Loss:[0.4493]
Loss:[0.4310]
Loss:[0.4240]
Loss:[0.4003]
Loss:[0.3896]
Loss:[0.3794]
Loss:[0.3736]
Loss:[0.3609]
Loss:[0.3447]
Loss:[0.3362]
Loss:[0.3228]
Loss:[0.3181]
Loss:[0.2972]
Loss:[0.2923]
Loss:[0.2846]
Loss:[0.2832]
Loss:[0.2727]
Loss:[0.2653]
Loss:[0.2633]
Loss:[0.2569]
Loss:[0.2496]
Loss:[0.2368]
Loss:[0.2423]
Loss:[0.2412]
Loss:[0.2258]
Loss:[0.2207]
Loss:[0.2190]
Loss:[0.2136]
Loss:[0.2233]
Loss:[0.2110]
Loss:[0.1970]
Loss:[0.2151]
Loss:[0.1888]
Loss:[0.2036]
Loss:[0.1963]
Loss:[0.1835]
Loss:[0.1904]
Loss:[0.1816]
Loss:[0.1693]
Loss:[0.1810]
Loss:[0.1849]
Loss:[0.1757]
Loss:[0.1777]
Loss:[0.1660]
Loss:[0.1727]
Loss:[0.1501]
Loss:[0.1609]
Loss:[0.1560]
Loss:[0.1623]
Loss:[0.1573]
Loss:[0.1546]
Loss:[0.1564]
Loss:[0.1591]
Loss:[0.1447]
Loss:[0.1434]
Loss:[0.1454]
Loss:[0.1443]
Loss:[0.1355]
Loss:[0.1373]
Loss:[0.1337]
Loss:[0.1342]
Loss:[0.1329]
Loss:[0.1229]
Loss:[0.1348]
Loss:[0.1187]
Loss:[0.1295]
Loss:[0.1167]
Loss:[0.1260]
Loss:[0.1302]
Loss:[0.1209]
Loss:[0.1302]
Loss:[0.1233]
Loss:[0.1157]
Loss:[0.1187]
Loss:[0.1181]
Loss:[0.1190]
Loss:[0.1108]
Loss:[0.1109]
Loss:[0.1162]
Loss:[0.1087]
Loss:[0.1107]
Loss:[0.1146]
Loss:[0.1160]
Loss:[0.1035]
Loss:[0.1103]
Loss:[0.1038]
Loss:[0.1158]
Loss:[0.1057]
Loss:[0.1051]
Loss:[0.1012]
Loss:[0.0934]
Loss:[0.1039]
Loss:[0.0984]
Loss:[0.0988]
Loss:[0.1014]
Loss:[0.0879]
Loss:[0.1075]
Loss:[0.0998]
Loss:[0.0934]
Loss:[0.0958]
Loss:[0.0863]
Loss:[0.0965]
Loss:[0.0892]
Loss:[0.0990]
Loss:[0.1043]
Loss:[0.1006]
Loss:[0.0886]
Loss:[0.0998]
Loss:[0.1033]
Loss:[0.0842]
Loss:[0.0895]
Loss:[0.0815]
Loss:[0.0890]
Loss:[0.0888]
Loss:[0.0840]
Loss:[0.0943]
Loss:[0.0870]
Loss:[0.0727]
Loss:[0.0846]
Loss:[0.0799]
Loss:[0.0857]
Loss:[0.0850]
Loss:[0.0894]
Loss:[0.0841]
Loss:[0.0708]
Loss:[0.0739]
Loss:[0.0822]
Loss:[0.0747]
Loss:[0.0756]
Loss:[0.0722]
Loss:[0.0742]
Loss:[0.0928]
Loss:[0.0812]
Loss:[0.0808]
Loss:[0.0786]
Loss:[0.0742]
Loss:[0.0773]
Loss:[0.0733]
Loss:[0.0666]
Loss:[0.0724]
Loss:[0.0691]
Loss:[0.0642]
Loss:[0.0684]
Loss:[0.0735]
Loss:[0.0738]
Loss:[0.0751]
Loss:[0.0727]
Loss:[0.0650]
Loss:[0.0696]
Loss:[0.0722]
Loss:[0.0748]
Loss:[0.0768]
Loss:[0.0624]
Loss:[0.0692]
Loss:[0.0681]
Loss:[0.0628]
Loss:[0.0713]
Loss:[0.0668]
Loss:[0.0592]
Loss:[0.0613]
Loss:[0.0640]
Loss:[0.0620]
Loss:[0.0642]
Loss:[0.0631]
Loss:[0.0660]
Loss:[0.0600]
Loss:[0.0668]
Loss:[0.0564]
Loss:[0.0581]
Loss:[0.0713]
Loss:[0.0713]
Loss:[0.0630]
Loss:[0.0611]
Loss:[0.0686]
Loss:[0.0569]
Loss:[0.0565]
Loss:[0.0624]
Loss:[0.0546]
Loss:[0.0655]
Loss:[0.0507]
Loss:[0.0607]
Loss:[0.0607]
Loss:[0.0589]
Loss:[0.0560]
Loss:[0.0499]
Loss:[0.0602]
Loss:[0.0570]
Loss:[0.0532]
Loss:[0.0563]
Loss:[0.0537]
Loss:[0.0518]
Loss:[0.0529]
Loss:[0.0557]
Loss:[0.0573]
Loss:[0.0488]
Loss:[0.0566]
Loss:[0.0569]
Loss:[0.0551]
Loss:[0.0629]
Loss:[0.0521]
Loss:[0.0562]
Loss:[0.0481]
Loss:[0.0516]
Loss:[0.0519]
Loss:[0.0494]
Loss:[0.0465]
Loss:[0.0428]
Loss:[0.0512]
Loss:[0.0468]
Loss:[0.0534]
Loss:[0.0487]
Loss:[0.0638]
Loss:[0.0481]
Loss:[0.0561]
Loss:[0.0584]
Loss:[0.0488]
Loss:[0.0484]
Loss:[0.0557]
Loss:[0.0491]
Loss:[0.0509]
Loss:[0.0393]
Loss:[0.0526]
Loss:[0.0471]
Loss:[0.0400]
Loss:[0.0500]
Loss:[0.0435]
Loss:[0.0489]
Loss:[0.0433]
Loss:[0.0478]
Loss:[0.0508]
Loss:[0.0503]
Loss:[0.0423]
Loss:[0.0472]
Loss:[0.0516]
Loss:[0.0491]
Loss:[0.0446]
Loss:[0.0445]
Loss:[0.0426]
Loss:[0.0378]
Loss:[0.0420]
Loss:[0.0458]
Loss:[0.0428]
Loss:[0.0436]
Loss:[0.0453]
Loss:[0.0436]
Loss:[0.0375]
Loss:[0.0425]
Loss:[0.0461]
Loss:[0.0406]
Loss:[0.0489]
Loss:[0.0498]
Loss:[0.0518]
Loss:[0.0384]
Loss:[0.0494]
Loss:[0.0364]
Loss:[0.0505]
Loss:[0.0481]
Loss:[0.0622]
Loss:[0.0419]
Loss:[0.0467]
Loss:[0.0547]
Loss:[0.0526]
Loss:[0.0392]
Loss:[0.0563]
Loss:[0.0354]
Loss:[0.0691]
Loss:[0.0374]
Loss:[0.0653]
Loss:[0.0530]
Loss:[0.0604]
Loss:[0.0428]
Loss:[0.0741]
Loss:[0.0550]
Loss:[0.0812]
Loss:[0.0389]
Loss:[0.0705]
Loss:[0.0399]
Loss:[0.0630]
Loss:[0.0316]
Loss:[0.0339]
Loss:[0.0456]
Loss:[0.0409]
Loss:[0.0359]
Loss:[0.0427]
Loss:[0.0346]
Loss:[0.0351]
Loss:[0.0351]
Loss:[0.0319]
Loss:[0.0380]
Loss:[0.0282]
Loss:[0.0340]
Loss:[0.0354]
Loss:[0.0366]
Loss:[0.0334]
Loss:[0.0366]
Loss:[0.0391]
Loss:[0.0366]
Loss:[0.0298]
Loss:[0.0364]
Loss:[0.0307]
Loss:[0.0328]
Loss:[0.0409]
Loss:[0.0313]
Loss:[0.0328]
Loss:[0.0320]
Loss:[0.0331]
Loss:[0.0341]
Loss:[0.0372]
Loss:[0.0309]
Loss:[0.0346]
Early stopping!
Loading 314th epoch
acc:[0.8130]
acc:[0.8140]
acc:[0.8130]
acc:[0.8130]
acc:[0.8110]
acc:[0.8130]
acc:[0.8110]
acc:[0.8100]
acc:[0.8090]
acc:[0.8120]
acc:[0.8140]
acc:[0.8090]
acc:[0.8130]
acc:[0.8120]
acc:[0.8130]
acc:[0.8100]
acc:[0.8130]
acc:[0.8120]
acc:[0.8130]
acc:[0.8110]
acc:[0.8130]
acc:[0.8130]
acc:[0.8130]
acc:[0.8120]
acc:[0.8100]
acc:[0.8110]
acc:[0.8120]
acc:[0.8120]
acc:[0.8130]
acc:[0.8130]
acc:[0.8130]
acc:[0.8120]
acc:[0.8140]
acc:[0.8120]
acc:[0.8110]
acc:[0.8140]
acc:[0.8140]
acc:[0.8140]
acc:[0.8100]
acc:[0.8130]
acc:[0.8120]
acc:[0.8120]
acc:[0.8110]
acc:[0.8130]
acc:[0.8120]
acc:[0.8120]
acc:[0.8130]
acc:[0.8130]
acc:[0.8140]
acc:[0.8120]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8122]
Mean:[81.2240]
Std :[0.1302]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='cora', drop_percent=0.05, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6905]
Loss:[0.6874]
Loss:[0.6819]
Loss:[0.6780]
Loss:[0.6712]
Loss:[0.6667]
Loss:[0.6577]
Loss:[0.6513]
Loss:[0.6405]
Loss:[0.6337]
Loss:[0.6239]
Loss:[0.6116]
Loss:[0.6047]
Loss:[0.5878]
Loss:[0.5799]
Loss:[0.5668]
Loss:[0.5558]
Loss:[0.5427]
Loss:[0.5279]
Loss:[0.5177]
Loss:[0.5004]
Loss:[0.4873]
Loss:[0.4749]
Loss:[0.4678]
Loss:[0.4525]
Loss:[0.4374]
Loss:[0.4308]
Loss:[0.4102]
Loss:[0.3955]
Loss:[0.3869]
Loss:[0.3881]
Loss:[0.3785]
Loss:[0.3557]
Loss:[0.3437]
Loss:[0.3401]
Loss:[0.3298]
Loss:[0.3128]
Loss:[0.3068]
Loss:[0.2973]
Loss:[0.2863]
Loss:[0.2787]
Loss:[0.2738]
Loss:[0.2686]
Loss:[0.2606]
Loss:[0.2539]
Loss:[0.2519]
Loss:[0.2506]
Loss:[0.2393]
Loss:[0.2305]
Loss:[0.2269]
Loss:[0.2250]
Loss:[0.2244]
Loss:[0.2210]
Loss:[0.2095]
Loss:[0.2020]
Loss:[0.2119]
Loss:[0.1922]
Loss:[0.2033]
Loss:[0.1971]
Loss:[0.1869]
Loss:[0.1856]
Loss:[0.1817]
Loss:[0.1694]
Loss:[0.1773]
Loss:[0.1827]
Loss:[0.1673]
Loss:[0.1712]
Loss:[0.1626]
Loss:[0.1678]
Loss:[0.1584]
Loss:[0.1633]
Loss:[0.1546]
Loss:[0.1651]
Loss:[0.1565]
Loss:[0.1451]
Loss:[0.1546]
Loss:[0.1663]
Loss:[0.1438]
Loss:[0.1390]
Loss:[0.1408]
Loss:[0.1399]
Loss:[0.1342]
Loss:[0.1314]
Loss:[0.1311]
Loss:[0.1306]
Loss:[0.1328]
Loss:[0.1231]
Loss:[0.1327]
Loss:[0.1152]
Loss:[0.1265]
Loss:[0.1083]
Loss:[0.1220]
Loss:[0.1281]
Loss:[0.1128]
Loss:[0.1250]
Loss:[0.1219]
Loss:[0.1116]
Loss:[0.1124]
Loss:[0.1182]
Loss:[0.1114]
Loss:[0.1053]
Loss:[0.1101]
Loss:[0.1075]
Loss:[0.1021]
Loss:[0.1107]
Loss:[0.1079]
Loss:[0.1046]
Loss:[0.0970]
Loss:[0.1070]
Loss:[0.0948]
Loss:[0.1089]
Loss:[0.0987]
Loss:[0.0986]
Loss:[0.0961]
Loss:[0.0911]
Loss:[0.0978]
Loss:[0.0938]
Loss:[0.0937]
Loss:[0.0977]
Loss:[0.0863]
Loss:[0.1058]
Loss:[0.0894]
Loss:[0.0923]
Loss:[0.0907]
Loss:[0.0827]
Loss:[0.0920]
Loss:[0.0856]
Loss:[0.0946]
Loss:[0.0986]
Loss:[0.0899]
Loss:[0.0827]
Loss:[0.0921]
Loss:[0.0937]
Loss:[0.0821]
Loss:[0.0816]
Loss:[0.0783]
Loss:[0.0835]
Loss:[0.0810]
Loss:[0.0816]
Loss:[0.0863]
Loss:[0.0807]
Loss:[0.0672]
Loss:[0.0825]
Loss:[0.0736]
Loss:[0.0809]
Loss:[0.0840]
Loss:[0.0822]
Loss:[0.0859]
Loss:[0.0730]
Loss:[0.0667]
Loss:[0.0797]
Loss:[0.0704]
Loss:[0.0684]
Loss:[0.0666]
Loss:[0.0675]
Loss:[0.0833]
Loss:[0.0762]
Loss:[0.0699]
Loss:[0.0744]
Loss:[0.0690]
Loss:[0.0694]
Loss:[0.0670]
Loss:[0.0645]
Loss:[0.0670]
Loss:[0.0643]
Loss:[0.0625]
Loss:[0.0652]
Loss:[0.0621]
Loss:[0.0689]
Loss:[0.0697]
Loss:[0.0661]
Loss:[0.0571]
Loss:[0.0635]
Loss:[0.0646]
Loss:[0.0662]
Loss:[0.0669]
Loss:[0.0576]
Loss:[0.0623]
Loss:[0.0638]
Loss:[0.0580]
Loss:[0.0635]
Loss:[0.0590]
Loss:[0.0553]
Loss:[0.0539]
Loss:[0.0544]
Loss:[0.0588]
Loss:[0.0570]
Loss:[0.0583]
Loss:[0.0613]
Loss:[0.0550]
Loss:[0.0603]
Loss:[0.0527]
Loss:[0.0543]
Loss:[0.0660]
Loss:[0.0653]
Loss:[0.0548]
Loss:[0.0543]
Loss:[0.0582]
Loss:[0.0534]
Loss:[0.0483]
Loss:[0.0560]
Loss:[0.0487]
Loss:[0.0571]
Loss:[0.0463]
Loss:[0.0543]
Loss:[0.0541]
Loss:[0.0542]
Loss:[0.0497]
Loss:[0.0464]
Loss:[0.0545]
Loss:[0.0513]
Loss:[0.0487]
Loss:[0.0483]
Loss:[0.0498]
Loss:[0.0455]
Loss:[0.0503]
Loss:[0.0467]
Loss:[0.0482]
Loss:[0.0407]
Loss:[0.0515]
Loss:[0.0481]
Loss:[0.0468]
Loss:[0.0534]
Loss:[0.0449]
Loss:[0.0465]
Loss:[0.0445]
Loss:[0.0437]
Loss:[0.0483]
Loss:[0.0434]
Loss:[0.0394]
Loss:[0.0410]
Loss:[0.0470]
Loss:[0.0429]
Loss:[0.0482]
Loss:[0.0452]
Loss:[0.0552]
Loss:[0.0430]
Loss:[0.0522]
Loss:[0.0524]
Loss:[0.0446]
Loss:[0.0442]
Loss:[0.0477]
Loss:[0.0457]
Loss:[0.0436]
Loss:[0.0356]
Loss:[0.0448]
Loss:[0.0420]
Loss:[0.0372]
Loss:[0.0447]
Loss:[0.0378]
Loss:[0.0417]
Loss:[0.0393]
Loss:[0.0398]
Loss:[0.0435]
Loss:[0.0453]
Loss:[0.0405]
Loss:[0.0400]
Loss:[0.0437]
Loss:[0.0419]
Loss:[0.0401]
Loss:[0.0400]
Loss:[0.0375]
Loss:[0.0331]
Loss:[0.0363]
Loss:[0.0426]
Loss:[0.0366]
Loss:[0.0356]
Loss:[0.0371]
Loss:[0.0384]
Loss:[0.0346]
Loss:[0.0357]
Loss:[0.0416]
Loss:[0.0371]
Loss:[0.0411]
Loss:[0.0420]
Loss:[0.0405]
Loss:[0.0285]
Loss:[0.0460]
Loss:[0.0323]
Loss:[0.0377]
Loss:[0.0364]
Loss:[0.0469]
Loss:[0.0364]
Loss:[0.0320]
Loss:[0.0398]
Loss:[0.0382]
Loss:[0.0384]
Loss:[0.0371]
Loss:[0.0301]
Loss:[0.0406]
Loss:[0.0375]
Loss:[0.0359]
Loss:[0.0397]
Loss:[0.0280]
Loss:[0.0408]
Loss:[0.0288]
Loss:[0.0313]
Loss:[0.0270]
Loss:[0.0284]
Loss:[0.0323]
Loss:[0.0329]
Loss:[0.0383]
Loss:[0.0261]
Loss:[0.0301]
Loss:[0.0315]
Loss:[0.0344]
Loss:[0.0290]
Loss:[0.0318]
Loss:[0.0293]
Loss:[0.0289]
Loss:[0.0317]
Loss:[0.0247]
Loss:[0.0332]
Loss:[0.0241]
Loss:[0.0292]
Loss:[0.0321]
Loss:[0.0315]
Loss:[0.0283]
Loss:[0.0264]
Loss:[0.0365]
Loss:[0.0315]
Loss:[0.0277]
Loss:[0.0331]
Loss:[0.0270]
Loss:[0.0322]
Loss:[0.0362]
Loss:[0.0278]
Loss:[0.0264]
Loss:[0.0257]
Loss:[0.0268]
Loss:[0.0288]
Loss:[0.0291]
Loss:[0.0287]
Loss:[0.0290]
Early stopping!
Loading 314th epoch
acc:[0.8120]
acc:[0.8190]
acc:[0.8150]
acc:[0.8180]
acc:[0.8150]
acc:[0.8180]
acc:[0.8150]
acc:[0.8130]
acc:[0.8170]
acc:[0.8160]
acc:[0.8150]
acc:[0.8150]
acc:[0.8140]
acc:[0.8150]
acc:[0.8140]
acc:[0.8170]
acc:[0.8160]
acc:[0.8140]
acc:[0.8170]
acc:[0.8170]
acc:[0.8140]
acc:[0.8160]
acc:[0.8150]
acc:[0.8160]
acc:[0.8130]
acc:[0.8170]
acc:[0.8130]
acc:[0.8180]
acc:[0.8130]
acc:[0.8160]
acc:[0.8150]
acc:[0.8140]
acc:[0.8150]
acc:[0.8160]
acc:[0.8140]
acc:[0.8120]
acc:[0.8180]
acc:[0.8150]
acc:[0.8150]
acc:[0.8140]
acc:[0.8130]
acc:[0.8170]
acc:[0.8160]
acc:[0.8160]
acc:[0.8150]
acc:[0.8130]
acc:[0.8170]
acc:[0.8140]
acc:[0.8160]
acc:[0.8150]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8153]
Mean:[81.5260]
Std :[0.1664]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='cora', drop_percent=0.08, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6900]
Loss:[0.6854]
Loss:[0.6796]
Loss:[0.6727]
Loss:[0.6653]
Loss:[0.6597]
Loss:[0.6508]
Loss:[0.6424]
Loss:[0.6344]
Loss:[0.6263]
Loss:[0.6173]
Loss:[0.6050]
Loss:[0.5989]
Loss:[0.5869]
Loss:[0.5757]
Loss:[0.5646]
Loss:[0.5559]
Loss:[0.5425]
Loss:[0.5260]
Loss:[0.5172]
Loss:[0.5026]
Loss:[0.4904]
Loss:[0.4760]
Loss:[0.4673]
Loss:[0.4571]
Loss:[0.4399]
Loss:[0.4269]
Loss:[0.4162]
Loss:[0.4005]
Loss:[0.3926]
Loss:[0.3836]
Loss:[0.3734]
Loss:[0.3612]
Loss:[0.3526]
Loss:[0.3353]
Loss:[0.3306]
Loss:[0.3132]
Loss:[0.3115]
Loss:[0.3007]
Loss:[0.2950]
Loss:[0.2825]
Loss:[0.2773]
Loss:[0.2686]
Loss:[0.2626]
Loss:[0.2552]
Loss:[0.2459]
Loss:[0.2451]
Loss:[0.2445]
Loss:[0.2360]
Loss:[0.2207]
Loss:[0.2193]
Loss:[0.2154]
Loss:[0.2158]
Loss:[0.2048]
Loss:[0.1962]
Loss:[0.2072]
Loss:[0.1913]
Loss:[0.1958]
Loss:[0.1862]
Loss:[0.1834]
Loss:[0.1821]
Loss:[0.1695]
Loss:[0.1623]
Loss:[0.1695]
Loss:[0.1721]
Loss:[0.1593]
Loss:[0.1586]
Loss:[0.1536]
Loss:[0.1521]
Loss:[0.1387]
Loss:[0.1490]
Loss:[0.1514]
Loss:[0.1513]
Loss:[0.1478]
Loss:[0.1390]
Loss:[0.1467]
Loss:[0.1357]
Loss:[0.1340]
Loss:[0.1283]
Loss:[0.1303]
Loss:[0.1286]
Loss:[0.1198]
Loss:[0.1241]
Loss:[0.1193]
Loss:[0.1205]
Loss:[0.1207]
Loss:[0.1128]
Loss:[0.1248]
Loss:[0.1088]
Loss:[0.1166]
Loss:[0.1008]
Loss:[0.1128]
Loss:[0.1160]
Loss:[0.1065]
Loss:[0.1081]
Loss:[0.1065]
Loss:[0.0955]
Loss:[0.0985]
Loss:[0.1057]
Loss:[0.1014]
Loss:[0.0970]
Loss:[0.0998]
Loss:[0.0998]
Loss:[0.0912]
Loss:[0.0940]
Loss:[0.0986]
Loss:[0.0922]
Loss:[0.0860]
Loss:[0.0960]
Loss:[0.0864]
Loss:[0.1003]
Loss:[0.0938]
Loss:[0.0858]
Loss:[0.0898]
Loss:[0.0781]
Loss:[0.0849]
Loss:[0.0861]
Loss:[0.0796]
Loss:[0.0854]
Loss:[0.0769]
Loss:[0.0879]
Loss:[0.0768]
Loss:[0.0733]
Loss:[0.0781]
Loss:[0.0712]
Loss:[0.0816]
Loss:[0.0736]
Loss:[0.0818]
Loss:[0.0823]
Loss:[0.0765]
Loss:[0.0740]
Loss:[0.0731]
Loss:[0.0810]
Loss:[0.0710]
Loss:[0.0652]
Loss:[0.0671]
Loss:[0.0703]
Loss:[0.0702]
Loss:[0.0665]
Loss:[0.0748]
Loss:[0.0687]
Loss:[0.0547]
Loss:[0.0668]
Loss:[0.0635]
Loss:[0.0643]
Loss:[0.0703]
Loss:[0.0687]
Loss:[0.0730]
Loss:[0.0626]
Loss:[0.0647]
Loss:[0.0719]
Loss:[0.0571]
Loss:[0.0620]
Loss:[0.0532]
Loss:[0.0631]
Loss:[0.0637]
Loss:[0.0596]
Loss:[0.0581]
Loss:[0.0568]
Loss:[0.0541]
Loss:[0.0565]
Loss:[0.0571]
Loss:[0.0528]
Loss:[0.0546]
Loss:[0.0533]
Loss:[0.0502]
Loss:[0.0536]
Loss:[0.0522]
Loss:[0.0549]
Loss:[0.0561]
Loss:[0.0548]
Loss:[0.0473]
Loss:[0.0489]
Loss:[0.0530]
Loss:[0.0523]
Loss:[0.0511]
Loss:[0.0487]
Loss:[0.0493]
Loss:[0.0549]
Loss:[0.0472]
Loss:[0.0519]
Loss:[0.0467]
Loss:[0.0445]
Loss:[0.0422]
Loss:[0.0463]
Loss:[0.0468]
Loss:[0.0441]
Loss:[0.0490]
Loss:[0.0501]
Loss:[0.0442]
Loss:[0.0486]
Loss:[0.0427]
Loss:[0.0434]
Loss:[0.0530]
Loss:[0.0517]
Loss:[0.0422]
Loss:[0.0450]
Loss:[0.0457]
Loss:[0.0412]
Loss:[0.0379]
Loss:[0.0439]
Loss:[0.0374]
Loss:[0.0402]
Loss:[0.0381]
Loss:[0.0463]
Loss:[0.0421]
Loss:[0.0408]
Loss:[0.0389]
Loss:[0.0352]
Loss:[0.0397]
Loss:[0.0393]
Loss:[0.0394]
Loss:[0.0351]
Loss:[0.0378]
Loss:[0.0361]
Loss:[0.0371]
Loss:[0.0365]
Loss:[0.0339]
Loss:[0.0334]
Loss:[0.0369]
Loss:[0.0365]
Loss:[0.0373]
Loss:[0.0436]
Loss:[0.0361]
Loss:[0.0368]
Loss:[0.0349]
Loss:[0.0336]
Loss:[0.0336]
Loss:[0.0329]
Loss:[0.0296]
Loss:[0.0301]
Loss:[0.0333]
Loss:[0.0358]
Loss:[0.0337]
Loss:[0.0330]
Loss:[0.0405]
Loss:[0.0354]
Loss:[0.0338]
Loss:[0.0371]
Loss:[0.0309]
Loss:[0.0316]
Loss:[0.0343]
Loss:[0.0331]
Loss:[0.0345]
Loss:[0.0267]
Loss:[0.0364]
Loss:[0.0300]
Loss:[0.0296]
Loss:[0.0321]
Loss:[0.0294]
Loss:[0.0289]
Loss:[0.0303]
Loss:[0.0304]
Loss:[0.0358]
Loss:[0.0351]
Loss:[0.0292]
Loss:[0.0315]
Loss:[0.0371]
Loss:[0.0331]
Loss:[0.0288]
Loss:[0.0331]
Loss:[0.0273]
Loss:[0.0261]
Loss:[0.0266]
Loss:[0.0327]
Loss:[0.0276]
Loss:[0.0286]
Loss:[0.0315]
Loss:[0.0285]
Loss:[0.0249]
Loss:[0.0240]
Loss:[0.0291]
Loss:[0.0241]
Loss:[0.0264]
Loss:[0.0308]
Loss:[0.0251]
Loss:[0.0232]
Loss:[0.0310]
Loss:[0.0262]
Loss:[0.0267]
Loss:[0.0266]
Loss:[0.0285]
Loss:[0.0237]
Loss:[0.0231]
Loss:[0.0251]
Loss:[0.0243]
Loss:[0.0240]
Loss:[0.0276]
Loss:[0.0210]
Loss:[0.0258]
Loss:[0.0251]
Loss:[0.0268]
Loss:[0.0249]
Loss:[0.0216]
Loss:[0.0240]
Loss:[0.0223]
Loss:[0.0213]
Loss:[0.0207]
Loss:[0.0198]
Loss:[0.0227]
Loss:[0.0238]
Loss:[0.0278]
Loss:[0.0201]
Loss:[0.0206]
Loss:[0.0235]
Loss:[0.0221]
Loss:[0.0235]
Loss:[0.0248]
Loss:[0.0194]
Loss:[0.0209]
Loss:[0.0220]
Loss:[0.0209]
Loss:[0.0217]
Loss:[0.0167]
Loss:[0.0207]
Loss:[0.0259]
Loss:[0.0235]
Loss:[0.0220]
Loss:[0.0219]
Loss:[0.0249]
Loss:[0.0252]
Loss:[0.0201]
Loss:[0.0269]
Loss:[0.0168]
Loss:[0.0232]
Loss:[0.0212]
Loss:[0.0187]
Loss:[0.0209]
Loss:[0.0203]
Loss:[0.0210]
Loss:[0.0208]
Loss:[0.0229]
Loss:[0.0194]
Loss:[0.0200]
Early stopping!
Loading 314th epoch
acc:[0.8020]
acc:[0.8050]
acc:[0.8030]
acc:[0.7980]
acc:[0.8030]
acc:[0.8010]
acc:[0.8030]
acc:[0.8010]
acc:[0.8030]
acc:[0.8030]
acc:[0.8010]
acc:[0.8040]
acc:[0.8030]
acc:[0.8010]
acc:[0.8020]
acc:[0.8010]
acc:[0.8040]
acc:[0.8040]
acc:[0.8010]
acc:[0.8010]
acc:[0.8010]
acc:[0.7970]
acc:[0.8040]
acc:[0.8030]
acc:[0.8030]
acc:[0.8000]
acc:[0.8010]
acc:[0.8020]
acc:[0.8020]
acc:[0.8020]
acc:[0.8030]
acc:[0.8010]
acc:[0.8030]
acc:[0.8010]
acc:[0.8020]
acc:[0.8010]
acc:[0.8040]
acc:[0.8030]
acc:[0.8030]
acc:[0.8010]
acc:[0.8020]
acc:[0.8010]
acc:[0.8030]
acc:[0.8020]
acc:[0.8040]
acc:[0.8010]
acc:[0.8020]
acc:[0.8030]
acc:[0.8040]
acc:[0.8030]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8021]
Mean:[80.2120]
Std :[0.1507]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='cora', drop_percent=0.1, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6931]
Loss:[0.6900]
Loss:[0.6846]
Loss:[0.6769]
Loss:[0.6687]
Loss:[0.6611]
Loss:[0.6562]
Loss:[0.6475]
Loss:[0.6394]
Loss:[0.6341]
Loss:[0.6233]
Loss:[0.6162]
Loss:[0.6078]
Loss:[0.5989]
Loss:[0.5907]
Loss:[0.5805]
Loss:[0.5691]
Loss:[0.5612]
Loss:[0.5477]
Loss:[0.5343]
Loss:[0.5225]
Loss:[0.5111]
Loss:[0.4990]
Loss:[0.4866]
Loss:[0.4813]
Loss:[0.4674]
Loss:[0.4497]
Loss:[0.4369]
Loss:[0.4278]
Loss:[0.4104]
Loss:[0.3988]
Loss:[0.3910]
Loss:[0.3814]
Loss:[0.3660]
Loss:[0.3604]
Loss:[0.3480]
Loss:[0.3391]
Loss:[0.3173]
Loss:[0.3093]
Loss:[0.3051]
Loss:[0.2999]
Loss:[0.2862]
Loss:[0.2764]
Loss:[0.2716]
Loss:[0.2622]
Loss:[0.2518]
Loss:[0.2454]
Loss:[0.2432]
Loss:[0.2403]
Loss:[0.2287]
Loss:[0.2172]
Loss:[0.2143]
Loss:[0.2106]
Loss:[0.2067]
Loss:[0.1970]
Loss:[0.1900]
Loss:[0.1996]
Loss:[0.1826]
Loss:[0.1885]
Loss:[0.1789]
Loss:[0.1696]
Loss:[0.1715]
Loss:[0.1651]
Loss:[0.1610]
Loss:[0.1655]
Loss:[0.1743]
Loss:[0.1590]
Loss:[0.1484]
Loss:[0.1468]
Loss:[0.1523]
Loss:[0.1422]
Loss:[0.1354]
Loss:[0.1468]
Loss:[0.1524]
Loss:[0.1386]
Loss:[0.1386]
Loss:[0.1404]
Loss:[0.1315]
Loss:[0.1422]
Loss:[0.1241]
Loss:[0.1308]
Loss:[0.1272]
Loss:[0.1137]
Loss:[0.1211]
Loss:[0.1121]
Loss:[0.1203]
Loss:[0.1163]
Loss:[0.1068]
Loss:[0.1124]
Loss:[0.1026]
Loss:[0.1157]
Loss:[0.0945]
Loss:[0.1075]
Loss:[0.1074]
Loss:[0.1012]
Loss:[0.1026]
Loss:[0.1002]
Loss:[0.0918]
Loss:[0.0941]
Loss:[0.0973]
Loss:[0.0973]
Loss:[0.0944]
Loss:[0.0931]
Loss:[0.0921]
Loss:[0.0893]
Loss:[0.0900]
Loss:[0.0866]
Loss:[0.0894]
Loss:[0.0831]
Loss:[0.0846]
Loss:[0.0808]
Loss:[0.0917]
Loss:[0.0842]
Loss:[0.0797]
Loss:[0.0796]
Loss:[0.0727]
Loss:[0.0834]
Loss:[0.0760]
Loss:[0.0751]
Loss:[0.0738]
Loss:[0.0682]
Loss:[0.0772]
Loss:[0.0753]
Loss:[0.0677]
Loss:[0.0726]
Loss:[0.0657]
Loss:[0.0717]
Loss:[0.0702]
Loss:[0.0754]
Loss:[0.0780]
Loss:[0.0720]
Loss:[0.0642]
Loss:[0.0700]
Loss:[0.0768]
Loss:[0.0657]
Loss:[0.0621]
Loss:[0.0614]
Loss:[0.0642]
Loss:[0.0653]
Loss:[0.0626]
Loss:[0.0685]
Loss:[0.0622]
Loss:[0.0527]
Loss:[0.0647]
Loss:[0.0572]
Loss:[0.0638]
Loss:[0.0621]
Loss:[0.0615]
Loss:[0.0583]
Loss:[0.0512]
Loss:[0.0589]
Loss:[0.0587]
Loss:[0.0578]
Loss:[0.0520]
Loss:[0.0539]
Loss:[0.0534]
Loss:[0.0589]
Loss:[0.0563]
Loss:[0.0533]
Loss:[0.0524]
Loss:[0.0533]
Loss:[0.0547]
Loss:[0.0512]
Loss:[0.0513]
Loss:[0.0505]
Loss:[0.0533]
Loss:[0.0461]
Loss:[0.0507]
Loss:[0.0480]
Loss:[0.0508]
Loss:[0.0500]
Loss:[0.0507]
Loss:[0.0454]
Loss:[0.0458]
Loss:[0.0500]
Loss:[0.0519]
Loss:[0.0455]
Loss:[0.0457]
Loss:[0.0460]
Loss:[0.0498]
Loss:[0.0453]
Loss:[0.0434]
Loss:[0.0441]
Loss:[0.0429]
Loss:[0.0387]
Loss:[0.0439]
Loss:[0.0432]
Loss:[0.0412]
Loss:[0.0435]
Loss:[0.0430]
Loss:[0.0390]
Loss:[0.0453]
Loss:[0.0375]
Loss:[0.0374]
Loss:[0.0468]
Loss:[0.0484]
Loss:[0.0392]
Loss:[0.0432]
Loss:[0.0401]
Loss:[0.0396]
Loss:[0.0372]
Loss:[0.0400]
Loss:[0.0302]
Loss:[0.0355]
Loss:[0.0356]
Loss:[0.0402]
Loss:[0.0377]
Loss:[0.0394]
Loss:[0.0371]
Loss:[0.0335]
Loss:[0.0328]
Loss:[0.0363]
Loss:[0.0369]
Loss:[0.0325]
Loss:[0.0329]
Loss:[0.0327]
Loss:[0.0326]
Loss:[0.0328]
Loss:[0.0295]
Loss:[0.0304]
Loss:[0.0348]
Loss:[0.0335]
Loss:[0.0335]
Loss:[0.0405]
Loss:[0.0301]
Loss:[0.0342]
Loss:[0.0304]
Loss:[0.0313]
Loss:[0.0288]
Loss:[0.0298]
Loss:[0.0273]
Loss:[0.0282]
Loss:[0.0299]
Loss:[0.0319]
Loss:[0.0287]
Loss:[0.0323]
Loss:[0.0375]
Loss:[0.0308]
Loss:[0.0313]
Loss:[0.0325]
Loss:[0.0294]
Loss:[0.0319]
Loss:[0.0306]
Loss:[0.0292]
Loss:[0.0329]
Loss:[0.0246]
Loss:[0.0313]
Loss:[0.0268]
Loss:[0.0251]
Loss:[0.0295]
Loss:[0.0253]
Loss:[0.0283]
Loss:[0.0249]
Loss:[0.0260]
Loss:[0.0315]
Loss:[0.0312]
Loss:[0.0267]
Loss:[0.0270]
Loss:[0.0337]
Loss:[0.0275]
Loss:[0.0257]
Loss:[0.0286]
Loss:[0.0249]
Loss:[0.0232]
Loss:[0.0228]
Loss:[0.0284]
Loss:[0.0228]
Loss:[0.0241]
Loss:[0.0276]
Loss:[0.0248]
Loss:[0.0217]
Loss:[0.0222]
Loss:[0.0258]
Loss:[0.0207]
Loss:[0.0241]
Loss:[0.0274]
Loss:[0.0211]
Loss:[0.0198]
Loss:[0.0270]
Loss:[0.0228]
Loss:[0.0227]
Loss:[0.0230]
Loss:[0.0255]
Loss:[0.0189]
Loss:[0.0210]
Loss:[0.0222]
Loss:[0.0208]
Loss:[0.0211]
Loss:[0.0230]
Loss:[0.0190]
Loss:[0.0219]
Loss:[0.0211]
Loss:[0.0220]
Loss:[0.0234]
Loss:[0.0200]
Loss:[0.0212]
Loss:[0.0208]
Loss:[0.0200]
Loss:[0.0187]
Loss:[0.0176]
Loss:[0.0215]
Loss:[0.0200]
Loss:[0.0226]
Loss:[0.0177]
Loss:[0.0206]
Loss:[0.0208]
Loss:[0.0196]
Loss:[0.0207]
Loss:[0.0214]
Loss:[0.0163]
Loss:[0.0190]
Loss:[0.0154]
Loss:[0.0186]
Loss:[0.0176]
Loss:[0.0157]
Loss:[0.0183]
Loss:[0.0208]
Loss:[0.0190]
Loss:[0.0182]
Loss:[0.0190]
Loss:[0.0211]
Loss:[0.0203]
Loss:[0.0171]
Loss:[0.0211]
Loss:[0.0162]
Loss:[0.0201]
Loss:[0.0208]
Loss:[0.0171]
Loss:[0.0178]
Loss:[0.0191]
Loss:[0.0184]
Loss:[0.0198]
Early stopping!
Loading 311th epoch
acc:[0.7990]
acc:[0.8010]
acc:[0.8030]
acc:[0.8050]
acc:[0.8010]
acc:[0.8020]
acc:[0.8030]
acc:[0.8010]
acc:[0.8000]
acc:[0.8000]
acc:[0.8020]
acc:[0.8000]
acc:[0.8000]
acc:[0.8020]
acc:[0.8000]
acc:[0.8010]
acc:[0.8000]
acc:[0.8000]
acc:[0.8020]
acc:[0.7980]
acc:[0.8060]
acc:[0.7980]
acc:[0.8000]
acc:[0.8040]
acc:[0.8030]
acc:[0.8030]
acc:[0.8010]
acc:[0.8020]
acc:[0.7990]
acc:[0.8000]
acc:[0.8020]
acc:[0.8020]
acc:[0.8020]
acc:[0.8030]
acc:[0.8050]
acc:[0.8030]
acc:[0.8000]
acc:[0.8000]
acc:[0.8030]
acc:[0.8030]
acc:[0.8030]
acc:[0.8010]
acc:[0.8030]
acc:[0.8010]
acc:[0.8030]
acc:[0.8000]
acc:[0.8010]
acc:[0.8010]
acc:[0.7990]
acc:[0.8020]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8015]
Mean:[80.1460]
Std :[0.1752]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='cora', drop_percent=0.2, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6932]
Loss:[0.6890]
Loss:[0.6810]
Loss:[0.6698]
Loss:[0.6555]
Loss:[0.6377]
Loss:[0.6230]
Loss:[0.6105]
Loss:[0.6014]
Loss:[0.5934]
Loss:[0.5879]
Loss:[0.5836]
Loss:[0.5746]
Loss:[0.5672]
Loss:[0.5557]
Loss:[0.5502]
Loss:[0.5360]
Loss:[0.5330]
Loss:[0.5236]
Loss:[0.5155]
Loss:[0.5061]
Loss:[0.4958]
Loss:[0.4882]
Loss:[0.4775]
Loss:[0.4742]
Loss:[0.4610]
Loss:[0.4510]
Loss:[0.4418]
Loss:[0.4328]
Loss:[0.4285]
Loss:[0.4132]
Loss:[0.4069]
Loss:[0.3954]
Loss:[0.3870]
Loss:[0.3767]
Loss:[0.3625]
Loss:[0.3571]
Loss:[0.3480]
Loss:[0.3367]
Loss:[0.3166]
Loss:[0.3241]
Loss:[0.3002]
Loss:[0.3016]
Loss:[0.2891]
Loss:[0.2862]
Loss:[0.2712]
Loss:[0.2655]
Loss:[0.2601]
Loss:[0.2486]
Loss:[0.2434]
Loss:[0.2366]
Loss:[0.2265]
Loss:[0.2242]
Loss:[0.2183]
Loss:[0.2038]
Loss:[0.1954]
Loss:[0.1919]
Loss:[0.1922]
Loss:[0.1784]
Loss:[0.1805]
Loss:[0.1710]
Loss:[0.1785]
Loss:[0.1678]
Loss:[0.1616]
Loss:[0.1554]
Loss:[0.1575]
Loss:[0.1478]
Loss:[0.1408]
Loss:[0.1379]
Loss:[0.1367]
Loss:[0.1267]
Loss:[0.1275]
Loss:[0.1306]
Loss:[0.1249]
Loss:[0.1259]
Loss:[0.1182]
Loss:[0.1120]
Loss:[0.1143]
Loss:[0.1162]
Loss:[0.1048]
Loss:[0.1042]
Loss:[0.1040]
Loss:[0.0954]
Loss:[0.0947]
Loss:[0.0957]
Loss:[0.0907]
Loss:[0.0899]
Loss:[0.0885]
Loss:[0.0884]
Loss:[0.0831]
Loss:[0.0873]
Loss:[0.0808]
Loss:[0.0849]
Loss:[0.0829]
Loss:[0.0771]
Loss:[0.0796]
Loss:[0.0718]
Loss:[0.0775]
Loss:[0.0678]
Loss:[0.0661]
Loss:[0.0688]
Loss:[0.0673]
Loss:[0.0679]
Loss:[0.0635]
Loss:[0.0657]
Loss:[0.0627]
Loss:[0.0657]
Loss:[0.0607]
Loss:[0.0557]
Loss:[0.0624]
Loss:[0.0556]
Loss:[0.0594]
Loss:[0.0555]
Loss:[0.0586]
Loss:[0.0577]
Loss:[0.0549]
Loss:[0.0540]
Loss:[0.0497]
Loss:[0.0519]
Loss:[0.0555]
Loss:[0.0486]
Loss:[0.0510]
Loss:[0.0477]
Loss:[0.0460]
Loss:[0.0441]
Loss:[0.0449]
Loss:[0.0492]
Loss:[0.0475]
Loss:[0.0475]
Loss:[0.0470]
Loss:[0.0437]
Loss:[0.0448]
Loss:[0.0437]
Loss:[0.0449]
Loss:[0.0421]
Loss:[0.0401]
Loss:[0.0388]
Loss:[0.0423]
Loss:[0.0395]
Loss:[0.0385]
Loss:[0.0434]
Loss:[0.0402]
Loss:[0.0361]
Loss:[0.0398]
Loss:[0.0369]
Loss:[0.0364]
Loss:[0.0385]
Loss:[0.0416]
Loss:[0.0398]
Loss:[0.0356]
Loss:[0.0384]
Loss:[0.0337]
Loss:[0.0386]
Loss:[0.0324]
Loss:[0.0355]
Loss:[0.0330]
Loss:[0.0341]
Loss:[0.0351]
Loss:[0.0318]
Loss:[0.0309]
Loss:[0.0299]
Loss:[0.0297]
Loss:[0.0280]
Loss:[0.0345]
Loss:[0.0325]
Loss:[0.0297]
Loss:[0.0286]
Loss:[0.0265]
Loss:[0.0296]
Loss:[0.0261]
Loss:[0.0282]
Loss:[0.0273]
Loss:[0.0240]
Loss:[0.0263]
Loss:[0.0285]
Loss:[0.0278]
Loss:[0.0258]
Loss:[0.0279]
Loss:[0.0267]
Loss:[0.0263]
Loss:[0.0269]
Loss:[0.0248]
Loss:[0.0216]
Loss:[0.0238]
Loss:[0.0235]
Loss:[0.0228]
Loss:[0.0240]
Loss:[0.0228]
Loss:[0.0217]
Loss:[0.0218]
Loss:[0.0234]
Loss:[0.0259]
Loss:[0.0237]
Loss:[0.0250]
Loss:[0.0247]
Loss:[0.0224]
Loss:[0.0215]
Loss:[0.0251]
Loss:[0.0207]
Loss:[0.0246]
Loss:[0.0202]
Loss:[0.0222]
Loss:[0.0189]
Loss:[0.0222]
Loss:[0.0204]
Loss:[0.0226]
Loss:[0.0182]
Loss:[0.0202]
Loss:[0.0213]
Loss:[0.0188]
Loss:[0.0178]
Loss:[0.0217]
Loss:[0.0188]
Loss:[0.0201]
Loss:[0.0190]
Loss:[0.0178]
Loss:[0.0191]
Loss:[0.0169]
Loss:[0.0171]
Loss:[0.0166]
Loss:[0.0198]
Loss:[0.0160]
Loss:[0.0164]
Loss:[0.0188]
Loss:[0.0181]
Loss:[0.0164]
Loss:[0.0161]
Loss:[0.0175]
Loss:[0.0168]
Loss:[0.0175]
Loss:[0.0160]
Loss:[0.0163]
Loss:[0.0187]
Loss:[0.0168]
Loss:[0.0175]
Loss:[0.0158]
Loss:[0.0203]
Loss:[0.0163]
Loss:[0.0177]
Loss:[0.0152]
Loss:[0.0164]
Loss:[0.0159]
Loss:[0.0160]
Loss:[0.0165]
Loss:[0.0153]
Loss:[0.0164]
Loss:[0.0160]
Loss:[0.0144]
Loss:[0.0145]
Loss:[0.0155]
Loss:[0.0133]
Loss:[0.0171]
Loss:[0.0141]
Loss:[0.0135]
Loss:[0.0171]
Loss:[0.0127]
Loss:[0.0130]
Loss:[0.0145]
Loss:[0.0147]
Loss:[0.0144]
Loss:[0.0135]
Loss:[0.0127]
Loss:[0.0112]
Loss:[0.0105]
Loss:[0.0136]
Loss:[0.0128]
Loss:[0.0121]
Loss:[0.0117]
Loss:[0.0141]
Loss:[0.0110]
Loss:[0.0107]
Loss:[0.0120]
Loss:[0.0129]
Loss:[0.0111]
Loss:[0.0109]
Loss:[0.0116]
Loss:[0.0097]
Loss:[0.0119]
Loss:[0.0110]
Loss:[0.0108]
Loss:[0.0111]
Loss:[0.0118]
Loss:[0.0122]
Loss:[0.0117]
Loss:[0.0111]
Loss:[0.0105]
Loss:[0.0126]
Loss:[0.0105]
Loss:[0.0111]
Loss:[0.0112]
Loss:[0.0110]
Loss:[0.0118]
Loss:[0.0111]
Loss:[0.0117]
Loss:[0.0096]
Loss:[0.0110]
Loss:[0.0098]
Loss:[0.0103]
Loss:[0.0112]
Loss:[0.0112]
Loss:[0.0095]
Loss:[0.0109]
Loss:[0.0099]
Loss:[0.0099]
Loss:[0.0114]
Loss:[0.0107]
Loss:[0.0093]
Loss:[0.0093]
Loss:[0.0101]
Loss:[0.0085]
Loss:[0.0108]
Loss:[0.0086]
Loss:[0.0081]
Loss:[0.0102]
Loss:[0.0087]
Loss:[0.0095]
Loss:[0.0096]
Loss:[0.0101]
Loss:[0.0100]
Loss:[0.0111]
Loss:[0.0118]
Loss:[0.0092]
Loss:[0.0101]
Loss:[0.0103]
Loss:[0.0085]
Loss:[0.0092]
Loss:[0.0081]
Loss:[0.0088]
Loss:[0.0076]
Loss:[0.0082]
Loss:[0.0096]
Loss:[0.0089]
Loss:[0.0100]
Loss:[0.0103]
Loss:[0.0086]
Loss:[0.0100]
Loss:[0.0102]
Loss:[0.0088]
Loss:[0.0108]
Loss:[0.0073]
Loss:[0.0084]
Loss:[0.0081]
Loss:[0.0087]
Loss:[0.0084]
Loss:[0.0085]
Loss:[0.0083]
Loss:[0.0085]
Loss:[0.0079]
Loss:[0.0084]
Loss:[0.0084]
Loss:[0.0087]
Loss:[0.0087]
Loss:[0.0069]
Loss:[0.0087]
Loss:[0.0081]
Loss:[0.0085]
Loss:[0.0087]
Loss:[0.0079]
Loss:[0.0079]
Loss:[0.0082]
Loss:[0.0078]
Loss:[0.0080]
Loss:[0.0087]
Loss:[0.0087]
Loss:[0.0083]
Loss:[0.0070]
Loss:[0.0071]
Loss:[0.0098]
Loss:[0.0074]
Loss:[0.0079]
Loss:[0.0072]
Loss:[0.0084]
Loss:[0.0075]
Early stopping!
Loading 352th epoch
acc:[0.8000]
acc:[0.8010]
acc:[0.7980]
acc:[0.7970]
acc:[0.7980]
acc:[0.7970]
acc:[0.7980]
acc:[0.8030]
acc:[0.8000]
acc:[0.8000]
acc:[0.7980]
acc:[0.8010]
acc:[0.7980]
acc:[0.7970]
acc:[0.7940]
acc:[0.7960]
acc:[0.7990]
acc:[0.7980]
acc:[0.7990]
acc:[0.7980]
acc:[0.7940]
acc:[0.7980]
acc:[0.7960]
acc:[0.7980]
acc:[0.8030]
acc:[0.7960]
acc:[0.8000]
acc:[0.7980]
acc:[0.7950]
acc:[0.7980]
acc:[0.8010]
acc:[0.7960]
acc:[0.7960]
acc:[0.8000]
acc:[0.7950]
acc:[0.7990]
acc:[0.8020]
acc:[0.7960]
acc:[0.7990]
acc:[0.7970]
acc:[0.7980]
acc:[0.7990]
acc:[0.7970]
acc:[0.7970]
acc:[0.7970]
acc:[0.7990]
acc:[0.8040]
acc:[0.7980]
acc:[0.8020]
acc:[0.7990]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7983]
Mean:[79.8340]
Std :[0.2246]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='cora', drop_percent=0.3, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6932]
Loss:[0.6862]
Loss:[0.6752]
Loss:[0.6589]
Loss:[0.6371]
Loss:[0.6128]
Loss:[0.5872]
Loss:[0.5638]
Loss:[0.5459]
Loss:[0.5332]
Loss:[0.5225]
Loss:[0.5225]
Loss:[0.5209]
Loss:[0.5122]
Loss:[0.5030]
Loss:[0.4969]
Loss:[0.4857]
Loss:[0.4760]
Loss:[0.4708]
Loss:[0.4627]
Loss:[0.4534]
Loss:[0.4453]
Loss:[0.4465]
Loss:[0.4331]
Loss:[0.4289]
Loss:[0.4272]
Loss:[0.4182]
Loss:[0.4102]
Loss:[0.4064]
Loss:[0.3941]
Loss:[0.3884]
Loss:[0.3801]
Loss:[0.3668]
Loss:[0.3625]
Loss:[0.3512]
Loss:[0.3465]
Loss:[0.3376]
Loss:[0.3306]
Loss:[0.3248]
Loss:[0.3181]
Loss:[0.3122]
Loss:[0.3001]
Loss:[0.3094]
Loss:[0.2911]
Loss:[0.2771]
Loss:[0.2740]
Loss:[0.2611]
Loss:[0.2632]
Loss:[0.2597]
Loss:[0.2410]
Loss:[0.2375]
Loss:[0.2357]
Loss:[0.2210]
Loss:[0.2194]
Loss:[0.2128]
Loss:[0.1940]
Loss:[0.1932]
Loss:[0.1833]
Loss:[0.1827]
Loss:[0.1855]
Loss:[0.1685]
Loss:[0.1693]
Loss:[0.1683]
Loss:[0.1710]
Loss:[0.1567]
Loss:[0.1533]
Loss:[0.1463]
Loss:[0.1431]
Loss:[0.1360]
Loss:[0.1342]
Loss:[0.1270]
Loss:[0.1242]
Loss:[0.1222]
Loss:[0.1166]
Loss:[0.1182]
Loss:[0.1136]
Loss:[0.1063]
Loss:[0.1107]
Loss:[0.1083]
Loss:[0.1110]
Loss:[0.0966]
Loss:[0.0952]
Loss:[0.0949]
Loss:[0.0870]
Loss:[0.0947]
Loss:[0.0831]
Loss:[0.0800]
Loss:[0.0840]
Loss:[0.0790]
Loss:[0.0756]
Loss:[0.0783]
Loss:[0.0730]
Loss:[0.0748]
Loss:[0.0738]
Loss:[0.0698]
Loss:[0.0768]
Loss:[0.0657]
Loss:[0.0674]
Loss:[0.0661]
Loss:[0.0604]
Loss:[0.0645]
Loss:[0.0596]
Loss:[0.0669]
Loss:[0.0607]
Loss:[0.0594]
Loss:[0.0567]
Loss:[0.0580]
Loss:[0.0540]
Loss:[0.0512]
Loss:[0.0553]
Loss:[0.0551]
Loss:[0.0519]
Loss:[0.0539]
Loss:[0.0436]
Loss:[0.0457]
Loss:[0.0480]
Loss:[0.0462]
Loss:[0.0432]
Loss:[0.0436]
Loss:[0.0388]
Loss:[0.0408]
Loss:[0.0430]
Loss:[0.0473]
Loss:[0.0418]
Loss:[0.0348]
Loss:[0.0365]
Loss:[0.0385]
Loss:[0.0385]
Loss:[0.0377]
Loss:[0.0394]
Loss:[0.0374]
Loss:[0.0374]
Loss:[0.0377]
Loss:[0.0371]
Loss:[0.0404]
Loss:[0.0344]
Loss:[0.0332]
Loss:[0.0355]
Loss:[0.0353]
Loss:[0.0323]
Loss:[0.0375]
Loss:[0.0326]
Loss:[0.0325]
Loss:[0.0325]
Loss:[0.0336]
Loss:[0.0308]
Loss:[0.0328]
Loss:[0.0342]
Loss:[0.0292]
Loss:[0.0282]
Loss:[0.0300]
Loss:[0.0291]
Loss:[0.0286]
Loss:[0.0273]
Loss:[0.0272]
Loss:[0.0298]
Loss:[0.0261]
Loss:[0.0273]
Loss:[0.0278]
Loss:[0.0271]
Loss:[0.0245]
Loss:[0.0210]
Loss:[0.0247]
Loss:[0.0300]
Loss:[0.0290]
Loss:[0.0275]
Loss:[0.0248]
Loss:[0.0238]
Loss:[0.0224]
Loss:[0.0227]
Loss:[0.0244]
Loss:[0.0214]
Loss:[0.0196]
Loss:[0.0223]
Loss:[0.0232]
Loss:[0.0217]
Loss:[0.0211]
Loss:[0.0204]
Loss:[0.0218]
Loss:[0.0195]
Loss:[0.0222]
Loss:[0.0219]
Loss:[0.0195]
Loss:[0.0204]
Loss:[0.0178]
Loss:[0.0192]
Loss:[0.0231]
Loss:[0.0215]
Loss:[0.0188]
Loss:[0.0206]
Loss:[0.0169]
Loss:[0.0180]
Loss:[0.0181]
Loss:[0.0181]
Loss:[0.0168]
Loss:[0.0189]
Loss:[0.0195]
Loss:[0.0191]
Loss:[0.0173]
Loss:[0.0194]
Loss:[0.0163]
Loss:[0.0166]
Loss:[0.0162]
Loss:[0.0173]
Loss:[0.0172]
Loss:[0.0176]
Loss:[0.0155]
Loss:[0.0120]
Loss:[0.0163]
Loss:[0.0172]
Loss:[0.0168]
Loss:[0.0163]
Loss:[0.0148]
Loss:[0.0190]
Loss:[0.0168]
Loss:[0.0164]
Loss:[0.0163]
Loss:[0.0145]
Loss:[0.0125]
Loss:[0.0126]
Loss:[0.0165]
Loss:[0.0142]
Loss:[0.0135]
Loss:[0.0150]
Loss:[0.0150]
Loss:[0.0146]
Loss:[0.0149]
Loss:[0.0151]
Early stopping!
Loading 207th epoch
acc:[0.7620]
acc:[0.7620]
acc:[0.7620]
acc:[0.7590]
acc:[0.7640]
acc:[0.7640]
acc:[0.7590]
acc:[0.7590]
acc:[0.7630]
acc:[0.7670]
acc:[0.7620]
acc:[0.7650]
acc:[0.7660]
acc:[0.7620]
acc:[0.7590]
acc:[0.7600]
acc:[0.7650]
acc:[0.7610]
acc:[0.7610]
acc:[0.7590]
acc:[0.7620]
acc:[0.7640]
acc:[0.7650]
acc:[0.7650]
acc:[0.7640]
acc:[0.7650]
acc:[0.7640]
acc:[0.7610]
acc:[0.7650]
acc:[0.7650]
acc:[0.7630]
acc:[0.7670]
acc:[0.7620]
acc:[0.7640]
acc:[0.7610]
acc:[0.7640]
acc:[0.7640]
acc:[0.7640]
acc:[0.7600]
acc:[0.7660]
acc:[0.7620]
acc:[0.7640]
acc:[0.7660]
acc:[0.7630]
acc:[0.7640]
acc:[0.7640]
acc:[0.7620]
acc:[0.7630]
acc:[0.7650]
acc:[0.7640]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7631]
Mean:[76.3060]
Std :[0.2152]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='cora', drop_percent=0.4, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6932]
Loss:[0.6827]
Loss:[0.6667]
Loss:[0.6446]
Loss:[0.6156]
Loss:[0.5807]
Loss:[0.5467]
Loss:[0.5132]
Loss:[0.4835]
Loss:[0.4624]
Loss:[0.4458]
Loss:[0.4415]
Loss:[0.4325]
Loss:[0.4357]
Loss:[0.4288]
Loss:[0.4209]
Loss:[0.4128]
Loss:[0.4065]
Loss:[0.3993]
Loss:[0.3951]
Loss:[0.3821]
Loss:[0.3773]
Loss:[0.3689]
Loss:[0.3661]
Loss:[0.3603]
Loss:[0.3565]
Loss:[0.3521]
Loss:[0.3411]
Loss:[0.3418]
Loss:[0.3342]
Loss:[0.3330]
Loss:[0.3297]
Loss:[0.3222]
Loss:[0.3193]
Loss:[0.3115]
Loss:[0.3004]
Loss:[0.3011]
Loss:[0.2911]
Loss:[0.2910]
Loss:[0.2875]
Loss:[0.2752]
Loss:[0.2695]
Loss:[0.2650]
Loss:[0.2676]
Loss:[0.2536]
Loss:[0.2506]
Loss:[0.2523]
Loss:[0.2521]
Loss:[0.2332]
Loss:[0.2282]
Loss:[0.2289]
Loss:[0.2233]
Loss:[0.2147]
Loss:[0.2151]
Loss:[0.2025]
Loss:[0.2003]
Loss:[0.1946]
Loss:[0.1929]
Loss:[0.1951]
Loss:[0.1787]
Loss:[0.1696]
Loss:[0.1737]
Loss:[0.1652]
Loss:[0.1643]
Loss:[0.1535]
Loss:[0.1565]
Loss:[0.1611]
Loss:[0.1460]
Loss:[0.1404]
Loss:[0.1439]
Loss:[0.1421]
Loss:[0.1237]
Loss:[0.1272]
Loss:[0.1275]
Loss:[0.1191]
Loss:[0.1172]
Loss:[0.1109]
Loss:[0.1213]
Loss:[0.1188]
Loss:[0.1053]
Loss:[0.1143]
Loss:[0.0971]
Loss:[0.1102]
Loss:[0.0941]
Loss:[0.0943]
Loss:[0.0930]
Loss:[0.0821]
Loss:[0.0848]
Loss:[0.0825]
Loss:[0.0821]
Loss:[0.0777]
Loss:[0.0811]
Loss:[0.0782]
Loss:[0.0705]
Loss:[0.0775]
Loss:[0.0705]
Loss:[0.0683]
Loss:[0.0700]
Loss:[0.0675]
Loss:[0.0659]
Loss:[0.0663]
Loss:[0.0608]
Loss:[0.0615]
Loss:[0.0612]
Loss:[0.0631]
Loss:[0.0552]
Loss:[0.0584]
Loss:[0.0520]
Loss:[0.0550]
Loss:[0.0526]
Loss:[0.0500]
Loss:[0.0490]
Loss:[0.0504]
Loss:[0.0455]
Loss:[0.0482]
Loss:[0.0457]
Loss:[0.0447]
Loss:[0.0419]
Loss:[0.0475]
Loss:[0.0432]
Loss:[0.0443]
Loss:[0.0395]
Loss:[0.0430]
Loss:[0.0389]
Loss:[0.0363]
Loss:[0.0361]
Loss:[0.0404]
Loss:[0.0384]
Loss:[0.0417]
Loss:[0.0418]
Loss:[0.0361]
Loss:[0.0391]
Loss:[0.0358]
Loss:[0.0363]
Loss:[0.0336]
Loss:[0.0316]
Loss:[0.0336]
Loss:[0.0366]
Loss:[0.0328]
Loss:[0.0323]
Loss:[0.0323]
Loss:[0.0308]
Loss:[0.0285]
Loss:[0.0338]
Loss:[0.0324]
Loss:[0.0309]
Loss:[0.0350]
Loss:[0.0309]
Loss:[0.0296]
Loss:[0.0276]
Loss:[0.0299]
Loss:[0.0280]
Loss:[0.0312]
Loss:[0.0240]
Loss:[0.0256]
Loss:[0.0273]
Loss:[0.0282]
Loss:[0.0288]
Loss:[0.0233]
Loss:[0.0240]
Loss:[0.0273]
Loss:[0.0233]
Loss:[0.0240]
Loss:[0.0276]
Loss:[0.0236]
Loss:[0.0260]
Loss:[0.0238]
Loss:[0.0216]
Loss:[0.0227]
Loss:[0.0215]
Loss:[0.0234]
Loss:[0.0217]
Loss:[0.0214]
Loss:[0.0210]
Loss:[0.0215]
Loss:[0.0222]
Loss:[0.0198]
Loss:[0.0198]
Loss:[0.0223]
Loss:[0.0220]
Loss:[0.0216]
Loss:[0.0175]
Loss:[0.0185]
Loss:[0.0208]
Loss:[0.0186]
Loss:[0.0224]
Loss:[0.0203]
Loss:[0.0198]
Loss:[0.0182]
Loss:[0.0207]
Loss:[0.0183]
Loss:[0.0194]
Loss:[0.0229]
Loss:[0.0176]
Loss:[0.0163]
Loss:[0.0166]
Loss:[0.0179]
Loss:[0.0188]
Loss:[0.0193]
Loss:[0.0194]
Loss:[0.0176]
Loss:[0.0169]
Loss:[0.0159]
Loss:[0.0142]
Loss:[0.0193]
Loss:[0.0161]
Loss:[0.0150]
Loss:[0.0130]
Loss:[0.0208]
Loss:[0.0142]
Loss:[0.0168]
Loss:[0.0162]
Loss:[0.0159]
Loss:[0.0131]
Loss:[0.0142]
Loss:[0.0130]
Loss:[0.0147]
Loss:[0.0145]
Loss:[0.0137]
Loss:[0.0110]
Loss:[0.0153]
Loss:[0.0157]
Loss:[0.0140]
Loss:[0.0136]
Loss:[0.0153]
Loss:[0.0138]
Loss:[0.0138]
Loss:[0.0160]
Loss:[0.0113]
Loss:[0.0151]
Loss:[0.0158]
Loss:[0.0120]
Loss:[0.0156]
Loss:[0.0137]
Loss:[0.0122]
Loss:[0.0140]
Loss:[0.0123]
Loss:[0.0142]
Loss:[0.0130]
Loss:[0.0111]
Early stopping!
Loading 219th epoch
acc:[0.7580]
acc:[0.7600]
acc:[0.7570]
acc:[0.7590]
acc:[0.7550]
acc:[0.7600]
acc:[0.7540]
acc:[0.7570]
acc:[0.7580]
acc:[0.7590]
acc:[0.7580]
acc:[0.7570]
acc:[0.7540]
acc:[0.7600]
acc:[0.7570]
acc:[0.7540]
acc:[0.7540]
acc:[0.7560]
acc:[0.7610]
acc:[0.7590]
acc:[0.7560]
acc:[0.7580]
acc:[0.7540]
acc:[0.7600]
acc:[0.7570]
acc:[0.7590]
acc:[0.7590]
acc:[0.7560]
acc:[0.7590]
acc:[0.7580]
acc:[0.7600]
acc:[0.7550]
acc:[0.7550]
acc:[0.7600]
acc:[0.7560]
acc:[0.7550]
acc:[0.7590]
acc:[0.7540]
acc:[0.7550]
acc:[0.7570]
acc:[0.7560]
acc:[0.7560]
acc:[0.7570]
acc:[0.7530]
acc:[0.7560]
acc:[0.7600]
acc:[0.7540]
acc:[0.7570]
acc:[0.7520]
acc:[0.7590]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7570]
Mean:[75.6980]
Std :[0.2245]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='cora', drop_percent=0.5, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6932]
Loss:[0.6790]
Loss:[0.6575]
Loss:[0.6287]
Loss:[0.5923]
Loss:[0.5487]
Loss:[0.5039]
Loss:[0.4631]
Loss:[0.4233]
Loss:[0.3912]
Loss:[0.3667]
Loss:[0.3563]
Loss:[0.3442]
Loss:[0.3369]
Loss:[0.3348]
Loss:[0.3317]
Loss:[0.3240]
Loss:[0.3216]
Loss:[0.3161]
Loss:[0.3062]
Loss:[0.2987]
Loss:[0.2943]
Loss:[0.2997]
Loss:[0.2842]
Loss:[0.2784]
Loss:[0.2671]
Loss:[0.2727]
Loss:[0.2609]
Loss:[0.2646]
Loss:[0.2586]
Loss:[0.2555]
Loss:[0.2590]
Loss:[0.2504]
Loss:[0.2562]
Loss:[0.2399]
Loss:[0.2410]
Loss:[0.2397]
Loss:[0.2338]
Loss:[0.2330]
Loss:[0.2340]
Loss:[0.2318]
Loss:[0.2200]
Loss:[0.2210]
Loss:[0.2128]
Loss:[0.2145]
Loss:[0.2024]
Loss:[0.2118]
Loss:[0.2101]
Loss:[0.1970]
Loss:[0.1963]
Loss:[0.1947]
Loss:[0.1990]
Loss:[0.1951]
Loss:[0.1966]
Loss:[0.1797]
Loss:[0.1818]
Loss:[0.1737]
Loss:[0.1707]
Loss:[0.1767]
Loss:[0.1696]
Loss:[0.1584]
Loss:[0.1682]
Loss:[0.1617]
Loss:[0.1585]
Loss:[0.1577]
Loss:[0.1578]
Loss:[0.1582]
Loss:[0.1360]
Loss:[0.1499]
Loss:[0.1454]
Loss:[0.1411]
Loss:[0.1366]
Loss:[0.1394]
Loss:[0.1348]
Loss:[0.1284]
Loss:[0.1211]
Loss:[0.1237]
Loss:[0.1148]
Loss:[0.1134]
Loss:[0.1166]
Loss:[0.1218]
Loss:[0.1115]
Loss:[0.1124]
Loss:[0.1050]
Loss:[0.1047]
Loss:[0.0999]
Loss:[0.0937]
Loss:[0.0927]
Loss:[0.0981]
Loss:[0.0905]
Loss:[0.0966]
Loss:[0.0962]
Loss:[0.0919]
Loss:[0.0840]
Loss:[0.0825]
Loss:[0.0787]
Loss:[0.0790]
Loss:[0.0815]
Loss:[0.0743]
Loss:[0.0754]
Loss:[0.0774]
Loss:[0.0730]
Loss:[0.0679]
Loss:[0.0670]
Loss:[0.0702]
Loss:[0.0637]
Loss:[0.0652]
Loss:[0.0598]
Loss:[0.0609]
Loss:[0.0629]
Loss:[0.0535]
Loss:[0.0621]
Loss:[0.0616]
Loss:[0.0538]
Loss:[0.0554]
Loss:[0.0584]
Loss:[0.0547]
Loss:[0.0516]
Loss:[0.0442]
Loss:[0.0467]
Loss:[0.0503]
Loss:[0.0471]
Loss:[0.0492]
Loss:[0.0458]
Loss:[0.0491]
Loss:[0.0450]
Loss:[0.0408]
Loss:[0.0479]
Loss:[0.0417]
Loss:[0.0445]
Loss:[0.0388]
Loss:[0.0447]
Loss:[0.0392]
Loss:[0.0376]
Loss:[0.0409]
Loss:[0.0372]
Loss:[0.0345]
Loss:[0.0355]
Loss:[0.0331]
Loss:[0.0363]
Loss:[0.0370]
Loss:[0.0318]
Loss:[0.0338]
Loss:[0.0344]
Loss:[0.0318]
Loss:[0.0339]
Loss:[0.0299]
Loss:[0.0346]
Loss:[0.0317]
Loss:[0.0324]
Loss:[0.0312]
Loss:[0.0233]
Loss:[0.0286]
Loss:[0.0258]
Loss:[0.0304]
Loss:[0.0301]
Loss:[0.0261]
Loss:[0.0292]
Loss:[0.0254]
Loss:[0.0241]
Loss:[0.0293]
Loss:[0.0258]
Loss:[0.0264]
Loss:[0.0315]
Loss:[0.0283]
Loss:[0.0242]
Loss:[0.0245]
Loss:[0.0256]
Loss:[0.0236]
Loss:[0.0214]
Loss:[0.0244]
Loss:[0.0215]
Loss:[0.0209]
Loss:[0.0240]
Loss:[0.0231]
Loss:[0.0211]
Loss:[0.0229]
Loss:[0.0220]
Loss:[0.0218]
Loss:[0.0196]
Loss:[0.0220]
Loss:[0.0216]
Loss:[0.0239]
Loss:[0.0215]
Loss:[0.0170]
Loss:[0.0204]
Loss:[0.0224]
Loss:[0.0216]
Loss:[0.0207]
Loss:[0.0221]
Loss:[0.0182]
Loss:[0.0221]
Loss:[0.0190]
Loss:[0.0167]
Loss:[0.0198]
Loss:[0.0194]
Loss:[0.0207]
Loss:[0.0244]
Loss:[0.0210]
Loss:[0.0178]
Loss:[0.0190]
Loss:[0.0151]
Loss:[0.0177]
Loss:[0.0152]
Loss:[0.0208]
Loss:[0.0175]
Loss:[0.0168]
Loss:[0.0135]
Loss:[0.0175]
Loss:[0.0147]
Loss:[0.0152]
Loss:[0.0166]
Loss:[0.0165]
Loss:[0.0173]
Loss:[0.0142]
Loss:[0.0135]
Loss:[0.0193]
Loss:[0.0160]
Loss:[0.0145]
Loss:[0.0119]
Loss:[0.0203]
Loss:[0.0132]
Loss:[0.0142]
Loss:[0.0126]
Loss:[0.0147]
Loss:[0.0152]
Loss:[0.0125]
Loss:[0.0139]
Loss:[0.0147]
Loss:[0.0136]
Loss:[0.0126]
Loss:[0.0121]
Loss:[0.0148]
Loss:[0.0146]
Loss:[0.0104]
Loss:[0.0124]
Loss:[0.0110]
Loss:[0.0150]
Loss:[0.0111]
Loss:[0.0101]
Loss:[0.0119]
Loss:[0.0144]
Loss:[0.0151]
Loss:[0.0150]
Loss:[0.0171]
Loss:[0.0146]
Loss:[0.0140]
Loss:[0.0127]
Loss:[0.0096]
Loss:[0.0138]
Loss:[0.0128]
Loss:[0.0137]
Loss:[0.0108]
Loss:[0.0092]
Loss:[0.0125]
Loss:[0.0127]
Loss:[0.0135]
Loss:[0.0156]
Loss:[0.0129]
Loss:[0.0103]
Loss:[0.0093]
Loss:[0.0076]
Loss:[0.0097]
Loss:[0.0106]
Loss:[0.0104]
Loss:[0.0106]
Loss:[0.0105]
Loss:[0.0184]
Loss:[0.0113]
Loss:[0.0102]
Loss:[0.0101]
Loss:[0.0099]
Loss:[0.0113]
Loss:[0.0117]
Loss:[0.0107]
Loss:[0.0105]
Loss:[0.0095]
Loss:[0.0119]
Loss:[0.0104]
Loss:[0.0125]
Loss:[0.0110]
Loss:[0.0113]
Early stopping!
Loading 261th epoch
acc:[0.6770]
acc:[0.6830]
acc:[0.6780]
acc:[0.6790]
acc:[0.6790]
acc:[0.6810]
acc:[0.6790]
acc:[0.6770]
acc:[0.6750]
acc:[0.6790]
acc:[0.6780]
acc:[0.6810]
acc:[0.6780]
acc:[0.6780]
acc:[0.6770]
acc:[0.6770]
acc:[0.6790]
acc:[0.6810]
acc:[0.6770]
acc:[0.6780]
acc:[0.6790]
acc:[0.6770]
acc:[0.6780]
acc:[0.6770]
acc:[0.6760]
acc:[0.6750]
acc:[0.6790]
acc:[0.6770]
acc:[0.6800]
acc:[0.6780]
acc:[0.6800]
acc:[0.6760]
acc:[0.6830]
acc:[0.6820]
acc:[0.6780]
acc:[0.6820]
acc:[0.6750]
acc:[0.6780]
acc:[0.6840]
acc:[0.6800]
acc:[0.6780]
acc:[0.6770]
acc:[0.6790]
acc:[0.6750]
acc:[0.6750]
acc:[0.6790]
acc:[0.6780]
acc:[0.6750]
acc:[0.6800]
acc:[0.6810]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.6784]
Mean:[67.8440]
Std :[0.2215]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='cora', drop_percent=0.6, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
Using CUDA
Loss:[0.6933]
Loss:[0.6748]
Loss:[0.6467]
Loss:[0.6101]
Loss:[0.5657]
Loss:[0.5141]
Loss:[0.4600]
Loss:[0.4102]
Loss:[0.3639]
Loss:[0.3230]
Loss:[0.2939]
Loss:[0.2740]
Loss:[0.2590]
Loss:[0.2514]
Loss:[0.2455]
Loss:[0.2412]
Loss:[0.2356]
Loss:[0.2318]
Loss:[0.2353]
Loss:[0.2251]
Loss:[0.2193]
Loss:[0.2112]
Loss:[0.2139]
Loss:[0.2020]
Loss:[0.2049]
Loss:[0.1997]
Loss:[0.1954]
Loss:[0.1966]
Loss:[0.1861]
Loss:[0.1792]
Loss:[0.1781]
Loss:[0.1737]
Loss:[0.1758]
Loss:[0.1805]
Loss:[0.1729]
Loss:[0.1706]
Loss:[0.1653]
Loss:[0.1678]
Loss:[0.1611]
Loss:[0.1596]
Loss:[0.1637]
Loss:[0.1524]
Loss:[0.1522]
Loss:[0.1561]
Loss:[0.1561]
Loss:[0.1449]
Loss:[0.1393]
Loss:[0.1525]
Loss:[0.1468]
Loss:[0.1463]
Loss:[0.1437]
Loss:[0.1485]
Loss:[0.1413]
Loss:[0.1377]
Loss:[0.1391]
Loss:[0.1378]
Loss:[0.1304]
Loss:[0.1304]
Loss:[0.1338]
Loss:[0.1208]
Loss:[0.1257]
Loss:[0.1308]
Loss:[0.1267]
Loss:[0.1199]
Loss:[0.1206]
Loss:[0.1192]
Loss:[0.1138]
Loss:[0.1097]
Loss:[0.1121]
Loss:[0.1174]
Loss:[0.1108]
Loss:[0.1158]
Loss:[0.1159]
Loss:[0.1103]
Loss:[0.1097]
Loss:[0.0993]
Loss:[0.1060]
Loss:[0.0983]
Loss:[0.1086]
Loss:[0.1045]
Loss:[0.0939]
Loss:[0.0906]
Loss:[0.0935]
Loss:[0.0935]
Loss:[0.0938]
Loss:[0.0954]
Loss:[0.0896]
Loss:[0.0909]
Loss:[0.0906]
Loss:[0.0817]
Loss:[0.0824]
Loss:[0.0827]
Loss:[0.0820]
Loss:[0.0825]
Loss:[0.0791]
Loss:[0.0751]
Loss:[0.0763]
Loss:[0.0789]
Loss:[0.0817]
Loss:[0.0728]
Loss:[0.0733]
Loss:[0.0714]
Loss:[0.0672]
Loss:[0.0684]
Loss:[0.0691]
Loss:[0.0733]
Loss:[0.0626]
Loss:[0.0652]
Loss:[0.0642]
Loss:[0.0606]
Loss:[0.0621]
Loss:[0.0614]
Loss:[0.0677]
Loss:[0.0621]
Loss:[0.0546]
Loss:[0.0621]
Loss:[0.0617]
Loss:[0.0507]
Loss:[0.0684]
Loss:[0.0572]
Loss:[0.0482]
Loss:[0.0605]
Loss:[0.0549]
Loss:[0.0590]
Loss:[0.0554]
Loss:[0.0469]
Loss:[0.0473]
Loss:[0.0492]
Loss:[0.0462]
Loss:[0.0488]
Loss:[0.0430]
Loss:[0.0443]
Loss:[0.0505]
Loss:[0.0377]
Loss:[0.0461]
Loss:[0.0376]
Loss:[0.0437]
Loss:[0.0382]
Loss:[0.0439]
Loss:[0.0400]
Loss:[0.0381]
Loss:[0.0330]
Loss:[0.0400]
Loss:[0.0353]
Loss:[0.0370]
Loss:[0.0369]
Loss:[0.0361]
Loss:[0.0374]
Loss:[0.0371]
Loss:[0.0371]
Loss:[0.0348]
Loss:[0.0342]
Loss:[0.0364]
Loss:[0.0354]
Loss:[0.0309]
Loss:[0.0303]
Loss:[0.0306]
Loss:[0.0324]
Loss:[0.0287]
Loss:[0.0298]
Loss:[0.0259]
Loss:[0.0321]
Loss:[0.0253]
Loss:[0.0396]
Loss:[0.0277]
Loss:[0.0284]
Loss:[0.0234]
Loss:[0.0259]
Loss:[0.0251]
Loss:[0.0256]
Loss:[0.0257]
Loss:[0.0268]
Loss:[0.0208]
Loss:[0.0198]
Loss:[0.0243]
Loss:[0.0228]
Loss:[0.0272]
Loss:[0.0250]
Loss:[0.0268]
Loss:[0.0258]
Loss:[0.0202]
Loss:[0.0195]
Loss:[0.0208]
Loss:[0.0208]
Loss:[0.0207]
Loss:[0.0218]
Loss:[0.0228]
Loss:[0.0198]
Loss:[0.0193]
Loss:[0.0257]
Loss:[0.0198]
Loss:[0.0219]
Loss:[0.0257]
Loss:[0.0208]
Loss:[0.0192]
Loss:[0.0194]
Loss:[0.0231]
Loss:[0.0212]
Loss:[0.0194]
Loss:[0.0180]
Loss:[0.0230]
Loss:[0.0153]
Loss:[0.0169]
Loss:[0.0183]
Loss:[0.0168]
Loss:[0.0161]
Loss:[0.0185]
Loss:[0.0238]
Loss:[0.0190]
Loss:[0.0191]
Loss:[0.0228]
Loss:[0.0168]
Loss:[0.0166]
Loss:[0.0189]
Loss:[0.0163]
Loss:[0.0177]
Loss:[0.0154]
Loss:[0.0213]
Loss:[0.0160]
Loss:[0.0141]
Loss:[0.0152]
Loss:[0.0161]
Loss:[0.0154]
Loss:[0.0152]
Loss:[0.0145]
Loss:[0.0156]
Loss:[0.0182]
Loss:[0.0143]
Loss:[0.0207]
Loss:[0.0155]
Loss:[0.0143]
Loss:[0.0131]
Loss:[0.0153]
Loss:[0.0162]
Loss:[0.0149]
Loss:[0.0147]
Loss:[0.0162]
Loss:[0.0124]
Loss:[0.0128]
Loss:[0.0157]
Loss:[0.0149]
Loss:[0.0152]
Loss:[0.0125]
Loss:[0.0132]
Loss:[0.0164]
Loss:[0.0160]
Loss:[0.0181]
Loss:[0.0162]
Loss:[0.0101]
Loss:[0.0163]
Loss:[0.0134]
Loss:[0.0140]
Loss:[0.0143]
Loss:[0.0122]
Loss:[0.0112]
Loss:[0.0167]
Loss:[0.0131]
Loss:[0.0136]
Loss:[0.0134]
Loss:[0.0143]
Loss:[0.0128]
Loss:[0.0165]
Loss:[0.0106]
Loss:[0.0109]
Loss:[0.0141]
Loss:[0.0123]
Loss:[0.0113]
Loss:[0.0161]
Loss:[0.0145]
Early stopping!
Loading 248th epoch
acc:[0.5610]
acc:[0.5600]
acc:[0.5640]
acc:[0.5630]
acc:[0.5630]
acc:[0.5680]
acc:[0.5620]
acc:[0.5610]
acc:[0.5540]
acc:[0.5570]
acc:[0.5610]
acc:[0.5590]
acc:[0.5660]
acc:[0.5600]
acc:[0.5570]
acc:[0.5600]
acc:[0.5580]
acc:[0.5650]
acc:[0.5620]
acc:[0.5570]
acc:[0.5670]
acc:[0.5660]
acc:[0.5640]
acc:[0.5560]
acc:[0.5600]
acc:[0.5610]
acc:[0.5610]
acc:[0.5630]
acc:[0.5630]
acc:[0.5620]
acc:[0.5600]
acc:[0.5620]
acc:[0.5660]
acc:[0.5570]
acc:[0.5630]
acc:[0.5570]
acc:[0.5560]
acc:[0.5650]
acc:[0.5620]
acc:[0.5640]
acc:[0.5570]
acc:[0.5620]
acc:[0.5610]
acc:[0.5620]
acc:[0.5610]
acc:[0.5560]
acc:[0.5610]
acc:[0.5590]
acc:[0.5600]
acc:[0.5660]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.5612]
Mean:[56.1160]
Std :[0.3235]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='citeseer', drop_percent=0.02, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6918]
Loss:[0.6890]
Loss:[0.6856]
Loss:[0.6812]
Loss:[0.6757]
Loss:[0.6700]
Loss:[0.6616]
Loss:[0.6546]
Loss:[0.6440]
Loss:[0.6337]
Loss:[0.6246]
Loss:[0.6100]
Loss:[0.5973]
Loss:[0.5856]
Loss:[0.5699]
Loss:[0.5561]
Loss:[0.5415]
Loss:[0.5238]
Loss:[0.5068]
Loss:[0.4907]
Loss:[0.4780]
Loss:[0.4593]
Loss:[0.4412]
Loss:[0.4241]
Loss:[0.4125]
Loss:[0.4005]
Loss:[0.3815]
Loss:[0.3657]
Loss:[0.3550]
Loss:[0.3403]
Loss:[0.3303]
Loss:[0.3198]
Loss:[0.3046]
Loss:[0.2875]
Loss:[0.2846]
Loss:[0.2843]
Loss:[0.2724]
Loss:[0.2571]
Loss:[0.2438]
Loss:[0.2371]
Loss:[0.2373]
Loss:[0.2307]
Loss:[0.2157]
Loss:[0.2130]
Loss:[0.2059]
Loss:[0.2005]
Loss:[0.1924]
Loss:[0.1901]
Loss:[0.1878]
Loss:[0.1862]
Loss:[0.1834]
Loss:[0.1736]
Loss:[0.1745]
Loss:[0.1716]
Loss:[0.1621]
Loss:[0.1561]
Loss:[0.1658]
Loss:[0.1631]
Loss:[0.1496]
Loss:[0.1502]
Loss:[0.1501]
Loss:[0.1500]
Loss:[0.1467]
Loss:[0.1436]
Loss:[0.1475]
Loss:[0.1404]
Loss:[0.1434]
Loss:[0.1401]
Loss:[0.1401]
Loss:[0.1351]
Loss:[0.1338]
Loss:[0.1280]
Loss:[0.1271]
Loss:[0.1354]
Loss:[0.1285]
Loss:[0.1274]
Loss:[0.1263]
Loss:[0.1291]
Loss:[0.1186]
Loss:[0.1193]
Loss:[0.1212]
Loss:[0.1224]
Loss:[0.1153]
Loss:[0.1168]
Loss:[0.1196]
Loss:[0.1224]
Loss:[0.1120]
Loss:[0.1140]
Loss:[0.1152]
Loss:[0.1105]
Loss:[0.1142]
Loss:[0.1117]
Loss:[0.1148]
Loss:[0.1136]
Loss:[0.1194]
Loss:[0.1146]
Loss:[0.1091]
Loss:[0.1125]
Loss:[0.1130]
Loss:[0.1073]
Loss:[0.1031]
Loss:[0.1090]
Loss:[0.1085]
Loss:[0.1094]
Loss:[0.1073]
Loss:[0.1049]
Loss:[0.1018]
Loss:[0.1062]
Loss:[0.1075]
Loss:[0.1084]
Loss:[0.1117]
Loss:[0.0969]
Loss:[0.1013]
Loss:[0.1136]
Loss:[0.1057]
Loss:[0.1127]
Loss:[0.1084]
Loss:[0.1003]
Loss:[0.1124]
Loss:[0.0987]
Loss:[0.1025]
Loss:[0.0992]
Loss:[0.0992]
Loss:[0.0943]
Loss:[0.0918]
Loss:[0.0928]
Loss:[0.0993]
Loss:[0.0958]
Loss:[0.1020]
Loss:[0.0985]
Loss:[0.1035]
Loss:[0.0946]
Loss:[0.0972]
Loss:[0.0891]
Loss:[0.0948]
Loss:[0.0911]
Loss:[0.0912]
Loss:[0.0962]
Loss:[0.0998]
Loss:[0.0910]
Loss:[0.0941]
Loss:[0.0879]
Loss:[0.0928]
Loss:[0.0943]
Loss:[0.0883]
Loss:[0.0964]
Loss:[0.0985]
Loss:[0.0956]
Loss:[0.0928]
Loss:[0.0964]
Loss:[0.0938]
Loss:[0.0960]
Loss:[0.0887]
Loss:[0.0950]
Loss:[0.0923]
Loss:[0.0916]
Loss:[0.0910]
Loss:[0.0903]
Loss:[0.0846]
Loss:[0.0943]
Loss:[0.0943]
Loss:[0.0938]
Loss:[0.0889]
Loss:[0.0849]
Loss:[0.0885]
Loss:[0.0878]
Loss:[0.0888]
Loss:[0.0901]
Loss:[0.0825]
Loss:[0.0918]
Loss:[0.0899]
Loss:[0.0892]
Loss:[0.0819]
Loss:[0.0880]
Loss:[0.0878]
Loss:[0.0880]
Loss:[0.0843]
Loss:[0.0885]
Loss:[0.0922]
Loss:[0.0913]
Loss:[0.0903]
Loss:[0.0931]
Loss:[0.0875]
Loss:[0.0866]
Loss:[0.0889]
Loss:[0.0852]
Loss:[0.0869]
Loss:[0.0906]
Loss:[0.0920]
Loss:[0.0831]
Loss:[0.0860]
Loss:[0.0942]
Loss:[0.0921]
Early stopping!
Loading 173th epoch
acc:[0.7320]
acc:[0.7340]
acc:[0.7330]
acc:[0.7300]
acc:[0.7310]
acc:[0.7330]
acc:[0.7300]
acc:[0.7330]
acc:[0.7310]
acc:[0.7310]
acc:[0.7320]
acc:[0.7320]
acc:[0.7320]
acc:[0.7310]
acc:[0.7320]
acc:[0.7340]
acc:[0.7320]
acc:[0.7310]
acc:[0.7330]
acc:[0.7310]
acc:[0.7320]
acc:[0.7330]
acc:[0.7350]
acc:[0.7340]
acc:[0.7330]
acc:[0.7310]
acc:[0.7340]
acc:[0.7320]
acc:[0.7330]
acc:[0.7300]
acc:[0.7340]
acc:[0.7320]
acc:[0.7340]
acc:[0.7330]
acc:[0.7340]
acc:[0.7290]
acc:[0.7290]
acc:[0.7320]
acc:[0.7340]
acc:[0.7320]
acc:[0.7330]
acc:[0.7300]
acc:[0.7340]
acc:[0.7310]
acc:[0.7350]
acc:[0.7310]
acc:[0.7310]
acc:[0.7330]
acc:[0.7280]
acc:[0.7330]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7321]
Mean:[73.2140]
Std :[0.1591]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='citeseer', drop_percent=0.05, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6918]
Loss:[0.6891]
Loss:[0.6858]
Loss:[0.6814]
Loss:[0.6760]
Loss:[0.6704]
Loss:[0.6621]
Loss:[0.6552]
Loss:[0.6448]
Loss:[0.6347]
Loss:[0.6256]
Loss:[0.6112]
Loss:[0.5989]
Loss:[0.5870]
Loss:[0.5716]
Loss:[0.5582]
Loss:[0.5437]
Loss:[0.5260]
Loss:[0.5092]
Loss:[0.4933]
Loss:[0.4808]
Loss:[0.4624]
Loss:[0.4444]
Loss:[0.4273]
Loss:[0.4159]
Loss:[0.4040]
Loss:[0.3853]
Loss:[0.3694]
Loss:[0.3588]
Loss:[0.3442]
Loss:[0.3348]
Loss:[0.3233]
Loss:[0.3084]
Loss:[0.2922]
Loss:[0.2891]
Loss:[0.2882]
Loss:[0.2766]
Loss:[0.2602]
Loss:[0.2482]
Loss:[0.2415]
Loss:[0.2404]
Loss:[0.2335]
Loss:[0.2199]
Loss:[0.2169]
Loss:[0.2095]
Loss:[0.2039]
Loss:[0.1958]
Loss:[0.1941]
Loss:[0.1919]
Loss:[0.1907]
Loss:[0.1876]
Loss:[0.1775]
Loss:[0.1779]
Loss:[0.1752]
Loss:[0.1659]
Loss:[0.1593]
Loss:[0.1684]
Loss:[0.1670]
Loss:[0.1530]
Loss:[0.1540]
Loss:[0.1530]
Loss:[0.1527]
Loss:[0.1485]
Loss:[0.1456]
Loss:[0.1496]
Loss:[0.1436]
Loss:[0.1458]
Loss:[0.1425]
Loss:[0.1425]
Loss:[0.1374]
Loss:[0.1351]
Loss:[0.1305]
Loss:[0.1289]
Loss:[0.1379]
Loss:[0.1310]
Loss:[0.1298]
Loss:[0.1291]
Loss:[0.1315]
Loss:[0.1200]
Loss:[0.1209]
Loss:[0.1231]
Loss:[0.1258]
Loss:[0.1172]
Loss:[0.1193]
Loss:[0.1220]
Loss:[0.1248]
Loss:[0.1138]
Loss:[0.1159]
Loss:[0.1170]
Loss:[0.1135]
Loss:[0.1158]
Loss:[0.1130]
Loss:[0.1153]
Loss:[0.1155]
Loss:[0.1202]
Loss:[0.1164]
Loss:[0.1106]
Loss:[0.1133]
Loss:[0.1145]
Loss:[0.1092]
Loss:[0.1036]
Loss:[0.1105]
Loss:[0.1103]
Loss:[0.1100]
Loss:[0.1089]
Loss:[0.1057]
Loss:[0.1022]
Loss:[0.1078]
Loss:[0.1078]
Loss:[0.1076]
Loss:[0.1116]
Loss:[0.0986]
Loss:[0.0994]
Loss:[0.1134]
Loss:[0.1066]
Loss:[0.1085]
Loss:[0.1044]
Loss:[0.0998]
Loss:[0.1069]
Loss:[0.0985]
Loss:[0.1000]
Loss:[0.0970]
Loss:[0.1009]
Loss:[0.0942]
Loss:[0.0924]
Loss:[0.0940]
Loss:[0.1012]
Loss:[0.0966]
Loss:[0.1040]
Loss:[0.1003]
Loss:[0.1046]
Loss:[0.0968]
Loss:[0.0989]
Loss:[0.0916]
Loss:[0.0974]
Loss:[0.0931]
Loss:[0.0938]
Loss:[0.0994]
Loss:[0.1018]
Loss:[0.0925]
Loss:[0.0953]
Loss:[0.0903]
Loss:[0.0935]
Loss:[0.0976]
Loss:[0.0918]
Loss:[0.0998]
Loss:[0.1004]
Loss:[0.0978]
Loss:[0.0950]
Loss:[0.0981]
Loss:[0.0971]
Loss:[0.0991]
Loss:[0.0895]
Loss:[0.0976]
Loss:[0.0955]
Loss:[0.0927]
Loss:[0.0944]
Loss:[0.0931]
Loss:[0.0869]
Loss:[0.0957]
Loss:[0.0948]
Loss:[0.0946]
Loss:[0.0904]
Loss:[0.0870]
Loss:[0.0895]
Loss:[0.0882]
Loss:[0.0886]
Loss:[0.0904]
Loss:[0.0829]
Loss:[0.0922]
Loss:[0.0907]
Loss:[0.0905]
Loss:[0.0848]
Loss:[0.0904]
Loss:[0.0896]
Loss:[0.0890]
Loss:[0.0856]
Loss:[0.0892]
Loss:[0.0940]
Loss:[0.0912]
Loss:[0.0908]
Loss:[0.0942]
Loss:[0.0893]
Loss:[0.0873]
Loss:[0.0902]
Loss:[0.0866]
Loss:[0.0889]
Loss:[0.0905]
Loss:[0.0929]
Early stopping!
Loading 169th epoch
acc:[0.7370]
acc:[0.7330]
acc:[0.7340]
acc:[0.7330]
acc:[0.7390]
acc:[0.7350]
acc:[0.7380]
acc:[0.7380]
acc:[0.7360]
acc:[0.7330]
acc:[0.7370]
acc:[0.7350]
acc:[0.7340]
acc:[0.7320]
acc:[0.7370]
acc:[0.7350]
acc:[0.7320]
acc:[0.7370]
acc:[0.7370]
acc:[0.7350]
acc:[0.7380]
acc:[0.7350]
acc:[0.7320]
acc:[0.7380]
acc:[0.7330]
acc:[0.7340]
acc:[0.7330]
acc:[0.7380]
acc:[0.7330]
acc:[0.7340]
acc:[0.7350]
acc:[0.7360]
acc:[0.7360]
acc:[0.7390]
acc:[0.7370]
acc:[0.7350]
acc:[0.7380]
acc:[0.7360]
acc:[0.7370]
acc:[0.7350]
acc:[0.7370]
acc:[0.7370]
acc:[0.7350]
acc:[0.7360]
acc:[0.7330]
acc:[0.7320]
acc:[0.7350]
acc:[0.7320]
acc:[0.7330]
acc:[0.7360]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7353]
Mean:[73.5300]
Std :[0.2043]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='citeseer', drop_percent=0.08, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6918]
Loss:[0.6892]
Loss:[0.6859]
Loss:[0.6816]
Loss:[0.6762]
Loss:[0.6706]
Loss:[0.6624]
Loss:[0.6557]
Loss:[0.6453]
Loss:[0.6354]
Loss:[0.6264]
Loss:[0.6121]
Loss:[0.5999]
Loss:[0.5883]
Loss:[0.5728]
Loss:[0.5595]
Loss:[0.5452]
Loss:[0.5279]
Loss:[0.5114]
Loss:[0.4952]
Loss:[0.4827]
Loss:[0.4651]
Loss:[0.4477]
Loss:[0.4297]
Loss:[0.4174]
Loss:[0.4063]
Loss:[0.3889]
Loss:[0.3726]
Loss:[0.3610]
Loss:[0.3455]
Loss:[0.3373]
Loss:[0.3278]
Loss:[0.3131]
Loss:[0.2950]
Loss:[0.2901]
Loss:[0.2902]
Loss:[0.2791]
Loss:[0.2646]
Loss:[0.2513]
Loss:[0.2434]
Loss:[0.2431]
Loss:[0.2373]
Loss:[0.2219]
Loss:[0.2187]
Loss:[0.2115]
Loss:[0.2068]
Loss:[0.1987]
Loss:[0.1970]
Loss:[0.1937]
Loss:[0.1932]
Loss:[0.1905]
Loss:[0.1794]
Loss:[0.1792]
Loss:[0.1760]
Loss:[0.1698]
Loss:[0.1625]
Loss:[0.1710]
Loss:[0.1688]
Loss:[0.1531]
Loss:[0.1563]
Loss:[0.1590]
Loss:[0.1627]
Loss:[0.1517]
Loss:[0.1499]
Loss:[0.1584]
Loss:[0.1458]
Loss:[0.1515]
Loss:[0.1513]
Loss:[0.1428]
Loss:[0.1402]
Loss:[0.1432]
Loss:[0.1310]
Loss:[0.1345]
Loss:[0.1429]
Loss:[0.1323]
Loss:[0.1325]
Loss:[0.1305]
Loss:[0.1337]
Loss:[0.1286]
Loss:[0.1256]
Loss:[0.1275]
Loss:[0.1332]
Loss:[0.1195]
Loss:[0.1238]
Loss:[0.1235]
Loss:[0.1262]
Loss:[0.1161]
Loss:[0.1180]
Loss:[0.1197]
Loss:[0.1122]
Loss:[0.1207]
Loss:[0.1145]
Loss:[0.1140]
Loss:[0.1204]
Loss:[0.1195]
Loss:[0.1140]
Loss:[0.1133]
Loss:[0.1148]
Loss:[0.1188]
Loss:[0.1100]
Loss:[0.1074]
Loss:[0.1122]
Loss:[0.1128]
Loss:[0.1128]
Loss:[0.1095]
Loss:[0.1093]
Loss:[0.1035]
Loss:[0.1094]
Loss:[0.1089]
Loss:[0.1066]
Loss:[0.1099]
Loss:[0.1012]
Loss:[0.1021]
Loss:[0.1118]
Loss:[0.1075]
Loss:[0.1104]
Loss:[0.1051]
Loss:[0.1017]
Loss:[0.1071]
Loss:[0.1027]
Loss:[0.1035]
Loss:[0.0980]
Loss:[0.1053]
Loss:[0.0951]
Loss:[0.0967]
Loss:[0.0953]
Loss:[0.1041]
Loss:[0.1018]
Loss:[0.1061]
Loss:[0.1053]
Loss:[0.1068]
Loss:[0.0994]
Loss:[0.1006]
Loss:[0.0945]
Loss:[0.1000]
Loss:[0.0946]
Loss:[0.0972]
Loss:[0.0995]
Loss:[0.1020]
Loss:[0.0952]
Loss:[0.0960]
Loss:[0.0925]
Loss:[0.0942]
Loss:[0.0981]
Loss:[0.0918]
Loss:[0.1009]
Loss:[0.1030]
Loss:[0.0983]
Loss:[0.0971]
Loss:[0.0997]
Loss:[0.0992]
Loss:[0.0973]
Loss:[0.0935]
Loss:[0.0985]
Loss:[0.0939]
Loss:[0.0969]
Loss:[0.0930]
Loss:[0.0918]
Loss:[0.0864]
Loss:[0.0984]
Loss:[0.0968]
Loss:[0.0983]
Loss:[0.0939]
Loss:[0.0884]
Loss:[0.0927]
Loss:[0.0890]
Loss:[0.0910]
Loss:[0.0929]
Loss:[0.0833]
Loss:[0.0946]
Loss:[0.0927]
Loss:[0.0935]
Loss:[0.0853]
Loss:[0.0936]
Loss:[0.0894]
Loss:[0.0896]
Loss:[0.0873]
Loss:[0.0902]
Loss:[0.0947]
Loss:[0.0927]
Loss:[0.0931]
Loss:[0.0951]
Loss:[0.0899]
Loss:[0.0892]
Loss:[0.0908]
Loss:[0.0869]
Loss:[0.0889]
Loss:[0.0911]
Loss:[0.0918]
Early stopping!
Loading 169th epoch
acc:[0.7240]
acc:[0.7190]
acc:[0.7220]
acc:[0.7250]
acc:[0.7210]
acc:[0.7230]
acc:[0.7260]
acc:[0.7260]
acc:[0.7250]
acc:[0.7240]
acc:[0.7250]
acc:[0.7250]
acc:[0.7240]
acc:[0.7270]
acc:[0.7280]
acc:[0.7220]
acc:[0.7230]
acc:[0.7220]
acc:[0.7250]
acc:[0.7230]
acc:[0.7240]
acc:[0.7280]
acc:[0.7230]
acc:[0.7270]
acc:[0.7250]
acc:[0.7260]
acc:[0.7210]
acc:[0.7240]
acc:[0.7270]
acc:[0.7170]
acc:[0.7260]
acc:[0.7240]
acc:[0.7260]
acc:[0.7240]
acc:[0.7230]
acc:[0.7240]
acc:[0.7210]
acc:[0.7240]
acc:[0.7250]
acc:[0.7260]
acc:[0.7230]
acc:[0.7200]
acc:[0.7230]
acc:[0.7260]
acc:[0.7240]
acc:[0.7210]
acc:[0.7240]
acc:[0.7280]
acc:[0.7230]
acc:[0.7220]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7240]
Mean:[72.3960]
Std :[0.2303]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='citeseer', drop_percent=0.1, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6918]
Loss:[0.6892]
Loss:[0.6860]
Loss:[0.6816]
Loss:[0.6764]
Loss:[0.6708]
Loss:[0.6627]
Loss:[0.6559]
Loss:[0.6457]
Loss:[0.6358]
Loss:[0.6268]
Loss:[0.6126]
Loss:[0.6005]
Loss:[0.5889]
Loss:[0.5737]
Loss:[0.5603]
Loss:[0.5461]
Loss:[0.5291]
Loss:[0.5124]
Loss:[0.4962]
Loss:[0.4839]
Loss:[0.4664]
Loss:[0.4490]
Loss:[0.4311]
Loss:[0.4189]
Loss:[0.4077]
Loss:[0.3907]
Loss:[0.3746]
Loss:[0.3630]
Loss:[0.3468]
Loss:[0.3387]
Loss:[0.3298]
Loss:[0.3155]
Loss:[0.2969]
Loss:[0.2911]
Loss:[0.2908]
Loss:[0.2810]
Loss:[0.2670]
Loss:[0.2529]
Loss:[0.2447]
Loss:[0.2448]
Loss:[0.2401]
Loss:[0.2244]
Loss:[0.2203]
Loss:[0.2134]
Loss:[0.2094]
Loss:[0.2007]
Loss:[0.1981]
Loss:[0.1952]
Loss:[0.1944]
Loss:[0.1914]
Loss:[0.1808]
Loss:[0.1800]
Loss:[0.1762]
Loss:[0.1704]
Loss:[0.1646]
Loss:[0.1738]
Loss:[0.1719]
Loss:[0.1562]
Loss:[0.1563]
Loss:[0.1599]
Loss:[0.1652]
Loss:[0.1544]
Loss:[0.1494]
Loss:[0.1584]
Loss:[0.1482]
Loss:[0.1503]
Loss:[0.1515]
Loss:[0.1460]
Loss:[0.1394]
Loss:[0.1411]
Loss:[0.1336]
Loss:[0.1329]
Loss:[0.1433]
Loss:[0.1341]
Loss:[0.1309]
Loss:[0.1324]
Loss:[0.1335]
Loss:[0.1255]
Loss:[0.1284]
Loss:[0.1282]
Loss:[0.1316]
Loss:[0.1243]
Loss:[0.1220]
Loss:[0.1255]
Loss:[0.1304]
Loss:[0.1165]
Loss:[0.1193]
Loss:[0.1174]
Loss:[0.1138]
Loss:[0.1185]
Loss:[0.1136]
Loss:[0.1159]
Loss:[0.1188]
Loss:[0.1210]
Loss:[0.1137]
Loss:[0.1141]
Loss:[0.1152]
Loss:[0.1164]
Loss:[0.1119]
Loss:[0.1067]
Loss:[0.1110]
Loss:[0.1142]
Loss:[0.1120]
Loss:[0.1112]
Loss:[0.1095]
Loss:[0.1057]
Loss:[0.1094]
Loss:[0.1122]
Loss:[0.1097]
Loss:[0.1096]
Loss:[0.1024]
Loss:[0.1022]
Loss:[0.1138]
Loss:[0.1092]
Loss:[0.1099]
Loss:[0.1075]
Loss:[0.1001]
Loss:[0.1078]
Loss:[0.1012]
Loss:[0.1041]
Loss:[0.0983]
Loss:[0.1034]
Loss:[0.0971]
Loss:[0.0955]
Loss:[0.0964]
Loss:[0.1026]
Loss:[0.1002]
Loss:[0.1050]
Loss:[0.1016]
Loss:[0.1096]
Loss:[0.0983]
Loss:[0.1017]
Loss:[0.0948]
Loss:[0.0990]
Loss:[0.0958]
Loss:[0.0926]
Loss:[0.0999]
Loss:[0.1041]
Loss:[0.0938]
Loss:[0.0974]
Loss:[0.0940]
Loss:[0.0971]
Loss:[0.0968]
Loss:[0.0949]
Loss:[0.1022]
Loss:[0.1018]
Loss:[0.1009]
Loss:[0.0994]
Loss:[0.1021]
Loss:[0.1025]
Loss:[0.1045]
Loss:[0.0936]
Loss:[0.1073]
Loss:[0.1038]
Loss:[0.0978]
Loss:[0.1063]
Early stopping!
Loading 137th epoch
acc:[0.7250]
acc:[0.7260]
acc:[0.7250]
acc:[0.7250]
acc:[0.7260]
acc:[0.7250]
acc:[0.7230]
acc:[0.7240]
acc:[0.7250]
acc:[0.7240]
acc:[0.7270]
acc:[0.7260]
acc:[0.7230]
acc:[0.7280]
acc:[0.7240]
acc:[0.7210]
acc:[0.7310]
acc:[0.7270]
acc:[0.7260]
acc:[0.7240]
acc:[0.7230]
acc:[0.7260]
acc:[0.7270]
acc:[0.7280]
acc:[0.7240]
acc:[0.7230]
acc:[0.7280]
acc:[0.7260]
acc:[0.7260]
acc:[0.7250]
acc:[0.7260]
acc:[0.7250]
acc:[0.7250]
acc:[0.7240]
acc:[0.7260]
acc:[0.7220]
acc:[0.7250]
acc:[0.7250]
acc:[0.7240]
acc:[0.7240]
acc:[0.7250]
acc:[0.7240]
acc:[0.7250]
acc:[0.7270]
acc:[0.7250]
acc:[0.7270]
acc:[0.7250]
acc:[0.7240]
acc:[0.7260]
acc:[0.7250]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7252]
Mean:[72.5200]
Std :[0.1702]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='citeseer', drop_percent=0.2, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6919]
Loss:[0.6893]
Loss:[0.6861]
Loss:[0.6820]
Loss:[0.6768]
Loss:[0.6715]
Loss:[0.6634]
Loss:[0.6570]
Loss:[0.6470]
Loss:[0.6372]
Loss:[0.6288]
Loss:[0.6148]
Loss:[0.6031]
Loss:[0.5920]
Loss:[0.5771]
Loss:[0.5645]
Loss:[0.5508]
Loss:[0.5337]
Loss:[0.5173]
Loss:[0.5024]
Loss:[0.4907]
Loss:[0.4725]
Loss:[0.4553]
Loss:[0.4393]
Loss:[0.4279]
Loss:[0.4164]
Loss:[0.3984]
Loss:[0.3831]
Loss:[0.3727]
Loss:[0.3581]
Loss:[0.3498]
Loss:[0.3392]
Loss:[0.3236]
Loss:[0.3067]
Loss:[0.3045]
Loss:[0.3042]
Loss:[0.2940]
Loss:[0.2765]
Loss:[0.2644]
Loss:[0.2586]
Loss:[0.2577]
Loss:[0.2496]
Loss:[0.2343]
Loss:[0.2326]
Loss:[0.2250]
Loss:[0.2193]
Loss:[0.2125]
Loss:[0.2097]
Loss:[0.2069]
Loss:[0.2059]
Loss:[0.1999]
Loss:[0.1931]
Loss:[0.1927]
Loss:[0.1868]
Loss:[0.1793]
Loss:[0.1719]
Loss:[0.1816]
Loss:[0.1792]
Loss:[0.1647]
Loss:[0.1651]
Loss:[0.1643]
Loss:[0.1666]
Loss:[0.1624]
Loss:[0.1589]
Loss:[0.1585]
Loss:[0.1548]
Loss:[0.1567]
Loss:[0.1505]
Loss:[0.1541]
Loss:[0.1510]
Loss:[0.1485]
Loss:[0.1420]
Loss:[0.1399]
Loss:[0.1456]
Loss:[0.1413]
Loss:[0.1364]
Loss:[0.1370]
Loss:[0.1398]
Loss:[0.1317]
Loss:[0.1366]
Loss:[0.1348]
Loss:[0.1333]
Loss:[0.1280]
Loss:[0.1310]
Loss:[0.1323]
Loss:[0.1324]
Loss:[0.1228]
Loss:[0.1284]
Loss:[0.1271]
Loss:[0.1194]
Loss:[0.1256]
Loss:[0.1234]
Loss:[0.1210]
Loss:[0.1264]
Loss:[0.1272]
Loss:[0.1213]
Loss:[0.1194]
Loss:[0.1223]
Loss:[0.1203]
Loss:[0.1173]
Loss:[0.1115]
Loss:[0.1173]
Loss:[0.1205]
Loss:[0.1206]
Loss:[0.1155]
Loss:[0.1128]
Loss:[0.1119]
Loss:[0.1152]
Loss:[0.1138]
Loss:[0.1169]
Loss:[0.1201]
Loss:[0.1058]
Loss:[0.1116]
Loss:[0.1240]
Loss:[0.1129]
Loss:[0.1218]
Loss:[0.1138]
Loss:[0.1063]
Loss:[0.1221]
Loss:[0.1044]
Loss:[0.1086]
Loss:[0.1044]
Loss:[0.1072]
Loss:[0.1016]
Loss:[0.0985]
Loss:[0.1024]
Loss:[0.1046]
Loss:[0.1038]
Loss:[0.1092]
Loss:[0.1042]
Loss:[0.1108]
Loss:[0.1023]
Loss:[0.1051]
Loss:[0.0972]
Loss:[0.1025]
Loss:[0.1000]
Loss:[0.0971]
Loss:[0.1046]
Loss:[0.1036]
Loss:[0.0988]
Loss:[0.1009]
Loss:[0.0952]
Loss:[0.0986]
Loss:[0.1014]
Loss:[0.0963]
Loss:[0.1032]
Loss:[0.1050]
Loss:[0.1011]
Loss:[0.0990]
Loss:[0.1049]
Loss:[0.1004]
Loss:[0.1026]
Loss:[0.0977]
Loss:[0.1026]
Loss:[0.0982]
Loss:[0.0972]
Loss:[0.0965]
Loss:[0.0973]
Loss:[0.0908]
Loss:[0.1009]
Loss:[0.0985]
Loss:[0.0978]
Loss:[0.0942]
Loss:[0.0922]
Loss:[0.0948]
Loss:[0.0940]
Loss:[0.0945]
Loss:[0.0956]
Loss:[0.0880]
Loss:[0.0942]
Loss:[0.0952]
Loss:[0.0955]
Loss:[0.0882]
Loss:[0.0955]
Loss:[0.0923]
Loss:[0.0962]
Loss:[0.0901]
Loss:[0.0933]
Loss:[0.0974]
Loss:[0.0973]
Loss:[0.0941]
Loss:[0.0991]
Loss:[0.0945]
Loss:[0.0902]
Loss:[0.0940]
Loss:[0.0913]
Loss:[0.0922]
Loss:[0.0944]
Loss:[0.0970]
Early stopping!
Loading 169th epoch
acc:[0.7220]
acc:[0.7220]
acc:[0.7190]
acc:[0.7200]
acc:[0.7220]
acc:[0.7200]
acc:[0.7240]
acc:[0.7240]
acc:[0.7230]
acc:[0.7200]
acc:[0.7260]
acc:[0.7240]
acc:[0.7190]
acc:[0.7210]
acc:[0.7250]
acc:[0.7240]
acc:[0.7200]
acc:[0.7250]
acc:[0.7240]
acc:[0.7250]
acc:[0.7240]
acc:[0.7220]
acc:[0.7240]
acc:[0.7230]
acc:[0.7200]
acc:[0.7220]
acc:[0.7200]
acc:[0.7230]
acc:[0.7200]
acc:[0.7200]
acc:[0.7210]
acc:[0.7230]
acc:[0.7240]
acc:[0.7260]
acc:[0.7250]
acc:[0.7250]
acc:[0.7170]
acc:[0.7250]
acc:[0.7250]
acc:[0.7240]
acc:[0.7220]
acc:[0.7220]
acc:[0.7270]
acc:[0.7210]
acc:[0.7190]
acc:[0.7220]
acc:[0.7240]
acc:[0.7190]
acc:[0.7230]
acc:[0.7240]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7225]
Mean:[72.2500]
Std :[0.2270]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='citeseer', drop_percent=0.3, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6920]
Loss:[0.6897]
Loss:[0.6868]
Loss:[0.6827]
Loss:[0.6783]
Loss:[0.6729]
Loss:[0.6658]
Loss:[0.6594]
Loss:[0.6497]
Loss:[0.6411]
Loss:[0.6322]
Loss:[0.6192]
Loss:[0.6082]
Loss:[0.5972]
Loss:[0.5831]
Loss:[0.5706]
Loss:[0.5568]
Loss:[0.5418]
Loss:[0.5257]
Loss:[0.5098]
Loss:[0.4981]
Loss:[0.4825]
Loss:[0.4678]
Loss:[0.4494]
Loss:[0.4364]
Loss:[0.4254]
Loss:[0.4116]
Loss:[0.3964]
Loss:[0.3842]
Loss:[0.3678]
Loss:[0.3604]
Loss:[0.3519]
Loss:[0.3397]
Loss:[0.3221]
Loss:[0.3139]
Loss:[0.3119]
Loss:[0.3062]
Loss:[0.2934]
Loss:[0.2766]
Loss:[0.2677]
Loss:[0.2695]
Loss:[0.2646]
Loss:[0.2463]
Loss:[0.2419]
Loss:[0.2355]
Loss:[0.2322]
Loss:[0.2257]
Loss:[0.2201]
Loss:[0.2202]
Loss:[0.2164]
Loss:[0.2118]
Loss:[0.2029]
Loss:[0.2009]
Loss:[0.2001]
Loss:[0.1892]
Loss:[0.1841]
Loss:[0.1896]
Loss:[0.1877]
Loss:[0.1747]
Loss:[0.1750]
Loss:[0.1768]
Loss:[0.1794]
Loss:[0.1729]
Loss:[0.1680]
Loss:[0.1673]
Loss:[0.1624]
Loss:[0.1666]
Loss:[0.1612]
Loss:[0.1618]
Loss:[0.1569]
Loss:[0.1585]
Loss:[0.1467]
Loss:[0.1506]
Loss:[0.1582]
Loss:[0.1489]
Loss:[0.1438]
Loss:[0.1459]
Loss:[0.1523]
Loss:[0.1407]
Loss:[0.1414]
Loss:[0.1398]
Loss:[0.1415]
Loss:[0.1398]
Loss:[0.1356]
Loss:[0.1377]
Loss:[0.1440]
Loss:[0.1305]
Loss:[0.1307]
Loss:[0.1309]
Loss:[0.1291]
Loss:[0.1319]
Loss:[0.1246]
Loss:[0.1255]
Loss:[0.1320]
Loss:[0.1306]
Loss:[0.1273]
Loss:[0.1238]
Loss:[0.1269]
Loss:[0.1283]
Loss:[0.1263]
Loss:[0.1151]
Loss:[0.1235]
Loss:[0.1286]
Loss:[0.1240]
Loss:[0.1196]
Loss:[0.1206]
Loss:[0.1188]
Loss:[0.1196]
Loss:[0.1206]
Loss:[0.1228]
Loss:[0.1253]
Loss:[0.1119]
Loss:[0.1186]
Loss:[0.1291]
Loss:[0.1174]
Loss:[0.1302]
Loss:[0.1251]
Loss:[0.1160]
Loss:[0.1381]
Loss:[0.1190]
Loss:[0.1173]
Loss:[0.1195]
Loss:[0.1142]
Loss:[0.1106]
Loss:[0.1056]
Loss:[0.1087]
Loss:[0.1122]
Loss:[0.1096]
Loss:[0.1191]
Loss:[0.1094]
Loss:[0.1163]
Loss:[0.1053]
Loss:[0.1092]
Loss:[0.1029]
Loss:[0.1070]
Loss:[0.1052]
Loss:[0.1024]
Loss:[0.1087]
Loss:[0.1063]
Loss:[0.1042]
Loss:[0.1031]
Loss:[0.1003]
Loss:[0.1043]
Loss:[0.1062]
Loss:[0.0992]
Loss:[0.1044]
Loss:[0.1075]
Loss:[0.1078]
Loss:[0.1020]
Loss:[0.1081]
Loss:[0.1046]
Loss:[0.1089]
Loss:[0.0995]
Loss:[0.1071]
Loss:[0.1037]
Loss:[0.1034]
Loss:[0.0997]
Loss:[0.0998]
Loss:[0.0929]
Loss:[0.1066]
Loss:[0.1039]
Loss:[0.1009]
Loss:[0.1012]
Loss:[0.0937]
Loss:[0.1003]
Loss:[0.0970]
Loss:[0.0975]
Loss:[0.1000]
Loss:[0.0928]
Loss:[0.0999]
Loss:[0.1004]
Loss:[0.0995]
Loss:[0.0944]
Loss:[0.1005]
Loss:[0.0950]
Loss:[0.0986]
Loss:[0.0933]
Loss:[0.0945]
Loss:[0.0996]
Loss:[0.1010]
Loss:[0.0990]
Loss:[0.1033]
Loss:[0.0977]
Loss:[0.0951]
Loss:[0.0955]
Loss:[0.0939]
Loss:[0.0962]
Loss:[0.0999]
Loss:[0.0954]
Early stopping!
Loading 169th epoch
acc:[0.7240]
acc:[0.7260]
acc:[0.7250]
acc:[0.7260]
acc:[0.7250]
acc:[0.7220]
acc:[0.7260]
acc:[0.7250]
acc:[0.7250]
acc:[0.7230]
acc:[0.7270]
acc:[0.7260]
acc:[0.7260]
acc:[0.7280]
acc:[0.7260]
acc:[0.7280]
acc:[0.7240]
acc:[0.7250]
acc:[0.7260]
acc:[0.7230]
acc:[0.7270]
acc:[0.7280]
acc:[0.7230]
acc:[0.7240]
acc:[0.7280]
acc:[0.7240]
acc:[0.7250]
acc:[0.7270]
acc:[0.7240]
acc:[0.7230]
acc:[0.7230]
acc:[0.7240]
acc:[0.7270]
acc:[0.7270]
acc:[0.7250]
acc:[0.7270]
acc:[0.7250]
acc:[0.7230]
acc:[0.7270]
acc:[0.7270]
acc:[0.7260]
acc:[0.7270]
acc:[0.7240]
acc:[0.7250]
acc:[0.7270]
acc:[0.7260]
acc:[0.7260]
acc:[0.7270]
acc:[0.7250]
acc:[0.7270]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7255]
Mean:[72.5480]
Std :[0.1581]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='citeseer', drop_percent=0.4, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6921]
Loss:[0.6898]
Loss:[0.6872]
Loss:[0.6832]
Loss:[0.6791]
Loss:[0.6737]
Loss:[0.6672]
Loss:[0.6607]
Loss:[0.6515]
Loss:[0.6435]
Loss:[0.6344]
Loss:[0.6221]
Loss:[0.6115]
Loss:[0.6006]
Loss:[0.5871]
Loss:[0.5748]
Loss:[0.5617]
Loss:[0.5476]
Loss:[0.5318]
Loss:[0.5163]
Loss:[0.5046]
Loss:[0.4896]
Loss:[0.4754]
Loss:[0.4574]
Loss:[0.4446]
Loss:[0.4349]
Loss:[0.4202]
Loss:[0.4055]
Loss:[0.3939]
Loss:[0.3777]
Loss:[0.3707]
Loss:[0.3623]
Loss:[0.3494]
Loss:[0.3322]
Loss:[0.3250]
Loss:[0.3223]
Loss:[0.3168]
Loss:[0.3050]
Loss:[0.2880]
Loss:[0.2768]
Loss:[0.2799]
Loss:[0.2745]
Loss:[0.2596]
Loss:[0.2537]
Loss:[0.2466]
Loss:[0.2427]
Loss:[0.2340]
Loss:[0.2298]
Loss:[0.2316]
Loss:[0.2247]
Loss:[0.2214]
Loss:[0.2133]
Loss:[0.2132]
Loss:[0.2105]
Loss:[0.2009]
Loss:[0.1927]
Loss:[0.1982]
Loss:[0.1972]
Loss:[0.1830]
Loss:[0.1848]
Loss:[0.1858]
Loss:[0.1838]
Loss:[0.1797]
Loss:[0.1763]
Loss:[0.1742]
Loss:[0.1693]
Loss:[0.1725]
Loss:[0.1680]
Loss:[0.1690]
Loss:[0.1634]
Loss:[0.1667]
Loss:[0.1572]
Loss:[0.1563]
Loss:[0.1628]
Loss:[0.1544]
Loss:[0.1539]
Loss:[0.1502]
Loss:[0.1586]
Loss:[0.1480]
Loss:[0.1529]
Loss:[0.1515]
Loss:[0.1459]
Loss:[0.1423]
Loss:[0.1462]
Loss:[0.1494]
Loss:[0.1481]
Loss:[0.1370]
Loss:[0.1430]
Loss:[0.1395]
Loss:[0.1323]
Loss:[0.1432]
Loss:[0.1347]
Loss:[0.1319]
Loss:[0.1373]
Loss:[0.1388]
Loss:[0.1352]
Loss:[0.1325]
Loss:[0.1339]
Loss:[0.1346]
Loss:[0.1290]
Loss:[0.1204]
Loss:[0.1305]
Loss:[0.1306]
Loss:[0.1298]
Loss:[0.1272]
Loss:[0.1261]
Loss:[0.1283]
Loss:[0.1296]
Loss:[0.1271]
Loss:[0.1342]
Loss:[0.1339]
Loss:[0.1191]
Loss:[0.1300]
Loss:[0.1377]
Loss:[0.1272]
Loss:[0.1427]
Loss:[0.1211]
Loss:[0.1286]
Loss:[0.1266]
Loss:[0.1200]
Loss:[0.1258]
Loss:[0.1113]
Loss:[0.1235]
Loss:[0.1101]
Loss:[0.1151]
Loss:[0.1094]
Loss:[0.1211]
Loss:[0.1136]
Loss:[0.1232]
Loss:[0.1165]
Loss:[0.1192]
Loss:[0.1149]
Loss:[0.1122]
Loss:[0.1093]
Loss:[0.1113]
Loss:[0.1128]
Loss:[0.1069]
Loss:[0.1119]
Loss:[0.1133]
Loss:[0.1107]
Loss:[0.1068]
Loss:[0.1058]
Loss:[0.1086]
Loss:[0.1101]
Loss:[0.1054]
Loss:[0.1072]
Loss:[0.1113]
Loss:[0.1164]
Loss:[0.1076]
Loss:[0.1141]
Loss:[0.1059]
Loss:[0.1129]
Loss:[0.1058]
Loss:[0.1082]
Loss:[0.1062]
Loss:[0.1074]
Loss:[0.1034]
Loss:[0.1016]
Loss:[0.0960]
Loss:[0.1066]
Loss:[0.1078]
Loss:[0.1045]
Loss:[0.1073]
Loss:[0.0989]
Loss:[0.1025]
Loss:[0.1009]
Loss:[0.1021]
Loss:[0.1036]
Loss:[0.0979]
Loss:[0.1024]
Loss:[0.1048]
Loss:[0.1020]
Loss:[0.0963]
Loss:[0.1056]
Loss:[0.0973]
Loss:[0.1011]
Loss:[0.1001]
Loss:[0.0986]
Loss:[0.1011]
Early stopping!
Loading 159th epoch
acc:[0.7240]
acc:[0.7250]
acc:[0.7260]
acc:[0.7250]
acc:[0.7230]
acc:[0.7220]
acc:[0.7240]
acc:[0.7230]
acc:[0.7240]
acc:[0.7240]
acc:[0.7250]
acc:[0.7250]
acc:[0.7240]
acc:[0.7250]
acc:[0.7230]
acc:[0.7250]
acc:[0.7230]
acc:[0.7230]
acc:[0.7250]
acc:[0.7230]
acc:[0.7250]
acc:[0.7250]
acc:[0.7210]
acc:[0.7250]
acc:[0.7230]
acc:[0.7220]
acc:[0.7240]
acc:[0.7240]
acc:[0.7250]
acc:[0.7250]
acc:[0.7240]
acc:[0.7230]
acc:[0.7210]
acc:[0.7230]
acc:[0.7240]
acc:[0.7230]
acc:[0.7230]
acc:[0.7230]
acc:[0.7250]
acc:[0.7250]
acc:[0.7250]
acc:[0.7240]
acc:[0.7220]
acc:[0.7230]
acc:[0.7250]
acc:[0.7240]
acc:[0.7250]
acc:[0.7250]
acc:[0.7220]
acc:[0.7270]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7239]
Mean:[72.3920]
Std :[0.1259]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='citeseer', drop_percent=0.5, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6923]
Loss:[0.6901]
Loss:[0.6878]
Loss:[0.6838]
Loss:[0.6807]
Loss:[0.6749]
Loss:[0.6699]
Loss:[0.6622]
Loss:[0.6551]
Loss:[0.6468]
Loss:[0.6371]
Loss:[0.6282]
Loss:[0.6164]
Loss:[0.6045]
Loss:[0.5952]
Loss:[0.5833]
Loss:[0.5672]
Loss:[0.5546]
Loss:[0.5431]
Loss:[0.5260]
Loss:[0.5131]
Loss:[0.4991]
Loss:[0.4874]
Loss:[0.4690]
Loss:[0.4559]
Loss:[0.4477]
Loss:[0.4334]
Loss:[0.4172]
Loss:[0.4066]
Loss:[0.3964]
Loss:[0.3887]
Loss:[0.3722]
Loss:[0.3619]
Loss:[0.3529]
Loss:[0.3418]
Loss:[0.3344]
Loss:[0.3318]
Loss:[0.3167]
Loss:[0.3009]
Loss:[0.2944]
Loss:[0.2947]
Loss:[0.2834]
Loss:[0.2738]
Loss:[0.2720]
Loss:[0.2587]
Loss:[0.2555]
Loss:[0.2481]
Loss:[0.2474]
Loss:[0.2412]
Loss:[0.2366]
Loss:[0.2372]
Loss:[0.2280]
Loss:[0.2223]
Loss:[0.2157]
Loss:[0.2148]
Loss:[0.2043]
Loss:[0.2104]
Loss:[0.2105]
Loss:[0.1985]
Loss:[0.1949]
Loss:[0.1971]
Loss:[0.2005]
Loss:[0.1939]
Loss:[0.1846]
Loss:[0.1871]
Loss:[0.1832]
Loss:[0.1816]
Loss:[0.1790]
Loss:[0.1798]
Loss:[0.1761]
Loss:[0.1723]
Loss:[0.1675]
Loss:[0.1686]
Loss:[0.1690]
Loss:[0.1629]
Loss:[0.1616]
Loss:[0.1601]
Loss:[0.1677]
Loss:[0.1551]
Loss:[0.1521]
Loss:[0.1525]
Loss:[0.1545]
Loss:[0.1496]
Loss:[0.1506]
Loss:[0.1509]
Loss:[0.1545]
Loss:[0.1439]
Loss:[0.1409]
Loss:[0.1437]
Loss:[0.1431]
Loss:[0.1435]
Loss:[0.1395]
Loss:[0.1429]
Loss:[0.1420]
Loss:[0.1424]
Loss:[0.1431]
Loss:[0.1392]
Loss:[0.1369]
Loss:[0.1380]
Loss:[0.1364]
Loss:[0.1252]
Loss:[0.1374]
Loss:[0.1375]
Loss:[0.1350]
Loss:[0.1327]
Loss:[0.1304]
Loss:[0.1304]
Loss:[0.1324]
Loss:[0.1322]
Loss:[0.1284]
Loss:[0.1303]
Loss:[0.1246]
Loss:[0.1177]
Loss:[0.1358]
Loss:[0.1288]
Loss:[0.1310]
Loss:[0.1264]
Loss:[0.1222]
Loss:[0.1270]
Loss:[0.1231]
Loss:[0.1267]
Loss:[0.1171]
Loss:[0.1217]
Loss:[0.1164]
Loss:[0.1137]
Loss:[0.1156]
Loss:[0.1195]
Loss:[0.1191]
Loss:[0.1217]
Loss:[0.1189]
Loss:[0.1228]
Loss:[0.1179]
Loss:[0.1152]
Loss:[0.1091]
Loss:[0.1154]
Loss:[0.1177]
Loss:[0.1123]
Loss:[0.1158]
Loss:[0.1163]
Loss:[0.1156]
Loss:[0.1137]
Loss:[0.1121]
Loss:[0.1143]
Loss:[0.1164]
Loss:[0.1120]
Loss:[0.1156]
Loss:[0.1195]
Loss:[0.1225]
Loss:[0.1106]
Loss:[0.1168]
Loss:[0.1126]
Loss:[0.1158]
Loss:[0.1111]
Loss:[0.1139]
Early stopping!
Loading 134th epoch
acc:[0.7220]
acc:[0.7210]
acc:[0.7230]
acc:[0.7220]
acc:[0.7220]
acc:[0.7200]
acc:[0.7200]
acc:[0.7250]
acc:[0.7220]
acc:[0.7200]
acc:[0.7220]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7210]
acc:[0.7230]
acc:[0.7210]
acc:[0.7200]
acc:[0.7220]
acc:[0.7200]
acc:[0.7230]
acc:[0.7230]
acc:[0.7220]
acc:[0.7220]
acc:[0.7220]
acc:[0.7220]
acc:[0.7230]
acc:[0.7210]
acc:[0.7240]
acc:[0.7220]
acc:[0.7200]
acc:[0.7210]
acc:[0.7210]
acc:[0.7220]
acc:[0.7190]
acc:[0.7200]
acc:[0.7220]
acc:[0.7230]
acc:[0.7190]
acc:[0.7210]
acc:[0.7220]
acc:[0.7230]
acc:[0.7240]
acc:[0.7230]
acc:[0.7230]
acc:[0.7220]
acc:[0.7220]
acc:[0.7230]
acc:[0.7210]
acc:[0.7230]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7218]
Mean:[72.1780]
Std :[0.1282]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='edge', dataset='citeseer', drop_percent=0.6, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6923]
Loss:[0.6902]
Loss:[0.6879]
Loss:[0.6841]
Loss:[0.6809]
Loss:[0.6754]
Loss:[0.6702]
Loss:[0.6633]
Loss:[0.6554]
Loss:[0.6479]
Loss:[0.6385]
Loss:[0.6281]
Loss:[0.6183]
Loss:[0.6068]
Loss:[0.5946]
Loss:[0.5841]
Loss:[0.5716]
Loss:[0.5568]
Loss:[0.5420]
Loss:[0.5278]
Loss:[0.5178]
Loss:[0.5028]
Loss:[0.4879]
Loss:[0.4715]
Loss:[0.4604]
Loss:[0.4521]
Loss:[0.4349]
Loss:[0.4202]
Loss:[0.4102]
Loss:[0.3967]
Loss:[0.3887]
Loss:[0.3812]
Loss:[0.3666]
Loss:[0.3498]
Loss:[0.3427]
Loss:[0.3395]
Loss:[0.3351]
Loss:[0.3223]
Loss:[0.3070]
Loss:[0.2936]
Loss:[0.2961]
Loss:[0.2935]
Loss:[0.2766]
Loss:[0.2739]
Loss:[0.2620]
Loss:[0.2581]
Loss:[0.2500]
Loss:[0.2468]
Loss:[0.2435]
Loss:[0.2399]
Loss:[0.2353]
Loss:[0.2282]
Loss:[0.2266]
Loss:[0.2189]
Loss:[0.2145]
Loss:[0.2079]
Loss:[0.2161]
Loss:[0.2170]
Loss:[0.2128]
Loss:[0.2026]
Loss:[0.1944]
Loss:[0.2064]
Loss:[0.2058]
Loss:[0.1892]
Loss:[0.1910]
Loss:[0.1910]
Loss:[0.1872]
Loss:[0.1814]
Loss:[0.1884]
Loss:[0.1811]
Loss:[0.1762]
Loss:[0.1781]
Loss:[0.1759]
Loss:[0.1710]
Loss:[0.1770]
Loss:[0.1626]
Loss:[0.1654]
Loss:[0.1751]
Loss:[0.1569]
Loss:[0.1634]
Loss:[0.1598]
Loss:[0.1576]
Loss:[0.1586]
Loss:[0.1553]
Loss:[0.1549]
Loss:[0.1582]
Loss:[0.1463]
Loss:[0.1461]
Loss:[0.1458]
Loss:[0.1467]
Loss:[0.1476]
Loss:[0.1405]
Loss:[0.1444]
Loss:[0.1466]
Loss:[0.1454]
Loss:[0.1453]
Loss:[0.1400]
Loss:[0.1408]
Loss:[0.1422]
Loss:[0.1373]
Loss:[0.1289]
Loss:[0.1411]
Loss:[0.1382]
Loss:[0.1375]
Loss:[0.1345]
Loss:[0.1347]
Loss:[0.1314]
Loss:[0.1364]
Loss:[0.1351]
Loss:[0.1301]
Loss:[0.1380]
Loss:[0.1286]
Loss:[0.1227]
Loss:[0.1394]
Loss:[0.1340]
Loss:[0.1340]
Loss:[0.1310]
Loss:[0.1242]
Loss:[0.1302]
Loss:[0.1252]
Loss:[0.1282]
Loss:[0.1212]
Loss:[0.1253]
Loss:[0.1181]
Loss:[0.1189]
Loss:[0.1228]
Loss:[0.1223]
Loss:[0.1233]
Loss:[0.1251]
Loss:[0.1247]
Loss:[0.1268]
Loss:[0.1190]
Loss:[0.1177]
Loss:[0.1141]
Loss:[0.1184]
Loss:[0.1194]
Loss:[0.1170]
Loss:[0.1202]
Loss:[0.1172]
Loss:[0.1142]
Loss:[0.1170]
Loss:[0.1143]
Loss:[0.1151]
Loss:[0.1191]
Loss:[0.1175]
Loss:[0.1241]
Loss:[0.1203]
Loss:[0.1181]
Loss:[0.1174]
Loss:[0.1178]
Loss:[0.1122]
Loss:[0.1172]
Loss:[0.1121]
Loss:[0.1153]
Loss:[0.1142]
Loss:[0.1142]
Loss:[0.1092]
Loss:[0.1113]
Loss:[0.1078]
Loss:[0.1141]
Loss:[0.1142]
Loss:[0.1120]
Loss:[0.1141]
Loss:[0.1076]
Loss:[0.1109]
Loss:[0.1122]
Loss:[0.1086]
Loss:[0.1069]
Loss:[0.1048]
Loss:[0.1126]
Loss:[0.1103]
Loss:[0.1098]
Loss:[0.1033]
Loss:[0.1090]
Loss:[0.1008]
Loss:[0.1075]
Loss:[0.1057]
Loss:[0.1073]
Loss:[0.1079]
Loss:[0.1132]
Loss:[0.1079]
Loss:[0.1087]
Loss:[0.1052]
Loss:[0.1089]
Loss:[0.1054]
Loss:[0.1060]
Loss:[0.1069]
Loss:[0.1093]
Loss:[0.1048]
Loss:[0.1012]
Loss:[0.1063]
Loss:[0.1107]
Loss:[0.1087]
Loss:[0.0990]
Loss:[0.1031]
Loss:[0.0999]
Loss:[0.1024]
Loss:[0.1080]
Loss:[0.1069]
Loss:[0.1014]
Loss:[0.1017]
Loss:[0.1010]
Loss:[0.0989]
Loss:[0.1028]
Loss:[0.0947]
Loss:[0.1013]
Loss:[0.0983]
Loss:[0.1042]
Loss:[0.1088]
Loss:[0.1006]
Loss:[0.1046]
Loss:[0.0984]
Loss:[0.1084]
Loss:[0.1031]
Loss:[0.0961]
Loss:[0.1040]
Loss:[0.1031]
Loss:[0.1076]
Loss:[0.1025]
Loss:[0.1074]
Loss:[0.1027]
Loss:[0.0942]
Loss:[0.1025]
Loss:[0.0912]
Loss:[0.1027]
Loss:[0.1032]
Loss:[0.1049]
Loss:[0.1083]
Loss:[0.1094]
Loss:[0.1014]
Loss:[0.1027]
Loss:[0.1001]
Loss:[0.0933]
Loss:[0.0986]
Loss:[0.1036]
Loss:[0.1025]
Loss:[0.0936]
Loss:[0.1008]
Loss:[0.0940]
Loss:[0.1044]
Loss:[0.1017]
Loss:[0.0993]
Loss:[0.1000]
Loss:[0.1007]
Early stopping!
Loading 224th epoch
acc:[0.7090]
acc:[0.7110]
acc:[0.7070]
acc:[0.7090]
acc:[0.7110]
acc:[0.7060]
acc:[0.7060]
acc:[0.7070]
acc:[0.7100]
acc:[0.7100]
acc:[0.7070]
acc:[0.7070]
acc:[0.7090]
acc:[0.7080]
acc:[0.7040]
acc:[0.7080]
acc:[0.7070]
acc:[0.7110]
acc:[0.7080]
acc:[0.7070]
acc:[0.7060]
acc:[0.7060]
acc:[0.7110]
acc:[0.7110]
acc:[0.7100]
acc:[0.7100]
acc:[0.7070]
acc:[0.7110]
acc:[0.7090]
acc:[0.7100]
acc:[0.7130]
acc:[0.7080]
acc:[0.7070]
acc:[0.7090]
acc:[0.7100]
acc:[0.7060]
acc:[0.7050]
acc:[0.7110]
acc:[0.7090]
acc:[0.7060]
acc:[0.7060]
acc:[0.7070]
acc:[0.7090]
acc:[0.7100]
acc:[0.7080]
acc:[0.7070]
acc:[0.7110]
acc:[0.7130]
acc:[0.7090]
acc:[0.7090]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7085]
Mean:[70.8520]
Std :[0.2063]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='citeseer', drop_percent=0.02, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6920]
Loss:[0.6893]
Loss:[0.6857]
Loss:[0.6812]
Loss:[0.6758]
Loss:[0.6700]
Loss:[0.6618]
Loss:[0.6548]
Loss:[0.6442]
Loss:[0.6347]
Loss:[0.6247]
Loss:[0.6112]
Loss:[0.5993]
Loss:[0.5860]
Loss:[0.5714]
Loss:[0.5585]
Loss:[0.5429]
Loss:[0.5248]
Loss:[0.5086]
Loss:[0.4928]
Loss:[0.4792]
Loss:[0.4633]
Loss:[0.4458]
Loss:[0.4256]
Loss:[0.4148]
Loss:[0.4032]
Loss:[0.3845]
Loss:[0.3684]
Loss:[0.3591]
Loss:[0.3448]
Loss:[0.3327]
Loss:[0.3224]
Loss:[0.3073]
Loss:[0.2916]
Loss:[0.2893]
Loss:[0.2857]
Loss:[0.2705]
Loss:[0.2581]
Loss:[0.2474]
Loss:[0.2422]
Loss:[0.2388]
Loss:[0.2317]
Loss:[0.2169]
Loss:[0.2160]
Loss:[0.2062]
Loss:[0.2013]
Loss:[0.1936]
Loss:[0.1925]
Loss:[0.1891]
Loss:[0.1879]
Loss:[0.1857]
Loss:[0.1742]
Loss:[0.1748]
Loss:[0.1710]
Loss:[0.1649]
Loss:[0.1598]
Loss:[0.1676]
Loss:[0.1646]
Loss:[0.1489]
Loss:[0.1534]
Loss:[0.1571]
Loss:[0.1540]
Loss:[0.1462]
Loss:[0.1498]
Loss:[0.1524]
Loss:[0.1408]
Loss:[0.1520]
Loss:[0.1493]
Loss:[0.1348]
Loss:[0.1442]
Loss:[0.1365]
Loss:[0.1304]
Loss:[0.1318]
Loss:[0.1352]
Loss:[0.1328]
Loss:[0.1245]
Loss:[0.1281]
Loss:[0.1321]
Loss:[0.1181]
Loss:[0.1211]
Loss:[0.1231]
Loss:[0.1220]
Loss:[0.1184]
Loss:[0.1174]
Loss:[0.1190]
Loss:[0.1216]
Loss:[0.1127]
Loss:[0.1162]
Loss:[0.1130]
Loss:[0.1096]
Loss:[0.1148]
Loss:[0.1094]
Loss:[0.1127]
Loss:[0.1140]
Loss:[0.1173]
Loss:[0.1107]
Loss:[0.1114]
Loss:[0.1122]
Loss:[0.1118]
Loss:[0.1081]
Loss:[0.1017]
Loss:[0.1090]
Loss:[0.1086]
Loss:[0.1086]
Loss:[0.1075]
Loss:[0.1050]
Loss:[0.0973]
Loss:[0.1052]
Loss:[0.1049]
Loss:[0.1047]
Loss:[0.1076]
Loss:[0.1000]
Loss:[0.0979]
Loss:[0.1091]
Loss:[0.1044]
Loss:[0.1053]
Loss:[0.1020]
Loss:[0.0988]
Loss:[0.1053]
Loss:[0.1006]
Loss:[0.1015]
Loss:[0.0959]
Loss:[0.1002]
Loss:[0.0908]
Loss:[0.0953]
Loss:[0.0934]
Loss:[0.1003]
Loss:[0.0977]
Loss:[0.1005]
Loss:[0.1027]
Loss:[0.1036]
Loss:[0.0957]
Loss:[0.0993]
Loss:[0.0934]
Loss:[0.0965]
Loss:[0.0941]
Loss:[0.0950]
Loss:[0.0969]
Loss:[0.0977]
Loss:[0.0926]
Loss:[0.0930]
Loss:[0.0900]
Loss:[0.0921]
Loss:[0.0946]
Loss:[0.0871]
Loss:[0.0961]
Loss:[0.0973]
Loss:[0.0969]
Loss:[0.0921]
Loss:[0.0978]
Loss:[0.0961]
Loss:[0.0938]
Loss:[0.0906]
Loss:[0.0956]
Loss:[0.0921]
Loss:[0.0886]
Loss:[0.0901]
Loss:[0.0898]
Loss:[0.0841]
Loss:[0.0966]
Loss:[0.0913]
Loss:[0.0941]
Loss:[0.0887]
Loss:[0.0849]
Loss:[0.0865]
Loss:[0.0869]
Loss:[0.0860]
Loss:[0.0900]
Loss:[0.0820]
Loss:[0.0902]
Loss:[0.0882]
Loss:[0.0857]
Loss:[0.0810]
Loss:[0.0889]
Loss:[0.0837]
Loss:[0.0861]
Loss:[0.0796]
Loss:[0.0874]
Loss:[0.0891]
Loss:[0.0902]
Loss:[0.0894]
Loss:[0.0922]
Loss:[0.0868]
Loss:[0.0839]
Loss:[0.0878]
Loss:[0.0813]
Loss:[0.0865]
Loss:[0.0889]
Loss:[0.0887]
Loss:[0.0830]
Loss:[0.0848]
Loss:[0.0927]
Loss:[0.0907]
Loss:[0.0805]
Loss:[0.0838]
Loss:[0.0795]
Loss:[0.0814]
Loss:[0.0898]
Loss:[0.0838]
Loss:[0.0862]
Loss:[0.0880]
Loss:[0.0833]
Loss:[0.0808]
Loss:[0.0806]
Loss:[0.0812]
Loss:[0.0779]
Loss:[0.0794]
Loss:[0.0829]
Loss:[0.0862]
Loss:[0.0859]
Loss:[0.0876]
Loss:[0.0812]
Loss:[0.0900]
Loss:[0.0859]
Loss:[0.0804]
Loss:[0.0887]
Loss:[0.0784]
Loss:[0.0857]
Loss:[0.0858]
Loss:[0.0819]
Loss:[0.0793]
Loss:[0.0764]
Loss:[0.0798]
Loss:[0.0772]
Loss:[0.0801]
Loss:[0.0779]
Loss:[0.0854]
Loss:[0.0861]
Loss:[0.0857]
Loss:[0.0814]
Loss:[0.0834]
Loss:[0.0830]
Loss:[0.0734]
Loss:[0.0780]
Loss:[0.0771]
Loss:[0.0870]
Loss:[0.0763]
Loss:[0.0824]
Loss:[0.0843]
Loss:[0.0848]
Loss:[0.0827]
Loss:[0.0835]
Loss:[0.0830]
Loss:[0.0831]
Loss:[0.0880]
Loss:[0.0779]
Loss:[0.0777]
Loss:[0.0821]
Loss:[0.0821]
Loss:[0.0827]
Loss:[0.0786]
Loss:[0.0830]
Loss:[0.0797]
Early stopping!
Loading 233th epoch
acc:[0.7220]
acc:[0.7250]
acc:[0.7210]
acc:[0.7230]
acc:[0.7230]
acc:[0.7230]
acc:[0.7230]
acc:[0.7200]
acc:[0.7240]
acc:[0.7260]
acc:[0.7220]
acc:[0.7240]
acc:[0.7230]
acc:[0.7220]
acc:[0.7250]
acc:[0.7220]
acc:[0.7210]
acc:[0.7240]
acc:[0.7200]
acc:[0.7200]
acc:[0.7220]
acc:[0.7200]
acc:[0.7200]
acc:[0.7230]
acc:[0.7250]
acc:[0.7270]
acc:[0.7230]
acc:[0.7240]
acc:[0.7250]
acc:[0.7220]
acc:[0.7240]
acc:[0.7220]
acc:[0.7210]
acc:[0.7230]
acc:[0.7230]
acc:[0.7230]
acc:[0.7170]
acc:[0.7220]
acc:[0.7230]
acc:[0.7240]
acc:[0.7190]
acc:[0.7190]
acc:[0.7220]
acc:[0.7240]
acc:[0.7200]
acc:[0.7250]
acc:[0.7250]
acc:[0.7210]
acc:[0.7240]
acc:[0.7220]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7225]
Mean:[72.2540]
Std :[0.1992]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='citeseer', drop_percent=0.05, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6917]
Loss:[0.6897]
Loss:[0.6850]
Loss:[0.6815]
Loss:[0.6741]
Loss:[0.6693]
Loss:[0.6599]
Loss:[0.6534]
Loss:[0.6415]
Loss:[0.6332]
Loss:[0.6203]
Loss:[0.6098]
Loss:[0.5971]
Loss:[0.5829]
Loss:[0.5727]
Loss:[0.5595]
Loss:[0.5421]
Loss:[0.5316]
Loss:[0.5170]
Loss:[0.4979]
Loss:[0.4869]
Loss:[0.4693]
Loss:[0.4540]
Loss:[0.4391]
Loss:[0.4291]
Loss:[0.4123]
Loss:[0.3941]
Loss:[0.3834]
Loss:[0.3753]
Loss:[0.3584]
Loss:[0.3443]
Loss:[0.3350]
Loss:[0.3229]
Loss:[0.3110]
Loss:[0.3008]
Loss:[0.2921]
Loss:[0.2872]
Loss:[0.2763]
Loss:[0.2642]
Loss:[0.2515]
Loss:[0.2489]
Loss:[0.2429]
Loss:[0.2297]
Loss:[0.2274]
Loss:[0.2173]
Loss:[0.2119]
Loss:[0.2040]
Loss:[0.2022]
Loss:[0.2005]
Loss:[0.1954]
Loss:[0.1921]
Loss:[0.1814]
Loss:[0.1795]
Loss:[0.1777]
Loss:[0.1671]
Loss:[0.1634]
Loss:[0.1740]
Loss:[0.1696]
Loss:[0.1535]
Loss:[0.1536]
Loss:[0.1549]
Loss:[0.1613]
Loss:[0.1508]
Loss:[0.1451]
Loss:[0.1505]
Loss:[0.1466]
Loss:[0.1442]
Loss:[0.1406]
Loss:[0.1407]
Loss:[0.1357]
Loss:[0.1345]
Loss:[0.1290]
Loss:[0.1283]
Loss:[0.1325]
Loss:[0.1313]
Loss:[0.1254]
Loss:[0.1243]
Loss:[0.1281]
Loss:[0.1218]
Loss:[0.1187]
Loss:[0.1203]
Loss:[0.1205]
Loss:[0.1171]
Loss:[0.1173]
Loss:[0.1189]
Loss:[0.1185]
Loss:[0.1104]
Loss:[0.1156]
Loss:[0.1131]
Loss:[0.1052]
Loss:[0.1143]
Loss:[0.1078]
Loss:[0.1100]
Loss:[0.1110]
Loss:[0.1116]
Loss:[0.1100]
Loss:[0.1056]
Loss:[0.1086]
Loss:[0.1092]
Loss:[0.1037]
Loss:[0.0970]
Loss:[0.1050]
Loss:[0.1051]
Loss:[0.1065]
Loss:[0.1039]
Loss:[0.1021]
Loss:[0.0954]
Loss:[0.1012]
Loss:[0.1026]
Loss:[0.1012]
Loss:[0.1037]
Loss:[0.0976]
Loss:[0.0906]
Loss:[0.1060]
Loss:[0.1014]
Loss:[0.1016]
Loss:[0.0995]
Loss:[0.0957]
Loss:[0.1024]
Loss:[0.0926]
Loss:[0.0963]
Loss:[0.0911]
Loss:[0.0985]
Loss:[0.0914]
Loss:[0.0908]
Loss:[0.0902]
Loss:[0.0970]
Loss:[0.0935]
Loss:[0.1003]
Loss:[0.0954]
Loss:[0.1011]
Loss:[0.0904]
Loss:[0.0942]
Loss:[0.0865]
Loss:[0.0906]
Loss:[0.0912]
Loss:[0.0813]
Loss:[0.0926]
Loss:[0.0968]
Loss:[0.0907]
Loss:[0.0909]
Loss:[0.0883]
Loss:[0.0920]
Loss:[0.0898]
Loss:[0.0977]
Loss:[0.1074]
Loss:[0.0973]
Loss:[0.1215]
Loss:[0.1124]
Loss:[0.1082]
Loss:[0.1105]
Loss:[0.0904]
Loss:[0.0948]
Loss:[0.0907]
Loss:[0.0988]
Loss:[0.0822]
Loss:[0.0954]
Early stopping!
Loading 137th epoch
acc:[0.7170]
acc:[0.7170]
acc:[0.7180]
acc:[0.7150]
acc:[0.7220]
acc:[0.7180]
acc:[0.7150]
acc:[0.7160]
acc:[0.7160]
acc:[0.7170]
acc:[0.7180]
acc:[0.7180]
acc:[0.7150]
acc:[0.7210]
acc:[0.7180]
acc:[0.7140]
acc:[0.7190]
acc:[0.7170]
acc:[0.7160]
acc:[0.7160]
acc:[0.7170]
acc:[0.7190]
acc:[0.7190]
acc:[0.7200]
acc:[0.7170]
acc:[0.7170]
acc:[0.7210]
acc:[0.7190]
acc:[0.7190]
acc:[0.7150]
acc:[0.7210]
acc:[0.7160]
acc:[0.7190]
acc:[0.7150]
acc:[0.7190]
acc:[0.7200]
acc:[0.7130]
acc:[0.7170]
acc:[0.7180]
acc:[0.7160]
acc:[0.7160]
acc:[0.7170]
acc:[0.7190]
acc:[0.7190]
acc:[0.7200]
acc:[0.7180]
acc:[0.7170]
acc:[0.7200]
acc:[0.7180]
acc:[0.7190]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7177]
Mean:[71.7660]
Std :[0.1955]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='citeseer', drop_percent=0.08, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6913]
Loss:[0.6879]
Loss:[0.6823]
Loss:[0.6760]
Loss:[0.6685]
Loss:[0.6588]
Loss:[0.6513]
Loss:[0.6393]
Loss:[0.6288]
Loss:[0.6206]
Loss:[0.6087]
Loss:[0.5975]
Loss:[0.5881]
Loss:[0.5768]
Loss:[0.5641]
Loss:[0.5564]
Loss:[0.5406]
Loss:[0.5260]
Loss:[0.5188]
Loss:[0.5024]
Loss:[0.4877]
Loss:[0.4716]
Loss:[0.4631]
Loss:[0.4465]
Loss:[0.4316]
Loss:[0.4206]
Loss:[0.4065]
Loss:[0.3935]
Loss:[0.3798]
Loss:[0.3659]
Loss:[0.3534]
Loss:[0.3458]
Loss:[0.3348]
Loss:[0.3253]
Loss:[0.3132]
Loss:[0.3018]
Loss:[0.2903]
Loss:[0.2864]
Loss:[0.2707]
Loss:[0.2619]
Loss:[0.2595]
Loss:[0.2550]
Loss:[0.2376]
Loss:[0.2337]
Loss:[0.2283]
Loss:[0.2213]
Loss:[0.2093]
Loss:[0.2119]
Loss:[0.2077]
Loss:[0.1980]
Loss:[0.1996]
Loss:[0.1943]
Loss:[0.1879]
Loss:[0.1819]
Loss:[0.1767]
Loss:[0.1715]
Loss:[0.1733]
Loss:[0.1747]
Loss:[0.1653]
Loss:[0.1609]
Loss:[0.1601]
Loss:[0.1654]
Loss:[0.1533]
Loss:[0.1570]
Loss:[0.1588]
Loss:[0.1437]
Loss:[0.1607]
Loss:[0.1502]
Loss:[0.1385]
Loss:[0.1483]
Loss:[0.1383]
Loss:[0.1334]
Loss:[0.1332]
Loss:[0.1335]
Loss:[0.1317]
Loss:[0.1242]
Loss:[0.1237]
Loss:[0.1302]
Loss:[0.1187]
Loss:[0.1171]
Loss:[0.1194]
Loss:[0.1199]
Loss:[0.1170]
Loss:[0.1144]
Loss:[0.1212]
Loss:[0.1180]
Loss:[0.1108]
Loss:[0.1148]
Loss:[0.1098]
Loss:[0.1075]
Loss:[0.1147]
Loss:[0.1061]
Loss:[0.1107]
Loss:[0.1105]
Loss:[0.1063]
Loss:[0.1058]
Loss:[0.1035]
Loss:[0.1072]
Loss:[0.1027]
Loss:[0.0988]
Loss:[0.1023]
Loss:[0.1052]
Loss:[0.1062]
Loss:[0.1024]
Loss:[0.1013]
Loss:[0.0998]
Loss:[0.0962]
Loss:[0.1013]
Loss:[0.0975]
Loss:[0.0976]
Loss:[0.1019]
Loss:[0.0950]
Loss:[0.0899]
Loss:[0.1037]
Loss:[0.0945]
Loss:[0.0989]
Loss:[0.0978]
Loss:[0.0903]
Loss:[0.0942]
Loss:[0.0898]
Loss:[0.0926]
Loss:[0.0839]
Loss:[0.0925]
Loss:[0.0861]
Loss:[0.0870]
Loss:[0.0885]
Loss:[0.0875]
Loss:[0.0899]
Loss:[0.0906]
Loss:[0.0920]
Loss:[0.0947]
Loss:[0.0826]
Loss:[0.0907]
Loss:[0.0811]
Loss:[0.0883]
Loss:[0.0806]
Loss:[0.0833]
Loss:[0.0852]
Loss:[0.0896]
Loss:[0.0821]
Loss:[0.0868]
Loss:[0.0766]
Loss:[0.0814]
Loss:[0.0858]
Loss:[0.0780]
Loss:[0.0798]
Loss:[0.0882]
Loss:[0.0822]
Loss:[0.0808]
Loss:[0.0866]
Loss:[0.0857]
Loss:[0.0844]
Loss:[0.0790]
Loss:[0.0856]
Loss:[0.0753]
Loss:[0.0771]
Loss:[0.0769]
Loss:[0.0779]
Loss:[0.0737]
Loss:[0.0857]
Loss:[0.0844]
Loss:[0.0748]
Loss:[0.0789]
Loss:[0.0773]
Loss:[0.0787]
Loss:[0.0750]
Loss:[0.0756]
Loss:[0.0764]
Loss:[0.0711]
Loss:[0.0800]
Loss:[0.0754]
Loss:[0.0770]
Loss:[0.0709]
Loss:[0.0771]
Loss:[0.0747]
Loss:[0.0713]
Loss:[0.0717]
Loss:[0.0738]
Loss:[0.0757]
Loss:[0.0763]
Loss:[0.0743]
Loss:[0.0770]
Loss:[0.0713]
Loss:[0.0721]
Loss:[0.0745]
Loss:[0.0687]
Loss:[0.0701]
Loss:[0.0703]
Loss:[0.0720]
Loss:[0.0693]
Loss:[0.0734]
Loss:[0.0742]
Loss:[0.0743]
Loss:[0.0684]
Loss:[0.0743]
Loss:[0.0668]
Loss:[0.0675]
Loss:[0.0693]
Loss:[0.0686]
Loss:[0.0698]
Loss:[0.0699]
Loss:[0.0713]
Loss:[0.0647]
Loss:[0.0652]
Loss:[0.0627]
Loss:[0.0658]
Loss:[0.0679]
Loss:[0.0683]
Loss:[0.0693]
Loss:[0.0639]
Loss:[0.0672]
Loss:[0.0678]
Loss:[0.0719]
Loss:[0.0677]
Loss:[0.0639]
Loss:[0.0681]
Loss:[0.0643]
Loss:[0.0671]
Loss:[0.0672]
Loss:[0.0634]
Loss:[0.0654]
Loss:[0.0582]
Loss:[0.0618]
Loss:[0.0598]
Loss:[0.0674]
Loss:[0.0605]
Loss:[0.0635]
Loss:[0.0655]
Loss:[0.0650]
Loss:[0.0629]
Loss:[0.0619]
Loss:[0.0584]
Loss:[0.0550]
Loss:[0.0589]
Loss:[0.0615]
Loss:[0.0662]
Loss:[0.0619]
Loss:[0.0630]
Loss:[0.0633]
Loss:[0.0593]
Loss:[0.0632]
Loss:[0.0654]
Loss:[0.0591]
Loss:[0.0626]
Loss:[0.0652]
Loss:[0.0563]
Loss:[0.0555]
Loss:[0.0611]
Loss:[0.0614]
Loss:[0.0560]
Loss:[0.0586]
Loss:[0.0609]
Loss:[0.0639]
Early stopping!
Loading 233th epoch
acc:[0.7010]
acc:[0.7030]
acc:[0.7070]
acc:[0.7040]
acc:[0.7070]
acc:[0.7040]
acc:[0.7070]
acc:[0.7030]
acc:[0.7060]
acc:[0.7050]
acc:[0.7050]
acc:[0.7070]
acc:[0.7070]
acc:[0.7040]
acc:[0.7020]
acc:[0.7040]
acc:[0.7060]
acc:[0.7090]
acc:[0.7040]
acc:[0.7020]
acc:[0.7070]
acc:[0.7050]
acc:[0.7040]
acc:[0.7030]
acc:[0.7030]
acc:[0.7050]
acc:[0.7040]
acc:[0.7060]
acc:[0.7080]
acc:[0.7060]
acc:[0.7070]
acc:[0.7080]
acc:[0.7020]
acc:[0.7060]
acc:[0.7060]
acc:[0.7090]
acc:[0.7010]
acc:[0.7030]
acc:[0.7080]
acc:[0.7050]
acc:[0.7050]
acc:[0.7060]
acc:[0.7040]
acc:[0.7070]
acc:[0.7050]
acc:[0.7060]
acc:[0.7050]
acc:[0.7030]
acc:[0.7050]
acc:[0.7050]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7051]
Mean:[70.5080]
Std :[0.1967]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='citeseer', drop_percent=0.1, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6911]
Loss:[0.6870]
Loss:[0.6807]
Loss:[0.6728]
Loss:[0.6643]
Loss:[0.6532]
Loss:[0.6446]
Loss:[0.6347]
Loss:[0.6222]
Loss:[0.6148]
Loss:[0.6059]
Loss:[0.5936]
Loss:[0.5863]
Loss:[0.5777]
Loss:[0.5652]
Loss:[0.5589]
Loss:[0.5430]
Loss:[0.5333]
Loss:[0.5248]
Loss:[0.5081]
Loss:[0.4998]
Loss:[0.4832]
Loss:[0.4714]
Loss:[0.4564]
Loss:[0.4446]
Loss:[0.4350]
Loss:[0.4185]
Loss:[0.4066]
Loss:[0.3926]
Loss:[0.3786]
Loss:[0.3684]
Loss:[0.3636]
Loss:[0.3578]
Loss:[0.3480]
Loss:[0.3213]
Loss:[0.3281]
Loss:[0.3169]
Loss:[0.2948]
Loss:[0.3006]
Loss:[0.2805]
Loss:[0.2756]
Loss:[0.2736]
Loss:[0.2510]
Loss:[0.2569]
Loss:[0.2444]
Loss:[0.2379]
Loss:[0.2311]
Loss:[0.2216]
Loss:[0.2262]
Loss:[0.2129]
Loss:[0.2133]
Loss:[0.2037]
Loss:[0.1960]
Loss:[0.1971]
Loss:[0.1863]
Loss:[0.1855]
Loss:[0.1859]
Loss:[0.1809]
Loss:[0.1756]
Loss:[0.1694]
Loss:[0.1620]
Loss:[0.1677]
Loss:[0.1608]
Loss:[0.1581]
Loss:[0.1557]
Loss:[0.1526]
Loss:[0.1525]
Loss:[0.1453]
Loss:[0.1426]
Loss:[0.1423]
Loss:[0.1382]
Loss:[0.1338]
Loss:[0.1311]
Loss:[0.1368]
Loss:[0.1347]
Loss:[0.1251]
Loss:[0.1249]
Loss:[0.1272]
Loss:[0.1205]
Loss:[0.1177]
Loss:[0.1184]
Loss:[0.1218]
Loss:[0.1172]
Loss:[0.1150]
Loss:[0.1184]
Loss:[0.1171]
Loss:[0.1116]
Loss:[0.1109]
Loss:[0.1086]
Loss:[0.1045]
Loss:[0.1085]
Loss:[0.1043]
Loss:[0.1062]
Loss:[0.1047]
Loss:[0.1049]
Loss:[0.1041]
Loss:[0.0984]
Loss:[0.0996]
Loss:[0.0973]
Loss:[0.0947]
Loss:[0.0954]
Loss:[0.0977]
Loss:[0.0991]
Loss:[0.0975]
Loss:[0.0958]
Loss:[0.0927]
Loss:[0.0897]
Loss:[0.0946]
Loss:[0.0885]
Loss:[0.0890]
Loss:[0.0925]
Loss:[0.0872]
Loss:[0.0851]
Loss:[0.0950]
Loss:[0.0850]
Loss:[0.0891]
Loss:[0.0865]
Loss:[0.0824]
Loss:[0.0857]
Loss:[0.0818]
Loss:[0.0856]
Loss:[0.0764]
Loss:[0.0848]
Loss:[0.0751]
Loss:[0.0792]
Loss:[0.0783]
Loss:[0.0839]
Loss:[0.0814]
Loss:[0.0773]
Loss:[0.0785]
Loss:[0.0818]
Loss:[0.0718]
Loss:[0.0797]
Loss:[0.0679]
Loss:[0.0756]
Loss:[0.0726]
Loss:[0.0703]
Loss:[0.0724]
Loss:[0.0767]
Loss:[0.0707]
Loss:[0.0758]
Loss:[0.0655]
Loss:[0.0658]
Loss:[0.0741]
Loss:[0.0665]
Loss:[0.0701]
Loss:[0.0742]
Loss:[0.0696]
Loss:[0.0674]
Loss:[0.0716]
Loss:[0.0679]
Loss:[0.0691]
Loss:[0.0639]
Loss:[0.0679]
Loss:[0.0672]
Loss:[0.0607]
Loss:[0.0659]
Loss:[0.0655]
Loss:[0.0613]
Loss:[0.0709]
Loss:[0.0704]
Loss:[0.0629]
Loss:[0.0645]
Loss:[0.0602]
Loss:[0.0654]
Loss:[0.0620]
Loss:[0.0611]
Loss:[0.0615]
Loss:[0.0574]
Loss:[0.0643]
Loss:[0.0603]
Loss:[0.0620]
Loss:[0.0559]
Loss:[0.0579]
Loss:[0.0592]
Loss:[0.0585]
Loss:[0.0586]
Loss:[0.0590]
Loss:[0.0602]
Loss:[0.0579]
Loss:[0.0580]
Loss:[0.0569]
Loss:[0.0556]
Loss:[0.0564]
Loss:[0.0596]
Loss:[0.0515]
Loss:[0.0561]
Loss:[0.0558]
Loss:[0.0551]
Loss:[0.0555]
Loss:[0.0581]
Loss:[0.0559]
Loss:[0.0559]
Loss:[0.0502]
Loss:[0.0516]
Loss:[0.0486]
Loss:[0.0472]
Loss:[0.0530]
Loss:[0.0483]
Loss:[0.0498]
Loss:[0.0540]
Loss:[0.0485]
Loss:[0.0482]
Loss:[0.0484]
Loss:[0.0449]
Loss:[0.0510]
Loss:[0.0477]
Loss:[0.0477]
Loss:[0.0496]
Loss:[0.0466]
Loss:[0.0498]
Loss:[0.0476]
Loss:[0.0515]
Loss:[0.0475]
Loss:[0.0479]
Loss:[0.0544]
Loss:[0.0441]
Loss:[0.0472]
Loss:[0.0469]
Loss:[0.0443]
Loss:[0.0482]
Loss:[0.0396]
Loss:[0.0433]
Loss:[0.0429]
Loss:[0.0497]
Loss:[0.0410]
Loss:[0.0435]
Loss:[0.0482]
Loss:[0.0439]
Loss:[0.0418]
Loss:[0.0430]
Loss:[0.0397]
Loss:[0.0345]
Loss:[0.0424]
Loss:[0.0421]
Loss:[0.0446]
Loss:[0.0425]
Loss:[0.0427]
Loss:[0.0458]
Loss:[0.0418]
Loss:[0.0429]
Loss:[0.0435]
Loss:[0.0390]
Loss:[0.0427]
Loss:[0.0428]
Loss:[0.0381]
Loss:[0.0348]
Loss:[0.0398]
Loss:[0.0396]
Loss:[0.0386]
Loss:[0.0400]
Loss:[0.0391]
Loss:[0.0411]
Early stopping!
Loading 233th epoch
acc:[0.7070]
acc:[0.7050]
acc:[0.7080]
acc:[0.7070]
acc:[0.7110]
acc:[0.7090]
acc:[0.7080]
acc:[0.7130]
acc:[0.7060]
acc:[0.7100]
acc:[0.7090]
acc:[0.7060]
acc:[0.7060]
acc:[0.7090]
acc:[0.7090]
acc:[0.7090]
acc:[0.7060]
acc:[0.7060]
acc:[0.7060]
acc:[0.7080]
acc:[0.7100]
acc:[0.7060]
acc:[0.7110]
acc:[0.7080]
acc:[0.7080]
acc:[0.7090]
acc:[0.7050]
acc:[0.7040]
acc:[0.7070]
acc:[0.7070]
acc:[0.7090]
acc:[0.7110]
acc:[0.7080]
acc:[0.7100]
acc:[0.7100]
acc:[0.7080]
acc:[0.7110]
acc:[0.7130]
acc:[0.7090]
acc:[0.7070]
acc:[0.7080]
acc:[0.7050]
acc:[0.7080]
acc:[0.7100]
acc:[0.7080]
acc:[0.7080]
acc:[0.7070]
acc:[0.7070]
acc:[0.7060]
acc:[0.7080]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7081]
Mean:[70.8080]
Std :[0.1998]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='citeseer', drop_percent=0.2, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6901]
Loss:[0.6827]
Loss:[0.6706]
Loss:[0.6546]
Loss:[0.6347]
Loss:[0.6160]
Loss:[0.5998]
Loss:[0.5880]
Loss:[0.5808]
Loss:[0.5787]
Loss:[0.5674]
Loss:[0.5606]
Loss:[0.5566]
Loss:[0.5475]
Loss:[0.5402]
Loss:[0.5298]
Loss:[0.5229]
Loss:[0.5131]
Loss:[0.5112]
Loss:[0.5040]
Loss:[0.4970]
Loss:[0.4853]
Loss:[0.4835]
Loss:[0.4731]
Loss:[0.4643]
Loss:[0.4591]
Loss:[0.4461]
Loss:[0.4414]
Loss:[0.4349]
Loss:[0.4241]
Loss:[0.4172]
Loss:[0.4103]
Loss:[0.3953]
Loss:[0.3923]
Loss:[0.3788]
Loss:[0.3708]
Loss:[0.3609]
Loss:[0.3555]
Loss:[0.3534]
Loss:[0.3367]
Loss:[0.3292]
Loss:[0.3206]
Loss:[0.3143]
Loss:[0.3002]
Loss:[0.2949]
Loss:[0.2831]
Loss:[0.2769]
Loss:[0.2698]
Loss:[0.2645]
Loss:[0.2514]
Loss:[0.2447]
Loss:[0.2361]
Loss:[0.2344]
Loss:[0.2287]
Loss:[0.2203]
Loss:[0.2045]
Loss:[0.2033]
Loss:[0.2013]
Loss:[0.1972]
Loss:[0.1892]
Loss:[0.1756]
Loss:[0.1759]
Loss:[0.1772]
Loss:[0.1663]
Loss:[0.1559]
Loss:[0.1584]
Loss:[0.1536]
Loss:[0.1439]
Loss:[0.1396]
Loss:[0.1474]
Loss:[0.1375]
Loss:[0.1275]
Loss:[0.1315]
Loss:[0.1279]
Loss:[0.1205]
Loss:[0.1169]
Loss:[0.1101]
Loss:[0.1138]
Loss:[0.1016]
Loss:[0.1049]
Loss:[0.1020]
Loss:[0.1013]
Loss:[0.0950]
Loss:[0.0954]
Loss:[0.0973]
Loss:[0.0921]
Loss:[0.0866]
Loss:[0.0888]
Loss:[0.0857]
Loss:[0.0798]
Loss:[0.0880]
Loss:[0.0798]
Loss:[0.0782]
Loss:[0.0760]
Loss:[0.0764]
Loss:[0.0763]
Loss:[0.0683]
Loss:[0.0721]
Loss:[0.0702]
Loss:[0.0644]
Loss:[0.0653]
Loss:[0.0643]
Loss:[0.0673]
Loss:[0.0675]
Loss:[0.0641]
Loss:[0.0592]
Loss:[0.0595]
Loss:[0.0582]
Loss:[0.0577]
Loss:[0.0566]
Loss:[0.0591]
Loss:[0.0514]
Loss:[0.0535]
Loss:[0.0565]
Loss:[0.0538]
Loss:[0.0545]
Loss:[0.0506]
Loss:[0.0466]
Loss:[0.0501]
Loss:[0.0461]
Loss:[0.0477]
Loss:[0.0469]
Loss:[0.0488]
Loss:[0.0454]
Loss:[0.0484]
Loss:[0.0436]
Loss:[0.0421]
Loss:[0.0403]
Loss:[0.0414]
Loss:[0.0431]
Loss:[0.0441]
Loss:[0.0392]
Loss:[0.0453]
Loss:[0.0399]
Loss:[0.0367]
Loss:[0.0403]
Loss:[0.0343]
Loss:[0.0387]
Loss:[0.0364]
Loss:[0.0359]
Loss:[0.0347]
Loss:[0.0354]
Loss:[0.0317]
Loss:[0.0372]
Loss:[0.0333]
Loss:[0.0315]
Loss:[0.0380]
Loss:[0.0330]
Loss:[0.0324]
Loss:[0.0348]
Loss:[0.0306]
Loss:[0.0326]
Loss:[0.0297]
Loss:[0.0303]
Loss:[0.0292]
Loss:[0.0296]
Loss:[0.0300]
Loss:[0.0302]
Loss:[0.0295]
Loss:[0.0281]
Loss:[0.0296]
Loss:[0.0250]
Loss:[0.0307]
Loss:[0.0271]
Loss:[0.0273]
Loss:[0.0262]
Loss:[0.0253]
Loss:[0.0265]
Loss:[0.0269]
Loss:[0.0277]
Loss:[0.0260]
Loss:[0.0255]
Loss:[0.0256]
Loss:[0.0254]
Loss:[0.0284]
Loss:[0.0254]
Loss:[0.0248]
Loss:[0.0237]
Loss:[0.0272]
Loss:[0.0239]
Loss:[0.0229]
Loss:[0.0234]
Loss:[0.0261]
Loss:[0.0231]
Loss:[0.0253]
Loss:[0.0245]
Loss:[0.0263]
Loss:[0.0229]
Loss:[0.0236]
Loss:[0.0233]
Loss:[0.0254]
Loss:[0.0210]
Loss:[0.0230]
Loss:[0.0170]
Loss:[0.0197]
Loss:[0.0186]
Loss:[0.0209]
Loss:[0.0184]
Loss:[0.0206]
Loss:[0.0217]
Loss:[0.0236]
Loss:[0.0204]
Loss:[0.0212]
Loss:[0.0193]
Loss:[0.0181]
Loss:[0.0221]
Loss:[0.0210]
Loss:[0.0189]
Loss:[0.0178]
Loss:[0.0203]
Loss:[0.0202]
Loss:[0.0205]
Loss:[0.0204]
Loss:[0.0176]
Early stopping!
Loading 194th epoch
acc:[0.6950]
acc:[0.6950]
acc:[0.6940]
acc:[0.6930]
acc:[0.6910]
acc:[0.6940]
acc:[0.6930]
acc:[0.6880]
acc:[0.6950]
acc:[0.6930]
acc:[0.6940]
acc:[0.6920]
acc:[0.6900]
acc:[0.6910]
acc:[0.6920]
acc:[0.6940]
acc:[0.6910]
acc:[0.6920]
acc:[0.6940]
acc:[0.6890]
acc:[0.6910]
acc:[0.6910]
acc:[0.6890]
acc:[0.6940]
acc:[0.6950]
acc:[0.6940]
acc:[0.6940]
acc:[0.6940]
acc:[0.6930]
acc:[0.6930]
acc:[0.6950]
acc:[0.6920]
acc:[0.6980]
acc:[0.6940]
acc:[0.6960]
acc:[0.6910]
acc:[0.6930]
acc:[0.6930]
acc:[0.6910]
acc:[0.6950]
acc:[0.6920]
acc:[0.6900]
acc:[0.6910]
acc:[0.6920]
acc:[0.6970]
acc:[0.6920]
acc:[0.6930]
acc:[0.6910]
acc:[0.6950]
acc:[0.6930]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.6928]
Mean:[69.2840]
Std :[0.2044]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='citeseer', drop_percent=0.3, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6931]
Loss:[0.6884]
Loss:[0.6776]
Loss:[0.6606]
Loss:[0.6377]
Loss:[0.6108]
Loss:[0.5814]
Loss:[0.5541]
Loss:[0.5305]
Loss:[0.5116]
Loss:[0.5011]
Loss:[0.4930]
Loss:[0.4884]
Loss:[0.4881]
Loss:[0.4887]
Loss:[0.4813]
Loss:[0.4704]
Loss:[0.4691]
Loss:[0.4559]
Loss:[0.4596]
Loss:[0.4440]
Loss:[0.4450]
Loss:[0.4339]
Loss:[0.4304]
Loss:[0.4215]
Loss:[0.4242]
Loss:[0.4158]
Loss:[0.4109]
Loss:[0.4076]
Loss:[0.4019]
Loss:[0.3958]
Loss:[0.3941]
Loss:[0.3899]
Loss:[0.3806]
Loss:[0.3791]
Loss:[0.3734]
Loss:[0.3725]
Loss:[0.3632]
Loss:[0.3585]
Loss:[0.3616]
Loss:[0.3480]
Loss:[0.3451]
Loss:[0.3440]
Loss:[0.3368]
Loss:[0.3295]
Loss:[0.3277]
Loss:[0.3229]
Loss:[0.3103]
Loss:[0.3105]
Loss:[0.3085]
Loss:[0.2990]
Loss:[0.2966]
Loss:[0.2903]
Loss:[0.2833]
Loss:[0.2799]
Loss:[0.2772]
Loss:[0.2637]
Loss:[0.2679]
Loss:[0.2602]
Loss:[0.2494]
Loss:[0.2513]
Loss:[0.2453]
Loss:[0.2413]
Loss:[0.2327]
Loss:[0.2302]
Loss:[0.2257]
Loss:[0.2155]
Loss:[0.2156]
Loss:[0.2145]
Loss:[0.2013]
Loss:[0.2064]
Loss:[0.1969]
Loss:[0.1904]
Loss:[0.1933]
Loss:[0.1842]
Loss:[0.1774]
Loss:[0.1748]
Loss:[0.1679]
Loss:[0.1646]
Loss:[0.1638]
Loss:[0.1546]
Loss:[0.1546]
Loss:[0.1535]
Loss:[0.1512]
Loss:[0.1414]
Loss:[0.1417]
Loss:[0.1369]
Loss:[0.1338]
Loss:[0.1298]
Loss:[0.1281]
Loss:[0.1251]
Loss:[0.1272]
Loss:[0.1178]
Loss:[0.1139]
Loss:[0.1149]
Loss:[0.1104]
Loss:[0.1107]
Loss:[0.1030]
Loss:[0.1016]
Loss:[0.0963]
Loss:[0.0955]
Loss:[0.0961]
Loss:[0.0971]
Loss:[0.0923]
Loss:[0.0889]
Loss:[0.0880]
Loss:[0.0864]
Loss:[0.0804]
Loss:[0.0826]
Loss:[0.0779]
Loss:[0.0795]
Loss:[0.0787]
Loss:[0.0780]
Loss:[0.0736]
Loss:[0.0765]
Loss:[0.0711]
Loss:[0.0727]
Loss:[0.0658]
Loss:[0.0686]
Loss:[0.0659]
Loss:[0.0626]
Loss:[0.0657]
Loss:[0.0586]
Loss:[0.0634]
Loss:[0.0599]
Loss:[0.0656]
Loss:[0.0595]
Loss:[0.0591]
Loss:[0.0607]
Loss:[0.0530]
Loss:[0.0552]
Loss:[0.0530]
Loss:[0.0528]
Loss:[0.0543]
Loss:[0.0499]
Loss:[0.0500]
Loss:[0.0514]
Loss:[0.0476]
Loss:[0.0498]
Loss:[0.0501]
Loss:[0.0452]
Loss:[0.0428]
Loss:[0.0422]
Loss:[0.0423]
Loss:[0.0468]
Loss:[0.0462]
Loss:[0.0413]
Loss:[0.0456]
Loss:[0.0420]
Loss:[0.0408]
Loss:[0.0436]
Loss:[0.0357]
Loss:[0.0378]
Loss:[0.0381]
Loss:[0.0368]
Loss:[0.0375]
Loss:[0.0342]
Loss:[0.0345]
Loss:[0.0405]
Loss:[0.0382]
Loss:[0.0330]
Loss:[0.0333]
Loss:[0.0326]
Loss:[0.0314]
Loss:[0.0343]
Loss:[0.0338]
Loss:[0.0326]
Loss:[0.0310]
Loss:[0.0337]
Loss:[0.0359]
Loss:[0.0349]
Loss:[0.0320]
Loss:[0.0326]
Loss:[0.0313]
Loss:[0.0304]
Loss:[0.0331]
Loss:[0.0311]
Loss:[0.0325]
Loss:[0.0293]
Loss:[0.0322]
Loss:[0.0293]
Loss:[0.0298]
Loss:[0.0290]
Loss:[0.0276]
Loss:[0.0268]
Loss:[0.0268]
Loss:[0.0256]
Loss:[0.0278]
Loss:[0.0263]
Loss:[0.0256]
Loss:[0.0302]
Loss:[0.0289]
Loss:[0.0213]
Loss:[0.0301]
Loss:[0.0236]
Loss:[0.0266]
Loss:[0.0223]
Loss:[0.0258]
Loss:[0.0234]
Loss:[0.0243]
Loss:[0.0248]
Loss:[0.0272]
Loss:[0.0249]
Loss:[0.0234]
Loss:[0.0241]
Loss:[0.0242]
Loss:[0.0257]
Loss:[0.0253]
Loss:[0.0211]
Loss:[0.0244]
Loss:[0.0207]
Loss:[0.0248]
Loss:[0.0220]
Loss:[0.0207]
Loss:[0.0214]
Loss:[0.0221]
Loss:[0.0252]
Loss:[0.0177]
Loss:[0.0223]
Loss:[0.0219]
Loss:[0.0182]
Loss:[0.0207]
Loss:[0.0197]
Loss:[0.0218]
Loss:[0.0197]
Loss:[0.0201]
Loss:[0.0198]
Loss:[0.0195]
Loss:[0.0161]
Loss:[0.0211]
Loss:[0.0206]
Loss:[0.0210]
Loss:[0.0159]
Loss:[0.0168]
Loss:[0.0189]
Loss:[0.0205]
Loss:[0.0183]
Loss:[0.0190]
Loss:[0.0205]
Loss:[0.0199]
Loss:[0.0218]
Loss:[0.0208]
Loss:[0.0196]
Loss:[0.0179]
Loss:[0.0235]
Loss:[0.0206]
Loss:[0.0196]
Loss:[0.0162]
Loss:[0.0171]
Loss:[0.0174]
Loss:[0.0181]
Loss:[0.0181]
Loss:[0.0184]
Early stopping!
Loading 232th epoch
acc:[0.6320]
acc:[0.6320]
acc:[0.6290]
acc:[0.6280]
acc:[0.6300]
acc:[0.6320]
acc:[0.6290]
acc:[0.6280]
acc:[0.6320]
acc:[0.6350]
acc:[0.6320]
acc:[0.6280]
acc:[0.6310]
acc:[0.6270]
acc:[0.6270]
acc:[0.6320]
acc:[0.6280]
acc:[0.6320]
acc:[0.6310]
acc:[0.6330]
acc:[0.6280]
acc:[0.6290]
acc:[0.6320]
acc:[0.6300]
acc:[0.6340]
acc:[0.6300]
acc:[0.6300]
acc:[0.6350]
acc:[0.6300]
acc:[0.6280]
acc:[0.6330]
acc:[0.6320]
acc:[0.6330]
acc:[0.6310]
acc:[0.6320]
acc:[0.6280]
acc:[0.6300]
acc:[0.6350]
acc:[0.6350]
acc:[0.6310]
acc:[0.6290]
acc:[0.6290]
acc:[0.6320]
acc:[0.6310]
acc:[0.6300]
acc:[0.6310]
acc:[0.6310]
acc:[0.6280]
acc:[0.6300]
acc:[0.6270]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.6306]
Mean:[63.0640]
Std :[0.2202]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='citeseer', drop_percent=0.4, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6932]
Loss:[0.6865]
Loss:[0.6719]
Loss:[0.6499]
Loss:[0.6207]
Loss:[0.5858]
Loss:[0.5480]
Loss:[0.5105]
Loss:[0.4758]
Loss:[0.4490]
Loss:[0.4290]
Loss:[0.4139]
Loss:[0.4060]
Loss:[0.4012]
Loss:[0.3946]
Loss:[0.3861]
Loss:[0.3837]
Loss:[0.3760]
Loss:[0.3705]
Loss:[0.3637]
Loss:[0.3607]
Loss:[0.3517]
Loss:[0.3496]
Loss:[0.3415]
Loss:[0.3421]
Loss:[0.3406]
Loss:[0.3335]
Loss:[0.3362]
Loss:[0.3305]
Loss:[0.3279]
Loss:[0.3210]
Loss:[0.3193]
Loss:[0.3113]
Loss:[0.3081]
Loss:[0.3099]
Loss:[0.3051]
Loss:[0.3048]
Loss:[0.3041]
Loss:[0.2965]
Loss:[0.2937]
Loss:[0.2906]
Loss:[0.2984]
Loss:[0.2906]
Loss:[0.2901]
Loss:[0.2829]
Loss:[0.2816]
Loss:[0.2787]
Loss:[0.2745]
Loss:[0.2744]
Loss:[0.2743]
Loss:[0.2727]
Loss:[0.2715]
Loss:[0.2632]
Loss:[0.2695]
Loss:[0.2597]
Loss:[0.2542]
Loss:[0.2557]
Loss:[0.2486]
Loss:[0.2484]
Loss:[0.2488]
Loss:[0.2412]
Loss:[0.2380]
Loss:[0.2378]
Loss:[0.2399]
Loss:[0.2319]
Loss:[0.2291]
Loss:[0.2280]
Loss:[0.2281]
Loss:[0.2152]
Loss:[0.2180]
Loss:[0.2175]
Loss:[0.2136]
Loss:[0.2121]
Loss:[0.2097]
Loss:[0.2073]
Loss:[0.1958]
Loss:[0.2006]
Loss:[0.2050]
Loss:[0.2020]
Loss:[0.1977]
Loss:[0.1851]
Loss:[0.1843]
Loss:[0.1848]
Loss:[0.1748]
Loss:[0.1769]
Loss:[0.1841]
Loss:[0.1788]
Loss:[0.1711]
Loss:[0.1686]
Loss:[0.1593]
Loss:[0.1583]
Loss:[0.1669]
Loss:[0.1658]
Loss:[0.1478]
Loss:[0.1485]
Loss:[0.1465]
Loss:[0.1521]
Loss:[0.1477]
Loss:[0.1565]
Loss:[0.1397]
Loss:[0.1501]
Loss:[0.1487]
Loss:[0.1407]
Loss:[0.1446]
Loss:[0.1245]
Loss:[0.1360]
Loss:[0.1225]
Loss:[0.1244]
Loss:[0.1229]
Loss:[0.1219]
Loss:[0.1248]
Loss:[0.1170]
Loss:[0.1168]
Loss:[0.1100]
Loss:[0.1159]
Loss:[0.1086]
Loss:[0.1076]
Loss:[0.1021]
Loss:[0.1038]
Loss:[0.1000]
Loss:[0.1028]
Loss:[0.0951]
Loss:[0.0908]
Loss:[0.0998]
Loss:[0.0903]
Loss:[0.0954]
Loss:[0.0905]
Loss:[0.0903]
Loss:[0.0877]
Loss:[0.0869]
Loss:[0.0846]
Loss:[0.0788]
Loss:[0.0787]
Loss:[0.0828]
Loss:[0.0766]
Loss:[0.0793]
Loss:[0.0762]
Loss:[0.0760]
Loss:[0.0727]
Loss:[0.0701]
Loss:[0.0724]
Loss:[0.0688]
Loss:[0.0694]
Loss:[0.0713]
Loss:[0.0671]
Loss:[0.0666]
Loss:[0.0620]
Loss:[0.0701]
Loss:[0.0669]
Loss:[0.0646]
Loss:[0.0620]
Loss:[0.0570]
Loss:[0.0619]
Loss:[0.0545]
Loss:[0.0578]
Loss:[0.0563]
Loss:[0.0525]
Loss:[0.0526]
Loss:[0.0567]
Loss:[0.0501]
Loss:[0.0494]
Loss:[0.0529]
Loss:[0.0525]
Loss:[0.0508]
Loss:[0.0501]
Loss:[0.0531]
Loss:[0.0497]
Loss:[0.0476]
Loss:[0.0510]
Loss:[0.0508]
Loss:[0.0471]
Loss:[0.0475]
Loss:[0.0480]
Loss:[0.0463]
Loss:[0.0424]
Loss:[0.0499]
Loss:[0.0427]
Loss:[0.0476]
Loss:[0.0416]
Loss:[0.0442]
Loss:[0.0441]
Loss:[0.0420]
Loss:[0.0416]
Loss:[0.0398]
Loss:[0.0419]
Loss:[0.0426]
Loss:[0.0403]
Loss:[0.0363]
Loss:[0.0371]
Loss:[0.0399]
Loss:[0.0408]
Loss:[0.0375]
Loss:[0.0327]
Loss:[0.0407]
Loss:[0.0350]
Loss:[0.0357]
Loss:[0.0369]
Loss:[0.0367]
Loss:[0.0352]
Loss:[0.0369]
Loss:[0.0347]
Loss:[0.0435]
Loss:[0.0392]
Loss:[0.0385]
Loss:[0.0362]
Loss:[0.0361]
Loss:[0.0350]
Loss:[0.0343]
Loss:[0.0325]
Loss:[0.0363]
Loss:[0.0309]
Loss:[0.0333]
Loss:[0.0312]
Loss:[0.0304]
Loss:[0.0337]
Loss:[0.0317]
Loss:[0.0311]
Loss:[0.0298]
Loss:[0.0288]
Loss:[0.0340]
Loss:[0.0262]
Loss:[0.0303]
Loss:[0.0305]
Loss:[0.0309]
Loss:[0.0300]
Loss:[0.0257]
Loss:[0.0283]
Loss:[0.0243]
Loss:[0.0298]
Loss:[0.0283]
Loss:[0.0297]
Loss:[0.0267]
Loss:[0.0256]
Loss:[0.0236]
Loss:[0.0267]
Loss:[0.0302]
Loss:[0.0264]
Loss:[0.0249]
Loss:[0.0277]
Loss:[0.0259]
Loss:[0.0284]
Loss:[0.0255]
Loss:[0.0279]
Loss:[0.0275]
Loss:[0.0276]
Loss:[0.0247]
Loss:[0.0252]
Loss:[0.0201]
Loss:[0.0231]
Loss:[0.0248]
Loss:[0.0230]
Loss:[0.0253]
Loss:[0.0259]
Loss:[0.0251]
Loss:[0.0224]
Loss:[0.0208]
Loss:[0.0270]
Loss:[0.0298]
Loss:[0.0249]
Loss:[0.0258]
Loss:[0.0260]
Loss:[0.0235]
Loss:[0.0215]
Loss:[0.0232]
Loss:[0.0218]
Loss:[0.0205]
Loss:[0.0188]
Loss:[0.0238]
Loss:[0.0223]
Loss:[0.0233]
Loss:[0.0231]
Loss:[0.0217]
Loss:[0.0203]
Loss:[0.0219]
Loss:[0.0216]
Loss:[0.0178]
Loss:[0.0217]
Loss:[0.0215]
Loss:[0.0261]
Loss:[0.0194]
Loss:[0.0197]
Loss:[0.0189]
Loss:[0.0188]
Loss:[0.0253]
Loss:[0.0198]
Loss:[0.0241]
Loss:[0.0210]
Loss:[0.0195]
Loss:[0.0221]
Loss:[0.0197]
Loss:[0.0198]
Loss:[0.0214]
Loss:[0.0180]
Loss:[0.0215]
Loss:[0.0165]
Loss:[0.0205]
Loss:[0.0182]
Loss:[0.0197]
Loss:[0.0214]
Loss:[0.0200]
Loss:[0.0187]
Loss:[0.0199]
Loss:[0.0173]
Loss:[0.0186]
Loss:[0.0166]
Loss:[0.0161]
Loss:[0.0222]
Loss:[0.0195]
Loss:[0.0177]
Loss:[0.0156]
Loss:[0.0178]
Loss:[0.0175]
Loss:[0.0197]
Loss:[0.0181]
Loss:[0.0151]
Loss:[0.0170]
Loss:[0.0157]
Loss:[0.0200]
Loss:[0.0171]
Loss:[0.0182]
Loss:[0.0220]
Loss:[0.0177]
Loss:[0.0121]
Loss:[0.0155]
Loss:[0.0177]
Loss:[0.0170]
Loss:[0.0175]
Loss:[0.0153]
Loss:[0.0156]
Loss:[0.0160]
Loss:[0.0168]
Loss:[0.0188]
Loss:[0.0174]
Loss:[0.0163]
Loss:[0.0156]
Loss:[0.0165]
Loss:[0.0199]
Loss:[0.0146]
Loss:[0.0193]
Loss:[0.0211]
Loss:[0.0160]
Loss:[0.0137]
Loss:[0.0166]
Early stopping!
Loading 322th epoch
acc:[0.4890]
acc:[0.4910]
acc:[0.4920]
acc:[0.4910]
acc:[0.4890]
acc:[0.4910]
acc:[0.4890]
acc:[0.4870]
acc:[0.4880]
acc:[0.4920]
acc:[0.4910]
acc:[0.4890]
acc:[0.4900]
acc:[0.4900]
acc:[0.4870]
acc:[0.4940]
acc:[0.4930]
acc:[0.4880]
acc:[0.4900]
acc:[0.4880]
acc:[0.4940]
acc:[0.4950]
acc:[0.4900]
acc:[0.4940]
acc:[0.4910]
acc:[0.4940]
acc:[0.4920]
acc:[0.4920]
acc:[0.4920]
acc:[0.4930]
acc:[0.4930]
acc:[0.4930]
acc:[0.4870]
acc:[0.4920]
acc:[0.4890]
acc:[0.4850]
acc:[0.4840]
acc:[0.4940]
acc:[0.4920]
acc:[0.4890]
acc:[0.4940]
acc:[0.4880]
acc:[0.4930]
acc:[0.4870]
acc:[0.4900]
acc:[0.4890]
acc:[0.4920]
acc:[0.4900]
acc:[0.4900]
acc:[0.4910]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.4906]
Mean:[49.0560]
Std :[0.2508]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='citeseer', drop_percent=0.5, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6932]
Loss:[0.6845]
Loss:[0.6653]
Loss:[0.6372]
Loss:[0.6011]
Loss:[0.5580]
Loss:[0.5112]
Loss:[0.4638]
Loss:[0.4184]
Loss:[0.3802]
Loss:[0.3491]
Loss:[0.3265]
Loss:[0.3112]
Loss:[0.3019]
Loss:[0.2919]
Loss:[0.2851]
Loss:[0.2793]
Loss:[0.2683]
Loss:[0.2670]
Loss:[0.2549]
Loss:[0.2541]
Loss:[0.2485]
Loss:[0.2392]
Loss:[0.2317]
Loss:[0.2334]
Loss:[0.2335]
Loss:[0.2318]
Loss:[0.2278]
Loss:[0.2215]
Loss:[0.2182]
Loss:[0.2219]
Loss:[0.2161]
Loss:[0.2187]
Loss:[0.2108]
Loss:[0.2141]
Loss:[0.2092]
Loss:[0.2008]
Loss:[0.2027]
Loss:[0.2019]
Loss:[0.2055]
Loss:[0.2007]
Loss:[0.2081]
Loss:[0.1946]
Loss:[0.1974]
Loss:[0.1932]
Loss:[0.1882]
Loss:[0.1913]
Loss:[0.1937]
Loss:[0.1966]
Loss:[0.1916]
Loss:[0.1944]
Loss:[0.1908]
Loss:[0.1803]
Loss:[0.1944]
Loss:[0.1867]
Loss:[0.1848]
Loss:[0.1852]
Loss:[0.1811]
Loss:[0.1789]
Loss:[0.1833]
Loss:[0.1775]
Loss:[0.1794]
Loss:[0.1757]
Loss:[0.1819]
Loss:[0.1735]
Loss:[0.1732]
Loss:[0.1759]
Loss:[0.1721]
Loss:[0.1736]
Loss:[0.1722]
Loss:[0.1758]
Loss:[0.1707]
Loss:[0.1731]
Loss:[0.1693]
Loss:[0.1660]
Loss:[0.1672]
Loss:[0.1649]
Loss:[0.1629]
Loss:[0.1646]
Loss:[0.1642]
Loss:[0.1684]
Loss:[0.1651]
Loss:[0.1631]
Loss:[0.1533]
Loss:[0.1556]
Loss:[0.1597]
Loss:[0.1592]
Loss:[0.1635]
Loss:[0.1591]
Loss:[0.1594]
Loss:[0.1574]
Loss:[0.1539]
Loss:[0.1454]
Loss:[0.1511]
Loss:[0.1497]
Loss:[0.1478]
Loss:[0.1572]
Loss:[0.1550]
Loss:[0.1498]
Loss:[0.1451]
Loss:[0.1454]
Loss:[0.1527]
Loss:[0.1478]
Loss:[0.1419]
Loss:[0.1408]
Loss:[0.1391]
Loss:[0.1437]
Loss:[0.1353]
Loss:[0.1408]
Loss:[0.1459]
Loss:[0.1398]
Loss:[0.1328]
Loss:[0.1321]
Loss:[0.1334]
Loss:[0.1361]
Loss:[0.1373]
Loss:[0.1311]
Loss:[0.1382]
Loss:[0.1236]
Loss:[0.1232]
Loss:[0.1256]
Loss:[0.1200]
Loss:[0.1216]
Loss:[0.1270]
Loss:[0.1266]
Loss:[0.1242]
Loss:[0.1235]
Loss:[0.1166]
Loss:[0.1253]
Loss:[0.1120]
Loss:[0.1246]
Loss:[0.1250]
Loss:[0.1262]
Loss:[0.1151]
Loss:[0.1210]
Loss:[0.1313]
Loss:[0.1131]
Loss:[0.1267]
Loss:[0.1170]
Loss:[0.1218]
Loss:[0.1369]
Loss:[0.1056]
Loss:[0.1240]
Loss:[0.1065]
Loss:[0.1185]
Loss:[0.1106]
Loss:[0.1136]
Loss:[0.1083]
Loss:[0.1107]
Loss:[0.1098]
Loss:[0.0964]
Loss:[0.1040]
Loss:[0.1049]
Loss:[0.1073]
Loss:[0.1008]
Loss:[0.1000]
Loss:[0.0984]
Loss:[0.0963]
Loss:[0.0946]
Loss:[0.0894]
Loss:[0.0880]
Loss:[0.0943]
Loss:[0.0906]
Loss:[0.0924]
Loss:[0.0886]
Loss:[0.0859]
Loss:[0.0802]
Loss:[0.0893]
Loss:[0.0908]
Loss:[0.0857]
Loss:[0.0824]
Loss:[0.0796]
Loss:[0.0820]
Loss:[0.0798]
Loss:[0.0747]
Loss:[0.0816]
Loss:[0.0811]
Loss:[0.0765]
Loss:[0.0749]
Loss:[0.0752]
Loss:[0.0760]
Loss:[0.0770]
Loss:[0.0786]
Loss:[0.0736]
Loss:[0.0756]
Loss:[0.0789]
Loss:[0.0671]
Loss:[0.0715]
Loss:[0.0717]
Loss:[0.0806]
Loss:[0.0718]
Loss:[0.0714]
Loss:[0.0674]
Loss:[0.0704]
Loss:[0.0689]
Loss:[0.0689]
Loss:[0.0688]
Loss:[0.0674]
Loss:[0.0650]
Loss:[0.0666]
Loss:[0.0647]
Loss:[0.0704]
Loss:[0.0653]
Loss:[0.0695]
Loss:[0.0625]
Loss:[0.0652]
Loss:[0.0627]
Loss:[0.0567]
Loss:[0.0587]
Loss:[0.0644]
Loss:[0.0551]
Loss:[0.0662]
Loss:[0.0551]
Loss:[0.0619]
Loss:[0.0590]
Loss:[0.0595]
Loss:[0.0546]
Loss:[0.0517]
Loss:[0.0531]
Loss:[0.0527]
Loss:[0.0581]
Loss:[0.0522]
Loss:[0.0582]
Loss:[0.0507]
Loss:[0.0530]
Loss:[0.0588]
Loss:[0.0514]
Loss:[0.0476]
Loss:[0.0508]
Loss:[0.0572]
Loss:[0.0469]
Loss:[0.0444]
Loss:[0.0511]
Loss:[0.0435]
Loss:[0.0505]
Loss:[0.0480]
Loss:[0.0520]
Loss:[0.0463]
Loss:[0.0469]
Loss:[0.0451]
Loss:[0.0523]
Loss:[0.0497]
Loss:[0.0522]
Loss:[0.0542]
Loss:[0.0475]
Loss:[0.0463]
Loss:[0.0456]
Loss:[0.0410]
Loss:[0.0409]
Loss:[0.0486]
Loss:[0.0425]
Loss:[0.0428]
Loss:[0.0490]
Loss:[0.0403]
Loss:[0.0433]
Loss:[0.0425]
Loss:[0.0453]
Loss:[0.0441]
Loss:[0.0421]
Loss:[0.0408]
Loss:[0.0422]
Loss:[0.0397]
Loss:[0.0418]
Loss:[0.0446]
Loss:[0.0358]
Loss:[0.0402]
Loss:[0.0340]
Loss:[0.0407]
Loss:[0.0408]
Loss:[0.0388]
Loss:[0.0447]
Loss:[0.0408]
Loss:[0.0364]
Loss:[0.0401]
Loss:[0.0397]
Loss:[0.0323]
Loss:[0.0390]
Loss:[0.0366]
Loss:[0.0386]
Loss:[0.0365]
Loss:[0.0359]
Loss:[0.0366]
Loss:[0.0323]
Loss:[0.0380]
Loss:[0.0357]
Loss:[0.0398]
Loss:[0.0368]
Loss:[0.0351]
Loss:[0.0370]
Loss:[0.0325]
Loss:[0.0335]
Loss:[0.0343]
Loss:[0.0327]
Loss:[0.0333]
Loss:[0.0311]
Loss:[0.0350]
Loss:[0.0309]
Loss:[0.0372]
Loss:[0.0358]
Loss:[0.0351]
Loss:[0.0362]
Loss:[0.0267]
Loss:[0.0343]
Loss:[0.0329]
Loss:[0.0278]
Loss:[0.0295]
Loss:[0.0344]
Loss:[0.0368]
Loss:[0.0356]
Loss:[0.0318]
Loss:[0.0356]
Loss:[0.0278]
Loss:[0.0329]
Loss:[0.0298]
Loss:[0.0290]
Loss:[0.0293]
Loss:[0.0291]
Loss:[0.0331]
Loss:[0.0304]
Loss:[0.0239]
Loss:[0.0291]
Loss:[0.0268]
Loss:[0.0252]
Loss:[0.0252]
Loss:[0.0297]
Loss:[0.0337]
Loss:[0.0301]
Loss:[0.0273]
Loss:[0.0305]
Loss:[0.0280]
Loss:[0.0285]
Loss:[0.0249]
Loss:[0.0244]
Loss:[0.0340]
Loss:[0.0283]
Loss:[0.0290]
Loss:[0.0329]
Loss:[0.0307]
Loss:[0.0286]
Loss:[0.0284]
Early stopping!
Loading 319th epoch
acc:[0.3490]
acc:[0.3480]
acc:[0.3500]
acc:[0.3540]
acc:[0.3490]
acc:[0.3480]
acc:[0.3450]
acc:[0.3510]
acc:[0.3500]
acc:[0.3470]
acc:[0.3540]
acc:[0.3470]
acc:[0.3480]
acc:[0.3500]
acc:[0.3510]
acc:[0.3490]
acc:[0.3510]
acc:[0.3500]
acc:[0.3440]
acc:[0.3510]
acc:[0.3460]
acc:[0.3420]
acc:[0.3490]
acc:[0.3500]
acc:[0.3380]
acc:[0.3450]
acc:[0.3440]
acc:[0.3480]
acc:[0.3470]
acc:[0.3410]
acc:[0.3460]
acc:[0.3470]
acc:[0.3500]
acc:[0.3480]
acc:[0.3490]
acc:[0.3470]
acc:[0.3470]
acc:[0.3540]
acc:[0.3430]
acc:[0.3470]
acc:[0.3480]
acc:[0.3520]
acc:[0.3500]
acc:[0.3440]
acc:[0.3460]
acc:[0.3440]
acc:[0.3450]
acc:[0.3520]
acc:[0.3470]
acc:[0.3480]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.3478]
Mean:[34.7800]
Std :[0.3283]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='mask', dataset='citeseer', drop_percent=0.6, gpu=3, seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Using CUDA
Loss:[0.6932]
Loss:[0.6823]
Loss:[0.6579]
Loss:[0.6236]
Loss:[0.5803]
Loss:[0.5296]
Loss:[0.4749]
Loss:[0.4197]
Loss:[0.3676]
Loss:[0.3225]
Loss:[0.2868]
Loss:[0.2600]
Loss:[0.2413]
Loss:[0.2273]
Loss:[0.2207]
Loss:[0.2117]
Loss:[0.2035]
Loss:[0.1979]
Loss:[0.1927]
Loss:[0.1874]
Loss:[0.1816]
Loss:[0.1722]
Loss:[0.1678]
Loss:[0.1592]
Loss:[0.1550]
Loss:[0.1611]
Loss:[0.1534]
Loss:[0.1500]
Loss:[0.1567]
Loss:[0.1440]
Loss:[0.1419]
Loss:[0.1419]
Loss:[0.1381]
Loss:[0.1403]
Loss:[0.1339]
Loss:[0.1274]
Loss:[0.1314]
Loss:[0.1293]
Loss:[0.1247]
Loss:[0.1258]
Loss:[0.1260]
Loss:[0.1229]
Loss:[0.1189]
Loss:[0.1234]
Loss:[0.1157]
Loss:[0.1173]
Loss:[0.1211]
Loss:[0.1200]
Loss:[0.1189]
Loss:[0.1159]
Loss:[0.1145]
Loss:[0.1187]
Loss:[0.1123]
Loss:[0.1154]
Loss:[0.1175]
Loss:[0.1108]
Loss:[0.1070]
Loss:[0.1106]
Loss:[0.1058]
Loss:[0.1101]
Loss:[0.1122]
Loss:[0.1016]
Loss:[0.1006]
Loss:[0.1055]
Loss:[0.1052]
Loss:[0.0979]
Loss:[0.1018]
Loss:[0.1014]
Loss:[0.0949]
Loss:[0.1068]
Loss:[0.1061]
Loss:[0.0944]
Loss:[0.1005]
Loss:[0.0933]
Loss:[0.0930]
Loss:[0.0986]
Loss:[0.0941]
Loss:[0.0979]
Loss:[0.0961]
Loss:[0.0955]
Loss:[0.0983]
Loss:[0.0953]
Loss:[0.0926]
Loss:[0.0885]
Loss:[0.0937]
Loss:[0.0932]
Loss:[0.0911]
Loss:[0.0891]
Loss:[0.0964]
Loss:[0.0895]
Loss:[0.0916]
Loss:[0.0841]
Loss:[0.0879]
Loss:[0.0859]
Loss:[0.0877]
Loss:[0.0815]
Loss:[0.0893]
Loss:[0.0789]
Loss:[0.0851]
Loss:[0.0798]
Loss:[0.0828]
Loss:[0.0823]
Loss:[0.0838]
Loss:[0.0818]
Loss:[0.0842]
Loss:[0.0763]
Loss:[0.0768]
Loss:[0.0829]
Loss:[0.0804]
Loss:[0.0755]
Loss:[0.0832]
Loss:[0.0804]
Loss:[0.0801]
Loss:[0.0820]
Loss:[0.0795]
Loss:[0.0814]
Loss:[0.0704]
Loss:[0.0871]
Loss:[0.0743]
Loss:[0.0710]
Loss:[0.0682]
Loss:[0.0714]
Loss:[0.0701]
Loss:[0.0720]
Loss:[0.0756]
Loss:[0.0735]
Loss:[0.0670]
Loss:[0.0673]
Loss:[0.0760]
Loss:[0.0704]
Loss:[0.0673]
Loss:[0.0653]
Loss:[0.0603]
Loss:[0.0750]
Loss:[0.0666]
Loss:[0.0655]
Loss:[0.0682]
Loss:[0.0642]
Loss:[0.0636]
Loss:[0.0659]
Loss:[0.0663]
Loss:[0.0586]
Loss:[0.0567]
Loss:[0.0682]
Loss:[0.0627]
Loss:[0.0599]
Loss:[0.0568]
Loss:[0.0627]
Loss:[0.0610]
Loss:[0.0572]
Loss:[0.0543]
Loss:[0.0567]
Loss:[0.0605]
Loss:[0.0580]
Loss:[0.0516]
Loss:[0.0543]
Loss:[0.0573]
Loss:[0.0590]
Loss:[0.0552]
Loss:[0.0610]
Loss:[0.0500]
Loss:[0.0550]
Loss:[0.0476]
Loss:[0.0502]
Loss:[0.0527]
Loss:[0.0538]
Loss:[0.0485]
Loss:[0.0535]
Loss:[0.0501]
Loss:[0.0481]
Loss:[0.0512]
Loss:[0.0480]
Loss:[0.0522]
Loss:[0.0471]
Loss:[0.0413]
Loss:[0.0534]
Loss:[0.0482]
Loss:[0.0479]
Loss:[0.0427]
Loss:[0.0467]
Loss:[0.0465]
Loss:[0.0417]
Loss:[0.0426]
Loss:[0.0458]
Loss:[0.0442]
Loss:[0.0462]
Loss:[0.0397]
Loss:[0.0406]
Loss:[0.0465]
Loss:[0.0453]
Loss:[0.0425]
Loss:[0.0380]
Loss:[0.0452]
Loss:[0.0422]
Loss:[0.0342]
Loss:[0.0359]
Loss:[0.0391]
Loss:[0.0422]
Loss:[0.0434]
Loss:[0.0338]
Loss:[0.0409]
Loss:[0.0379]
Loss:[0.0382]
Loss:[0.0392]
Loss:[0.0437]
Loss:[0.0388]
Loss:[0.0360]
Loss:[0.0358]
Loss:[0.0357]
Loss:[0.0376]
Loss:[0.0344]
Loss:[0.0331]
Loss:[0.0336]
Loss:[0.0377]
Loss:[0.0348]
Loss:[0.0339]
Loss:[0.0369]
Loss:[0.0289]
Loss:[0.0307]
Loss:[0.0270]
Loss:[0.0323]
Loss:[0.0350]
Loss:[0.0343]
Loss:[0.0288]
Loss:[0.0294]
Loss:[0.0334]
Loss:[0.0312]
Loss:[0.0280]
Loss:[0.0336]
Loss:[0.0318]
Loss:[0.0325]
Loss:[0.0262]
Loss:[0.0327]
Loss:[0.0286]
Loss:[0.0297]
Loss:[0.0350]
Loss:[0.0309]
Loss:[0.0272]
Loss:[0.0282]
Loss:[0.0259]
Loss:[0.0288]
Loss:[0.0278]
Loss:[0.0300]
Loss:[0.0294]
Loss:[0.0337]
Loss:[0.0292]
Loss:[0.0304]
Loss:[0.0265]
Loss:[0.0238]
Loss:[0.0246]
Loss:[0.0256]
Loss:[0.0271]
Loss:[0.0272]
Loss:[0.0255]
Loss:[0.0280]
Loss:[0.0254]
Loss:[0.0316]
Loss:[0.0229]
Loss:[0.0227]
Loss:[0.0289]
Loss:[0.0255]
Loss:[0.0260]
Loss:[0.0239]
Loss:[0.0202]
Loss:[0.0242]
Loss:[0.0243]
Loss:[0.0233]
Loss:[0.0236]
Loss:[0.0215]
Loss:[0.0255]
Loss:[0.0269]
Loss:[0.0240]
Loss:[0.0244]
Loss:[0.0251]
Loss:[0.0256]
Loss:[0.0215]
Loss:[0.0271]
Loss:[0.0241]
Loss:[0.0201]
Loss:[0.0201]
Loss:[0.0279]
Loss:[0.0236]
Loss:[0.0238]
Loss:[0.0222]
Loss:[0.0214]
Loss:[0.0244]
Loss:[0.0244]
Loss:[0.0214]
Loss:[0.0235]
Loss:[0.0226]
Loss:[0.0228]
Loss:[0.0185]
Loss:[0.0206]
Loss:[0.0234]
Loss:[0.0197]
Loss:[0.0206]
Loss:[0.0226]
Loss:[0.0283]
Loss:[0.0169]
Loss:[0.0179]
Loss:[0.0227]
Loss:[0.0194]
Loss:[0.0182]
Loss:[0.0199]
Loss:[0.0181]
Loss:[0.0180]
Loss:[0.0158]
Loss:[0.0221]
Loss:[0.0222]
Loss:[0.0229]
Loss:[0.0221]
Loss:[0.0180]
Loss:[0.0188]
Loss:[0.0168]
Loss:[0.0194]
Loss:[0.0184]
Loss:[0.0191]
Loss:[0.0207]
Loss:[0.0218]
Loss:[0.0153]
Loss:[0.0177]
Loss:[0.0155]
Loss:[0.0150]
Loss:[0.0152]
Loss:[0.0174]
Loss:[0.0191]
Loss:[0.0204]
Loss:[0.0168]
Loss:[0.0176]
Loss:[0.0190]
Loss:[0.0194]
Loss:[0.0156]
Loss:[0.0151]
Loss:[0.0208]
Loss:[0.0184]
Loss:[0.0171]
Loss:[0.0189]
Loss:[0.0190]
Loss:[0.0158]
Loss:[0.0161]
Loss:[0.0164]
Loss:[0.0136]
Loss:[0.0205]
Loss:[0.0175]
Loss:[0.0167]
Loss:[0.0127]
Loss:[0.0185]
Loss:[0.0150]
Loss:[0.0181]
Loss:[0.0175]
Loss:[0.0172]
Loss:[0.0162]
Loss:[0.0144]
Loss:[0.0144]
Loss:[0.0193]
Loss:[0.0209]
Loss:[0.0154]
Loss:[0.0170]
Loss:[0.0180]
Loss:[0.0146]
Loss:[0.0132]
Loss:[0.0142]
Loss:[0.0144]
Loss:[0.0184]
Loss:[0.0191]
Loss:[0.0177]
Early stopping!
Loading 345th epoch
acc:[0.3740]
acc:[0.3720]
acc:[0.3660]
acc:[0.3740]
acc:[0.3740]
acc:[0.3680]
acc:[0.3660]
acc:[0.3630]
acc:[0.3680]
acc:[0.3690]
acc:[0.3740]
acc:[0.3710]
acc:[0.3660]
acc:[0.3720]
acc:[0.3740]
acc:[0.3740]
acc:[0.3700]
acc:[0.3750]
acc:[0.3720]
acc:[0.3720]
acc:[0.3680]
acc:[0.3700]
acc:[0.3680]
acc:[0.3760]
acc:[0.3690]
acc:[0.3770]
acc:[0.3680]
acc:[0.3720]
acc:[0.3620]
acc:[0.3720]
acc:[0.3660]
acc:[0.3720]
acc:[0.3630]
acc:[0.3670]
acc:[0.3740]
acc:[0.3650]
acc:[0.3660]
acc:[0.3700]
acc:[0.3660]
acc:[0.3800]
acc:[0.3720]
acc:[0.3690]
acc:[0.3690]
acc:[0.3700]
acc:[0.3710]
acc:[0.3670]
acc:[0.3660]
acc:[0.3690]
acc:[0.3690]
acc:[0.3720]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.3700]
Mean:[36.9980]
Std :[0.3788]
----------------------------------------------------------------------------------------------------
