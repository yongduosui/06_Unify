----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='cora', drop_percent=0.02, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=39)
----------------------------------------------------------------------------------------------------
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.7000]
Loss:[0.6857]
Loss:[0.6900]
Loss:[0.6860]
Loss:[0.6778]
Loss:[0.6742]
Loss:[0.6731]
Loss:[0.6628]
Loss:[0.6573]
Loss:[0.6523]
Loss:[0.6374]
Loss:[0.6347]
Loss:[0.6209]
Loss:[0.6094]
Loss:[0.6003]
Loss:[0.5825]
Loss:[0.5729]
Loss:[0.5558]
Loss:[0.5440]
Loss:[0.5242]
Loss:[0.5138]
Loss:[0.4968]
Loss:[0.4833]
Loss:[0.4683]
Loss:[0.4550]
Loss:[0.4401]
Loss:[0.4254]
Loss:[0.4147]
Loss:[0.3960]
Loss:[0.3821]
Loss:[0.3716]
Loss:[0.3574]
Loss:[0.3418]
Loss:[0.3274]
Loss:[0.3205]
Loss:[0.3177]
Loss:[0.3066]
Loss:[0.2938]
Loss:[0.2802]
Loss:[0.2855]
Loss:[0.2707]
Loss:[0.2678]
Loss:[0.2574]
Loss:[0.2560]
Loss:[0.2442]
Loss:[0.2257]
Loss:[0.2336]
Loss:[0.2214]
Loss:[0.2224]
Loss:[0.2120]
Loss:[0.2114]
Loss:[0.2075]
Loss:[0.2046]
Loss:[0.1923]
Loss:[0.2000]
Loss:[0.1907]
Loss:[0.1789]
Loss:[0.1820]
Loss:[0.1635]
Loss:[0.1696]
Loss:[0.1913]
Loss:[0.1746]
Loss:[0.1579]
Loss:[0.1570]
Loss:[0.1496]
Loss:[0.1561]
Loss:[0.1463]
Loss:[0.1443]
Loss:[0.1539]
Loss:[0.1466]
Loss:[0.1478]
Loss:[0.1306]
Loss:[0.1369]
Loss:[0.1378]
Loss:[0.1367]
Loss:[0.1337]
Loss:[0.1385]
Loss:[0.1328]
Loss:[0.1249]
Loss:[0.1325]
Loss:[0.1251]
Loss:[0.1251]
Loss:[0.1205]
Loss:[0.1237]
Loss:[0.1177]
Loss:[0.1188]
Loss:[0.1125]
Loss:[0.1163]
Loss:[0.1126]
Loss:[0.1107]
Loss:[0.1044]
Loss:[0.1049]
Loss:[0.1033]
Loss:[0.1132]
Loss:[0.0970]
Loss:[0.1088]
Loss:[0.1078]
Loss:[0.1197]
Loss:[0.0930]
Loss:[0.1165]
Loss:[0.1139]
Loss:[0.1053]
Loss:[0.1318]
Loss:[0.1121]
Loss:[0.1317]
Loss:[0.1145]
Loss:[0.0902]
Loss:[0.0945]
Loss:[0.1037]
Loss:[0.1171]
Loss:[0.0853]
Loss:[0.1119]
Loss:[0.0829]
Loss:[0.0935]
Loss:[0.0847]
Loss:[0.0888]
Loss:[0.0770]
Loss:[0.0832]
Loss:[0.0828]
Loss:[0.0874]
Loss:[0.0741]
Loss:[0.0680]
Loss:[0.0773]
Loss:[0.0751]
Loss:[0.0680]
Loss:[0.0713]
Loss:[0.0720]
Loss:[0.0738]
Loss:[0.0712]
Loss:[0.0683]
Loss:[0.0681]
Loss:[0.0670]
Loss:[0.0779]
Loss:[0.0621]
Loss:[0.0653]
Loss:[0.0637]
Loss:[0.0637]
Loss:[0.0628]
Loss:[0.0615]
Loss:[0.0676]
Loss:[0.0576]
Loss:[0.0608]
Loss:[0.0674]
Loss:[0.0536]
Loss:[0.0591]
Loss:[0.0596]
Loss:[0.0560]
Loss:[0.0648]
Loss:[0.0577]
Loss:[0.0590]
Loss:[0.0613]
Loss:[0.0534]
Loss:[0.0561]
Loss:[0.0559]
Loss:[0.0554]
Loss:[0.0580]
Loss:[0.0578]
Loss:[0.0573]
Loss:[0.0481]
Loss:[0.0536]
Loss:[0.0440]
Loss:[0.0574]
Loss:[0.0475]
Loss:[0.0644]
Loss:[0.0515]
Loss:[0.0516]
Loss:[0.0551]
Loss:[0.0464]
Loss:[0.0527]
Loss:[0.0548]
Loss:[0.0474]
Loss:[0.0448]
Loss:[0.0462]
Loss:[0.0421]
Loss:[0.0531]
Loss:[0.0552]
Loss:[0.0489]
Loss:[0.0493]
Loss:[0.0430]
Loss:[0.0463]
Loss:[0.0414]
Loss:[0.0476]
Loss:[0.0366]
Loss:[0.0395]
Loss:[0.0363]
Loss:[0.0409]
Loss:[0.0404]
Loss:[0.0430]
Loss:[0.0411]
Loss:[0.0473]
Loss:[0.0447]
Loss:[0.0436]
Loss:[0.0400]
Loss:[0.0418]
Loss:[0.0397]
Loss:[0.0342]
Loss:[0.0447]
Loss:[0.0401]
Loss:[0.0419]
Loss:[0.0353]
Loss:[0.0368]
Loss:[0.0377]
Loss:[0.0426]
Loss:[0.0391]
Loss:[0.0361]
Loss:[0.0339]
Loss:[0.0295]
Loss:[0.0362]
Loss:[0.0377]
Loss:[0.0377]
Loss:[0.0390]
Loss:[0.0359]
Loss:[0.0373]
Loss:[0.0315]
Loss:[0.0355]
Loss:[0.0329]
Loss:[0.0349]
Loss:[0.0315]
Loss:[0.0432]
Loss:[0.0285]
Loss:[0.0344]
Loss:[0.0307]
Loss:[0.0351]
Loss:[0.0271]
Loss:[0.0309]
Loss:[0.0288]
Loss:[0.0281]
Loss:[0.0283]
Loss:[0.0358]
Loss:[0.0322]
Loss:[0.0290]
Loss:[0.0351]
Loss:[0.0320]
Loss:[0.0340]
Loss:[0.0331]
Loss:[0.0274]
Loss:[0.0377]
Loss:[0.0302]
Loss:[0.0245]
Loss:[0.0344]
Loss:[0.0256]
Loss:[0.0270]
Loss:[0.0297]
Loss:[0.0298]
Loss:[0.0298]
Loss:[0.0316]
Loss:[0.0355]
Loss:[0.0270]
Loss:[0.0233]
Loss:[0.0237]
Loss:[0.0318]
Loss:[0.0231]
Loss:[0.0234]
Loss:[0.0285]
Loss:[0.0314]
Loss:[0.0290]
Loss:[0.0290]
Loss:[0.0233]
Loss:[0.0229]
Loss:[0.0272]
Loss:[0.0271]
Loss:[0.0287]
Loss:[0.0230]
Loss:[0.0252]
Loss:[0.0285]
Loss:[0.0245]
Loss:[0.0243]
Loss:[0.0247]
Loss:[0.0280]
Loss:[0.0209]
Loss:[0.0245]
Loss:[0.0243]
Loss:[0.0237]
Loss:[0.0219]
Loss:[0.0295]
Loss:[0.0253]
Loss:[0.0261]
Loss:[0.0253]
Loss:[0.0233]
Loss:[0.0211]
Loss:[0.0216]
Loss:[0.0231]
Loss:[0.0221]
Loss:[0.0234]
Loss:[0.0259]
Loss:[0.0226]
Loss:[0.0252]
Loss:[0.0285]
Loss:[0.0212]
Loss:[0.0256]
Early stopping!
Loading 270th epoch
acc:[0.8130]
acc:[0.8130]
acc:[0.8140]
acc:[0.8140]
acc:[0.8130]
acc:[0.8130]
acc:[0.8120]
acc:[0.8150]
acc:[0.8150]
acc:[0.8150]
acc:[0.8160]
acc:[0.8160]
acc:[0.8130]
acc:[0.8140]
acc:[0.8120]
acc:[0.8140]
acc:[0.8120]
acc:[0.8130]
acc:[0.8120]
acc:[0.8120]
acc:[0.8130]
acc:[0.8110]
acc:[0.8160]
acc:[0.8120]
acc:[0.8140]
acc:[0.8130]
acc:[0.8160]
acc:[0.8150]
acc:[0.8130]
acc:[0.8140]
acc:[0.8170]
acc:[0.8130]
acc:[0.8120]
acc:[0.8180]
acc:[0.8140]
acc:[0.8140]
acc:[0.8130]
acc:[0.8140]
acc:[0.8160]
acc:[0.8170]
acc:[0.8130]
acc:[0.8150]
acc:[0.8130]
acc:[0.8160]
acc:[0.8150]
acc:[0.8130]
acc:[0.8140]
acc:[0.8160]
acc:[0.8150]
acc:[0.8130]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8140]
Mean:[81.3980]
Std :[0.1571]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='cora', drop_percent=0.05, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=39)
----------------------------------------------------------------------------------------------------
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.7000]
Loss:[0.6857]
Loss:[0.6900]
Loss:[0.6860]
Loss:[0.6778]
Loss:[0.6742]
Loss:[0.6731]
Loss:[0.6628]
Loss:[0.6573]
Loss:[0.6523]
Loss:[0.6374]
Loss:[0.6347]
Loss:[0.6209]
Loss:[0.6094]
Loss:[0.6003]
Loss:[0.5825]
Loss:[0.5729]
Loss:[0.5558]
Loss:[0.5440]
Loss:[0.5242]
Loss:[0.5138]
Loss:[0.4968]
Loss:[0.4833]
Loss:[0.4683]
Loss:[0.4550]
Loss:[0.4401]
Loss:[0.4253]
Loss:[0.4147]
Loss:[0.3960]
Loss:[0.3822]
Loss:[0.3715]
Loss:[0.3573]
Loss:[0.3419]
Loss:[0.3275]
Loss:[0.3204]
Loss:[0.3175]
Loss:[0.3066]
Loss:[0.2940]
Loss:[0.2802]
Loss:[0.2853]
Loss:[0.2708]
Loss:[0.2680]
Loss:[0.2574]
Loss:[0.2560]
Loss:[0.2444]
Loss:[0.2258]
Loss:[0.2335]
Loss:[0.2213]
Loss:[0.2224]
Loss:[0.2120]
Loss:[0.2115]
Loss:[0.2076]
Loss:[0.2047]
Loss:[0.1923]
Loss:[0.2000]
Loss:[0.1907]
Loss:[0.1789]
Loss:[0.1821]
Loss:[0.1638]
Loss:[0.1698]
Loss:[0.1910]
Loss:[0.1742]
Loss:[0.1578]
Loss:[0.1573]
Loss:[0.1499]
Loss:[0.1561]
Loss:[0.1463]
Loss:[0.1447]
Loss:[0.1542]
Loss:[0.1468]
Loss:[0.1478]
Loss:[0.1310]
Loss:[0.1374]
Loss:[0.1380]
Loss:[0.1364]
Loss:[0.1334]
Loss:[0.1384]
Loss:[0.1329]
Loss:[0.1251]
Loss:[0.1325]
Loss:[0.1246]
Loss:[0.1242]
Loss:[0.1203]
Loss:[0.1238]
Loss:[0.1176]
Loss:[0.1187]
Loss:[0.1126]
Loss:[0.1163]
Loss:[0.1124]
Loss:[0.1102]
Loss:[0.1046]
Loss:[0.1050]
Loss:[0.1027]
Loss:[0.1130]
Loss:[0.0975]
Loss:[0.1082]
Loss:[0.1065]
Loss:[0.1206]
Loss:[0.0939]
Loss:[0.1134]
Loss:[0.1111]
Loss:[0.1032]
Loss:[0.1244]
Loss:[0.1124]
Loss:[0.1226]
Loss:[0.1264]
Loss:[0.0864]
Loss:[0.0962]
Loss:[0.0919]
Loss:[0.1287]
Loss:[0.0877]
Loss:[0.1215]
Loss:[0.0805]
Loss:[0.1065]
Loss:[0.0923]
Loss:[0.1016]
Loss:[0.0757]
Loss:[0.1000]
Loss:[0.0871]
Loss:[0.1037]
Loss:[0.0739]
Loss:[0.0864]
Loss:[0.0796]
Loss:[0.0891]
Loss:[0.0690]
Loss:[0.0821]
Loss:[0.0722]
Loss:[0.0792]
Loss:[0.0753]
Loss:[0.0708]
Loss:[0.0714]
Loss:[0.0678]
Loss:[0.0839]
Loss:[0.0606]
Loss:[0.0688]
Loss:[0.0630]
Loss:[0.0663]
Loss:[0.0636]
Loss:[0.0644]
Loss:[0.0679]
Loss:[0.0581]
Loss:[0.0622]
Loss:[0.0673]
Loss:[0.0574]
Loss:[0.0600]
Loss:[0.0630]
Loss:[0.0559]
Loss:[0.0663]
Loss:[0.0588]
Loss:[0.0606]
Loss:[0.0628]
Loss:[0.0552]
Loss:[0.0571]
Loss:[0.0567]
Loss:[0.0567]
Loss:[0.0587]
Loss:[0.0592]
Loss:[0.0578]
Loss:[0.0490]
Loss:[0.0544]
Loss:[0.0445]
Loss:[0.0578]
Loss:[0.0472]
Loss:[0.0643]
Loss:[0.0521]
Loss:[0.0526]
Loss:[0.0558]
Loss:[0.0468]
Loss:[0.0529]
Loss:[0.0551]
Loss:[0.0479]
Loss:[0.0456]
Loss:[0.0472]
Loss:[0.0425]
Loss:[0.0535]
Loss:[0.0557]
Loss:[0.0500]
Loss:[0.0503]
Loss:[0.0438]
Loss:[0.0469]
Loss:[0.0421]
Loss:[0.0483]
Loss:[0.0372]
Loss:[0.0407]
Loss:[0.0372]
Loss:[0.0416]
Loss:[0.0412]
Loss:[0.0439]
Loss:[0.0420]
Loss:[0.0477]
Loss:[0.0457]
Loss:[0.0445]
Loss:[0.0408]
Loss:[0.0426]
Loss:[0.0397]
Loss:[0.0354]
Loss:[0.0444]
Loss:[0.0411]
Loss:[0.0418]
Loss:[0.0360]
Loss:[0.0363]
Loss:[0.0390]
Loss:[0.0423]
Loss:[0.0398]
Loss:[0.0368]
Loss:[0.0343]
Loss:[0.0300]
Loss:[0.0355]
Loss:[0.0384]
Loss:[0.0375]
Loss:[0.0402]
Loss:[0.0365]
Loss:[0.0380]
Loss:[0.0315]
Loss:[0.0368]
Loss:[0.0338]
Loss:[0.0361]
Loss:[0.0320]
Loss:[0.0440]
Loss:[0.0295]
Loss:[0.0357]
Loss:[0.0315]
Loss:[0.0361]
Loss:[0.0276]
Loss:[0.0318]
Loss:[0.0297]
Loss:[0.0288]
Loss:[0.0293]
Loss:[0.0370]
Loss:[0.0336]
Loss:[0.0297]
Loss:[0.0365]
Loss:[0.0330]
Loss:[0.0352]
Loss:[0.0341]
Loss:[0.0285]
Loss:[0.0396]
Loss:[0.0303]
Loss:[0.0252]
Loss:[0.0341]
Loss:[0.0259]
Loss:[0.0270]
Loss:[0.0299]
Loss:[0.0300]
Loss:[0.0306]
Loss:[0.0316]
Loss:[0.0359]
Loss:[0.0282]
Loss:[0.0232]
Loss:[0.0253]
Loss:[0.0323]
Loss:[0.0261]
Loss:[0.0243]
Loss:[0.0306]
Loss:[0.0327]
Loss:[0.0319]
Loss:[0.0301]
Loss:[0.0253]
Loss:[0.0242]
Loss:[0.0291]
Loss:[0.0292]
Loss:[0.0306]
Loss:[0.0245]
Loss:[0.0264]
Loss:[0.0296]
Loss:[0.0257]
Loss:[0.0258]
Loss:[0.0263]
Loss:[0.0293]
Early stopping!
Loading 249th epoch
acc:[0.8170]
acc:[0.8150]
acc:[0.8170]
acc:[0.8170]
acc:[0.8160]
acc:[0.8180]
acc:[0.8140]
acc:[0.8170]
acc:[0.8180]
acc:[0.8190]
acc:[0.8180]
acc:[0.8160]
acc:[0.8160]
acc:[0.8180]
acc:[0.8160]
acc:[0.8160]
acc:[0.8180]
acc:[0.8150]
acc:[0.8180]
acc:[0.8170]
acc:[0.8170]
acc:[0.8160]
acc:[0.8170]
acc:[0.8170]
acc:[0.8160]
acc:[0.8190]
acc:[0.8190]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
acc:[0.8190]
acc:[0.8180]
acc:[0.8160]
acc:[0.8190]
acc:[0.8170]
acc:[0.8210]
acc:[0.8170]
acc:[0.8140]
acc:[0.8180]
acc:[0.8190]
acc:[0.8140]
acc:[0.8190]
acc:[0.8160]
acc:[0.8190]
acc:[0.8190]
acc:[0.8170]
acc:[0.8180]
acc:[0.8180]
acc:[0.8160]
acc:[0.8180]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8172]
Mean:[81.7160]
Std :[0.1490]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='cora', drop_percent=0.08, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=39)
----------------------------------------------------------------------------------------------------
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.7000]
Loss:[0.6857]
Loss:[0.6900]
Loss:[0.6860]
Loss:[0.6778]
Loss:[0.6742]
Loss:[0.6731]
Loss:[0.6628]
Loss:[0.6573]
Loss:[0.6523]
Loss:[0.6374]
Loss:[0.6347]
Loss:[0.6209]
Loss:[0.6094]
Loss:[0.6003]
Loss:[0.5825]
Loss:[0.5729]
Loss:[0.5558]
Loss:[0.5440]
Loss:[0.5242]
Loss:[0.5138]
Loss:[0.4968]
Loss:[0.4833]
Loss:[0.4683]
Loss:[0.4550]
Loss:[0.4401]
Loss:[0.4254]
Loss:[0.4147]
Loss:[0.3959]
Loss:[0.3821]
Loss:[0.3716]
Loss:[0.3574]
Loss:[0.3418]
Loss:[0.3274]
Loss:[0.3205]
Loss:[0.3176]
Loss:[0.3066]
Loss:[0.2938]
Loss:[0.2802]
Loss:[0.2854]
Loss:[0.2707]
Loss:[0.2679]
Loss:[0.2574]
Loss:[0.2560]
Loss:[0.2443]
Loss:[0.2257]
Loss:[0.2336]
Loss:[0.2214]
Loss:[0.2224]
Loss:[0.2120]
Loss:[0.2115]
Loss:[0.2075]
Loss:[0.2046]
Loss:[0.1923]
Loss:[0.2000]
Loss:[0.1907]
Loss:[0.1789]
Loss:[0.1821]
Loss:[0.1636]
Loss:[0.1697]
Loss:[0.1912]
Loss:[0.1745]
Loss:[0.1578]
Loss:[0.1571]
Loss:[0.1497]
Loss:[0.1561]
Loss:[0.1463]
Loss:[0.1444]
Loss:[0.1539]
Loss:[0.1467]
Loss:[0.1478]
Loss:[0.1307]
Loss:[0.1370]
Loss:[0.1379]
Loss:[0.1367]
Loss:[0.1337]
Loss:[0.1385]
Loss:[0.1329]
Loss:[0.1250]
Loss:[0.1324]
Loss:[0.1250]
Loss:[0.1249]
Loss:[0.1205]
Loss:[0.1238]
Loss:[0.1177]
Loss:[0.1188]
Loss:[0.1126]
Loss:[0.1163]
Loss:[0.1125]
Loss:[0.1105]
Loss:[0.1044]
Loss:[0.1049]
Loss:[0.1030]
Loss:[0.1131]
Loss:[0.0972]
Loss:[0.1085]
Loss:[0.1075]
Loss:[0.1204]
Loss:[0.0932]
Loss:[0.1158]
Loss:[0.1137]
Loss:[0.1048]
Loss:[0.1310]
Loss:[0.1131]
Loss:[0.1308]
Loss:[0.1169]
Loss:[0.0884]
Loss:[0.0945]
Loss:[0.1011]
Loss:[0.1189]
Loss:[0.0838]
Loss:[0.1123]
Loss:[0.0813]
Loss:[0.0950]
Loss:[0.0842]
Loss:[0.0894]
Loss:[0.0759]
Loss:[0.0847]
Loss:[0.0826]
Loss:[0.0885]
Loss:[0.0740]
Loss:[0.0691]
Loss:[0.0779]
Loss:[0.0761]
Loss:[0.0680]
Loss:[0.0724]
Loss:[0.0722]
Loss:[0.0743]
Loss:[0.0711]
Loss:[0.0690]
Loss:[0.0685]
Loss:[0.0674]
Loss:[0.0779]
Loss:[0.0630]
Loss:[0.0655]
Loss:[0.0642]
Loss:[0.0639]
Loss:[0.0630]
Loss:[0.0615]
Loss:[0.0680]
Loss:[0.0578]
Loss:[0.0610]
Loss:[0.0675]
Loss:[0.0534]
Loss:[0.0590]
Loss:[0.0595]
Loss:[0.0559]
Loss:[0.0651]
Loss:[0.0577]
Loss:[0.0590]
Loss:[0.0614]
Loss:[0.0534]
Loss:[0.0562]
Loss:[0.0560]
Loss:[0.0554]
Loss:[0.0581]
Loss:[0.0578]
Loss:[0.0574]
Loss:[0.0482]
Loss:[0.0536]
Loss:[0.0441]
Loss:[0.0575]
Loss:[0.0474]
Loss:[0.0644]
Loss:[0.0515]
Loss:[0.0516]
Loss:[0.0551]
Loss:[0.0464]
Loss:[0.0527]
Loss:[0.0549]
Loss:[0.0474]
Loss:[0.0448]
Loss:[0.0462]
Loss:[0.0422]
Loss:[0.0531]
Loss:[0.0553]
Loss:[0.0489]
Loss:[0.0496]
Loss:[0.0431]
Loss:[0.0464]
Loss:[0.0415]
Loss:[0.0476]
Loss:[0.0367]
Loss:[0.0395]
Loss:[0.0364]
Loss:[0.0410]
Loss:[0.0404]
Loss:[0.0431]
Loss:[0.0412]
Loss:[0.0474]
Loss:[0.0448]
Loss:[0.0438]
Loss:[0.0400]
Loss:[0.0419]
Loss:[0.0397]
Loss:[0.0343]
Loss:[0.0446]
Loss:[0.0403]
Loss:[0.0419]
Loss:[0.0353]
Loss:[0.0367]
Loss:[0.0377]
Loss:[0.0426]
Loss:[0.0389]
Loss:[0.0362]
Loss:[0.0338]
Loss:[0.0297]
Loss:[0.0360]
Loss:[0.0378]
Loss:[0.0376]
Loss:[0.0391]
Loss:[0.0359]
Loss:[0.0373]
Loss:[0.0314]
Loss:[0.0356]
Loss:[0.0328]
Loss:[0.0349]
Loss:[0.0315]
Loss:[0.0432]
Loss:[0.0287]
Loss:[0.0344]
Loss:[0.0307]
Loss:[0.0352]
Loss:[0.0272]
Loss:[0.0309]
Loss:[0.0289]
Loss:[0.0281]
Loss:[0.0284]
Loss:[0.0359]
Loss:[0.0323]
Loss:[0.0290]
Loss:[0.0352]
Loss:[0.0319]
Loss:[0.0341]
Loss:[0.0332]
Loss:[0.0275]
Loss:[0.0377]
Loss:[0.0302]
Loss:[0.0245]
Loss:[0.0344]
Loss:[0.0255]
Loss:[0.0271]
Loss:[0.0296]
Loss:[0.0300]
Loss:[0.0298]
Loss:[0.0316]
Loss:[0.0355]
Loss:[0.0270]
Loss:[0.0234]
Loss:[0.0237]
Loss:[0.0319]
Loss:[0.0232]
Loss:[0.0234]
Loss:[0.0286]
Loss:[0.0313]
Loss:[0.0292]
Loss:[0.0291]
Loss:[0.0234]
Loss:[0.0230]
Loss:[0.0273]
Loss:[0.0272]
Loss:[0.0288]
Loss:[0.0231]
Loss:[0.0253]
Loss:[0.0285]
Loss:[0.0246]
Loss:[0.0244]
Loss:[0.0248]
Loss:[0.0281]
Loss:[0.0210]
Loss:[0.0246]
Loss:[0.0244]
Loss:[0.0238]
Loss:[0.0219]
Loss:[0.0296]
Loss:[0.0252]
Loss:[0.0261]
Loss:[0.0253]
Loss:[0.0234]
Loss:[0.0212]
Loss:[0.0217]
Loss:[0.0232]
Loss:[0.0220]
Loss:[0.0233]
Loss:[0.0260]
Loss:[0.0228]
Loss:[0.0254]
Loss:[0.0286]
Loss:[0.0212]
Loss:[0.0256]
Early stopping!
Loading 270th epoch
acc:[0.8150]
acc:[0.8140]
acc:[0.8150]
acc:[0.8150]
acc:[0.8120]
acc:[0.8150]
acc:[0.8150]
acc:[0.8130]
acc:[0.8150]
acc:[0.8150]
acc:[0.8170]
acc:[0.8160]
acc:[0.8140]
acc:[0.8130]
acc:[0.8130]
acc:[0.8150]
acc:[0.8140]
acc:[0.8120]
acc:[0.8150]
acc:[0.8120]
acc:[0.8150]
acc:[0.8110]
acc:[0.8140]
acc:[0.8150]
acc:[0.8160]
acc:[0.8150]
acc:[0.8130]
acc:[0.8140]
acc:[0.8140]
acc:[0.8130]
acc:[0.8150]
acc:[0.8150]
acc:[0.8140]
acc:[0.8160]
acc:[0.8150]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8130]
acc:[0.8160]
acc:[0.8130]
acc:[0.8140]
acc:[0.8120]
acc:[0.8150]
acc:[0.8160]
acc:[0.8140]
acc:[0.8140]
acc:[0.8140]
acc:[0.8140]
acc:[0.8160]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8143]
Mean:[81.4280]
Std :[0.1262]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='cora', drop_percent=0.1, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=39)
----------------------------------------------------------------------------------------------------
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.7000]
Loss:[0.6857]
Loss:[0.6900]
Loss:[0.6860]
Loss:[0.6778]
Loss:[0.6742]
Loss:[0.6731]
Loss:[0.6628]
Loss:[0.6573]
Loss:[0.6523]
Loss:[0.6374]
Loss:[0.6347]
Loss:[0.6209]
Loss:[0.6094]
Loss:[0.6003]
Loss:[0.5825]
Loss:[0.5729]
Loss:[0.5558]
Loss:[0.5440]
Loss:[0.5242]
Loss:[0.5138]
Loss:[0.4968]
Loss:[0.4833]
Loss:[0.4683]
Loss:[0.4550]
Loss:[0.4401]
Loss:[0.4254]
Loss:[0.4147]
Loss:[0.3960]
Loss:[0.3821]
Loss:[0.3716]
Loss:[0.3574]
Loss:[0.3418]
Loss:[0.3274]
Loss:[0.3205]
Loss:[0.3177]
Loss:[0.3066]
Loss:[0.2938]
Loss:[0.2802]
Loss:[0.2855]
Loss:[0.2707]
Loss:[0.2678]
Loss:[0.2574]
Loss:[0.2560]
Loss:[0.2443]
Loss:[0.2257]
Loss:[0.2336]
Loss:[0.2214]
Loss:[0.2224]
Loss:[0.2120]
Loss:[0.2115]
Loss:[0.2076]
Loss:[0.2046]
Loss:[0.1923]
Loss:[0.2000]
Loss:[0.1908]
Loss:[0.1789]
Loss:[0.1821]
Loss:[0.1636]
Loss:[0.1697]
Loss:[0.1911]
Loss:[0.1745]
Loss:[0.1578]
Loss:[0.1571]
Loss:[0.1497]
Loss:[0.1561]
Loss:[0.1463]
Loss:[0.1444]
Loss:[0.1539]
Loss:[0.1467]
Loss:[0.1478]
Loss:[0.1307]
Loss:[0.1369]
Loss:[0.1379]
Loss:[0.1367]
Loss:[0.1336]
Loss:[0.1386]
Loss:[0.1329]
Loss:[0.1250]
Loss:[0.1325]
Loss:[0.1251]
Loss:[0.1252]
Loss:[0.1205]
Loss:[0.1238]
Loss:[0.1177]
Loss:[0.1188]
Loss:[0.1125]
Loss:[0.1163]
Loss:[0.1126]
Loss:[0.1106]
Loss:[0.1044]
Loss:[0.1049]
Loss:[0.1032]
Loss:[0.1131]
Loss:[0.0971]
Loss:[0.1087]
Loss:[0.1077]
Loss:[0.1200]
Loss:[0.0931]
Loss:[0.1163]
Loss:[0.1138]
Loss:[0.1051]
Loss:[0.1316]
Loss:[0.1126]
Loss:[0.1313]
Loss:[0.1154]
Loss:[0.0894]
Loss:[0.0945]
Loss:[0.1026]
Loss:[0.1178]
Loss:[0.0846]
Loss:[0.1120]
Loss:[0.0822]
Loss:[0.0941]
Loss:[0.0844]
Loss:[0.0889]
Loss:[0.0765]
Loss:[0.0838]
Loss:[0.0826]
Loss:[0.0877]
Loss:[0.0740]
Loss:[0.0683]
Loss:[0.0775]
Loss:[0.0754]
Loss:[0.0680]
Loss:[0.0716]
Loss:[0.0721]
Loss:[0.0739]
Loss:[0.0712]
Loss:[0.0685]
Loss:[0.0683]
Loss:[0.0671]
Loss:[0.0779]
Loss:[0.0625]
Loss:[0.0654]
Loss:[0.0639]
Loss:[0.0638]
Loss:[0.0629]
Loss:[0.0615]
Loss:[0.0677]
Loss:[0.0577]
Loss:[0.0609]
Loss:[0.0676]
Loss:[0.0535]
Loss:[0.0591]
Loss:[0.0596]
Loss:[0.0560]
Loss:[0.0649]
Loss:[0.0578]
Loss:[0.0590]
Loss:[0.0614]
Loss:[0.0534]
Loss:[0.0562]
Loss:[0.0559]
Loss:[0.0554]
Loss:[0.0581]
Loss:[0.0579]
Loss:[0.0573]
Loss:[0.0481]
Loss:[0.0536]
Loss:[0.0441]
Loss:[0.0575]
Loss:[0.0474]
Loss:[0.0644]
Loss:[0.0515]
Loss:[0.0515]
Loss:[0.0551]
Loss:[0.0464]
Loss:[0.0527]
Loss:[0.0549]
Loss:[0.0474]
Loss:[0.0448]
Loss:[0.0461]
Loss:[0.0422]
Loss:[0.0531]
Loss:[0.0553]
Loss:[0.0488]
Loss:[0.0494]
Loss:[0.0431]
Loss:[0.0464]
Loss:[0.0414]
Loss:[0.0477]
Loss:[0.0366]
Loss:[0.0394]
Loss:[0.0364]
Loss:[0.0410]
Loss:[0.0404]
Loss:[0.0431]
Loss:[0.0411]
Loss:[0.0474]
Loss:[0.0448]
Loss:[0.0437]
Loss:[0.0400]
Loss:[0.0418]
Loss:[0.0398]
Loss:[0.0342]
Loss:[0.0447]
Loss:[0.0402]
Loss:[0.0420]
Loss:[0.0352]
Loss:[0.0368]
Loss:[0.0377]
Loss:[0.0427]
Loss:[0.0390]
Loss:[0.0361]
Loss:[0.0338]
Loss:[0.0296]
Loss:[0.0361]
Loss:[0.0377]
Loss:[0.0376]
Loss:[0.0391]
Loss:[0.0359]
Loss:[0.0373]
Loss:[0.0313]
Loss:[0.0356]
Loss:[0.0327]
Loss:[0.0348]
Loss:[0.0315]
Loss:[0.0431]
Loss:[0.0286]
Loss:[0.0344]
Loss:[0.0308]
Loss:[0.0352]
Loss:[0.0272]
Loss:[0.0308]
Loss:[0.0288]
Loss:[0.0281]
Loss:[0.0283]
Loss:[0.0357]
Loss:[0.0322]
Loss:[0.0290]
Loss:[0.0351]
Loss:[0.0319]
Loss:[0.0340]
Loss:[0.0331]
Loss:[0.0274]
Loss:[0.0377]
Loss:[0.0301]
Loss:[0.0245]
Loss:[0.0343]
Loss:[0.0256]
Loss:[0.0269]
Loss:[0.0296]
Loss:[0.0297]
Loss:[0.0299]
Loss:[0.0316]
Loss:[0.0356]
Loss:[0.0269]
Loss:[0.0234]
Loss:[0.0237]
Loss:[0.0319]
Loss:[0.0232]
Loss:[0.0234]
Loss:[0.0285]
Loss:[0.0313]
Loss:[0.0291]
Loss:[0.0291]
Loss:[0.0233]
Loss:[0.0229]
Loss:[0.0272]
Loss:[0.0271]
Loss:[0.0288]
Loss:[0.0231]
Loss:[0.0254]
Loss:[0.0285]
Loss:[0.0246]
Loss:[0.0243]
Loss:[0.0248]
Loss:[0.0280]
Loss:[0.0209]
Loss:[0.0245]
Loss:[0.0244]
Loss:[0.0237]
Loss:[0.0219]
Loss:[0.0295]
Loss:[0.0253]
Loss:[0.0261]
Loss:[0.0253]
Loss:[0.0234]
Loss:[0.0211]
Loss:[0.0217]
Loss:[0.0231]
Loss:[0.0219]
Loss:[0.0233]
Loss:[0.0259]
Loss:[0.0226]
Loss:[0.0254]
Loss:[0.0285]
Loss:[0.0212]
Loss:[0.0255]
Early stopping!
Loading 270th epoch
acc:[0.8130]
acc:[0.8130]
acc:[0.8150]
acc:[0.8140]
acc:[0.8090]
acc:[0.8150]
acc:[0.8140]
acc:[0.8160]
acc:[0.8150]
acc:[0.8160]
acc:[0.8150]
acc:[0.8150]
acc:[0.8150]
acc:[0.8140]
acc:[0.8120]
acc:[0.8140]
acc:[0.8120]
acc:[0.8110]
acc:[0.8160]
acc:[0.8120]
acc:[0.8140]
acc:[0.8130]
acc:[0.8120]
acc:[0.8110]
acc:[0.8150]
acc:[0.8130]
acc:[0.8110]
acc:[0.8150]
acc:[0.8120]
acc:[0.8140]
acc:[0.8150]
acc:[0.8150]
acc:[0.8140]
acc:[0.8150]
acc:[0.8140]
acc:[0.8130]
acc:[0.8150]
acc:[0.8130]
acc:[0.8110]
acc:[0.8160]
acc:[0.8130]
acc:[0.8130]
acc:[0.8120]
acc:[0.8150]
acc:[0.8160]
acc:[0.8130]
acc:[0.8140]
acc:[0.8140]
acc:[0.8140]
acc:[0.8140]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8137]
Mean:[81.3700]
Std :[0.1581]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='cora', drop_percent=0.2, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=39)
----------------------------------------------------------------------------------------------------
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.7000]
Loss:[0.6857]
Loss:[0.6900]
Loss:[0.6860]
Loss:[0.6778]
Loss:[0.6742]
Loss:[0.6731]
Loss:[0.6628]
Loss:[0.6573]
Loss:[0.6523]
Loss:[0.6374]
Loss:[0.6347]
Loss:[0.6209]
Loss:[0.6094]
Loss:[0.6003]
Loss:[0.5825]
Loss:[0.5729]
Loss:[0.5559]
Loss:[0.5440]
Loss:[0.5242]
Loss:[0.5138]
Loss:[0.4968]
Loss:[0.4833]
Loss:[0.4683]
Loss:[0.4550]
Loss:[0.4401]
Loss:[0.4254]
Loss:[0.4147]
Loss:[0.3960]
Loss:[0.3822]
Loss:[0.3716]
Loss:[0.3574]
Loss:[0.3419]
Loss:[0.3274]
Loss:[0.3205]
Loss:[0.3176]
Loss:[0.3066]
Loss:[0.2938]
Loss:[0.2802]
Loss:[0.2854]
Loss:[0.2707]
Loss:[0.2678]
Loss:[0.2574]
Loss:[0.2560]
Loss:[0.2442]
Loss:[0.2257]
Loss:[0.2336]
Loss:[0.2213]
Loss:[0.2224]
Loss:[0.2120]
Loss:[0.2115]
Loss:[0.2075]
Loss:[0.2046]
Loss:[0.1923]
Loss:[0.2000]
Loss:[0.1907]
Loss:[0.1789]
Loss:[0.1820]
Loss:[0.1636]
Loss:[0.1697]
Loss:[0.1912]
Loss:[0.1744]
Loss:[0.1578]
Loss:[0.1571]
Loss:[0.1498]
Loss:[0.1561]
Loss:[0.1462]
Loss:[0.1444]
Loss:[0.1540]
Loss:[0.1467]
Loss:[0.1478]
Loss:[0.1307]
Loss:[0.1370]
Loss:[0.1379]
Loss:[0.1366]
Loss:[0.1335]
Loss:[0.1384]
Loss:[0.1328]
Loss:[0.1250]
Loss:[0.1325]
Loss:[0.1250]
Loss:[0.1247]
Loss:[0.1203]
Loss:[0.1237]
Loss:[0.1177]
Loss:[0.1188]
Loss:[0.1125]
Loss:[0.1164]
Loss:[0.1126]
Loss:[0.1105]
Loss:[0.1044]
Loss:[0.1048]
Loss:[0.1031]
Loss:[0.1131]
Loss:[0.0972]
Loss:[0.1085]
Loss:[0.1075]
Loss:[0.1203]
Loss:[0.0932]
Loss:[0.1160]
Loss:[0.1136]
Loss:[0.1049]
Loss:[0.1310]
Loss:[0.1128]
Loss:[0.1309]
Loss:[0.1164]
Loss:[0.0886]
Loss:[0.0944]
Loss:[0.1014]
Loss:[0.1187]
Loss:[0.0839]
Loss:[0.1122]
Loss:[0.0814]
Loss:[0.0948]
Loss:[0.0842]
Loss:[0.0894]
Loss:[0.0759]
Loss:[0.0845]
Loss:[0.0825]
Loss:[0.0882]
Loss:[0.0739]
Loss:[0.0688]
Loss:[0.0777]
Loss:[0.0759]
Loss:[0.0680]
Loss:[0.0721]
Loss:[0.0721]
Loss:[0.0742]
Loss:[0.0710]
Loss:[0.0688]
Loss:[0.0683]
Loss:[0.0673]
Loss:[0.0779]
Loss:[0.0629]
Loss:[0.0655]
Loss:[0.0641]
Loss:[0.0638]
Loss:[0.0630]
Loss:[0.0615]
Loss:[0.0679]
Loss:[0.0576]
Loss:[0.0609]
Loss:[0.0675]
Loss:[0.0534]
Loss:[0.0589]
Loss:[0.0595]
Loss:[0.0559]
Loss:[0.0650]
Loss:[0.0577]
Loss:[0.0589]
Loss:[0.0615]
Loss:[0.0534]
Loss:[0.0561]
Loss:[0.0559]
Loss:[0.0554]
Loss:[0.0581]
Loss:[0.0577]
Loss:[0.0573]
Loss:[0.0481]
Loss:[0.0536]
Loss:[0.0440]
Loss:[0.0574]
Loss:[0.0474]
Loss:[0.0642]
Loss:[0.0514]
Loss:[0.0516]
Loss:[0.0550]
Loss:[0.0463]
Loss:[0.0527]
Loss:[0.0549]
Loss:[0.0474]
Loss:[0.0448]
Loss:[0.0461]
Loss:[0.0421]
Loss:[0.0531]
Loss:[0.0552]
Loss:[0.0489]
Loss:[0.0493]
Loss:[0.0430]
Loss:[0.0463]
Loss:[0.0415]
Loss:[0.0476]
Loss:[0.0365]
Loss:[0.0395]
Loss:[0.0364]
Loss:[0.0409]
Loss:[0.0405]
Loss:[0.0430]
Loss:[0.0411]
Loss:[0.0473]
Loss:[0.0447]
Loss:[0.0437]
Loss:[0.0400]
Loss:[0.0419]
Loss:[0.0397]
Loss:[0.0343]
Loss:[0.0446]
Loss:[0.0401]
Loss:[0.0418]
Loss:[0.0353]
Loss:[0.0366]
Loss:[0.0377]
Loss:[0.0426]
Loss:[0.0388]
Loss:[0.0362]
Loss:[0.0338]
Loss:[0.0296]
Loss:[0.0359]
Loss:[0.0378]
Loss:[0.0376]
Loss:[0.0391]
Loss:[0.0358]
Loss:[0.0373]
Loss:[0.0313]
Loss:[0.0354]
Loss:[0.0328]
Loss:[0.0349]
Loss:[0.0315]
Loss:[0.0431]
Loss:[0.0287]
Loss:[0.0343]
Loss:[0.0308]
Loss:[0.0352]
Loss:[0.0272]
Loss:[0.0309]
Loss:[0.0288]
Loss:[0.0281]
Loss:[0.0284]
Loss:[0.0359]
Loss:[0.0323]
Loss:[0.0290]
Loss:[0.0350]
Loss:[0.0319]
Loss:[0.0341]
Loss:[0.0331]
Loss:[0.0274]
Loss:[0.0376]
Loss:[0.0302]
Loss:[0.0244]
Loss:[0.0344]
Loss:[0.0256]
Loss:[0.0270]
Loss:[0.0296]
Loss:[0.0299]
Loss:[0.0298]
Loss:[0.0316]
Loss:[0.0355]
Loss:[0.0270]
Loss:[0.0233]
Loss:[0.0237]
Loss:[0.0317]
Loss:[0.0232]
Loss:[0.0233]
Loss:[0.0286]
Loss:[0.0313]
Loss:[0.0291]
Loss:[0.0291]
Loss:[0.0233]
Loss:[0.0229]
Loss:[0.0272]
Loss:[0.0271]
Loss:[0.0288]
Loss:[0.0230]
Loss:[0.0254]
Loss:[0.0283]
Loss:[0.0245]
Loss:[0.0243]
Loss:[0.0247]
Loss:[0.0280]
Loss:[0.0210]
Loss:[0.0246]
Loss:[0.0244]
Loss:[0.0238]
Loss:[0.0219]
Loss:[0.0296]
Loss:[0.0252]
Loss:[0.0261]
Loss:[0.0252]
Loss:[0.0234]
Loss:[0.0211]
Loss:[0.0217]
Loss:[0.0231]
Loss:[0.0221]
Loss:[0.0235]
Loss:[0.0260]
Loss:[0.0227]
Loss:[0.0253]
Loss:[0.0284]
Loss:[0.0213]
Loss:[0.0257]
Early stopping!
Loading 270th epoch
acc:[0.8140]
acc:[0.8120]
acc:[0.8140]
acc:[0.8140]
acc:[0.8120]
acc:[0.8140]
acc:[0.8130]
acc:[0.8130]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
acc:[0.8150]
acc:[0.8140]
acc:[0.8140]
acc:[0.8140]
acc:[0.8170]
acc:[0.8140]
acc:[0.8120]
acc:[0.8150]
acc:[0.8130]
acc:[0.8140]
acc:[0.8120]
acc:[0.8140]
acc:[0.8160]
acc:[0.8150]
acc:[0.8130]
acc:[0.8170]
acc:[0.8150]
acc:[0.8120]
acc:[0.8140]
acc:[0.8130]
acc:[0.8130]
acc:[0.8130]
acc:[0.8190]
acc:[0.8150]
acc:[0.8150]
acc:[0.8150]
acc:[0.8160]
acc:[0.8130]
acc:[0.8180]
acc:[0.8130]
acc:[0.8140]
acc:[0.8130]
acc:[0.8150]
acc:[0.8140]
acc:[0.8130]
acc:[0.8150]
acc:[0.8150]
acc:[0.8140]
acc:[0.8160]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8143]
Mean:[81.4340]
Std :[0.1586]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='cora', drop_percent=0.3, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=39)
----------------------------------------------------------------------------------------------------
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.7000]
Loss:[0.6857]
Loss:[0.6900]
Loss:[0.6860]
Loss:[0.6778]
Loss:[0.6742]
Loss:[0.6731]
Loss:[0.6628]
Loss:[0.6573]
Loss:[0.6523]
Loss:[0.6374]
Loss:[0.6347]
Loss:[0.6209]
Loss:[0.6094]
Loss:[0.6003]
Loss:[0.5825]
Loss:[0.5729]
Loss:[0.5559]
Loss:[0.5440]
Loss:[0.5242]
Loss:[0.5138]
Loss:[0.4968]
Loss:[0.4833]
Loss:[0.4683]
Loss:[0.4550]
Loss:[0.4401]
Loss:[0.4254]
Loss:[0.4147]
Loss:[0.3960]
Loss:[0.3822]
Loss:[0.3716]
Loss:[0.3574]
Loss:[0.3418]
Loss:[0.3275]
Loss:[0.3205]
Loss:[0.3177]
Loss:[0.3066]
Loss:[0.2938]
Loss:[0.2802]
Loss:[0.2854]
Loss:[0.2707]
Loss:[0.2678]
Loss:[0.2574]
Loss:[0.2560]
Loss:[0.2442]
Loss:[0.2257]
Loss:[0.2336]
Loss:[0.2214]
Loss:[0.2224]
Loss:[0.2120]
Loss:[0.2115]
Loss:[0.2075]
Loss:[0.2046]
Loss:[0.1923]
Loss:[0.2000]
Loss:[0.1907]
Loss:[0.1789]
Loss:[0.1820]
Loss:[0.1636]
Loss:[0.1697]
Loss:[0.1912]
Loss:[0.1744]
Loss:[0.1578]
Loss:[0.1571]
Loss:[0.1498]
Loss:[0.1561]
Loss:[0.1462]
Loss:[0.1444]
Loss:[0.1539]
Loss:[0.1467]
Loss:[0.1478]
Loss:[0.1308]
Loss:[0.1370]
Loss:[0.1378]
Loss:[0.1365]
Loss:[0.1335]
Loss:[0.1384]
Loss:[0.1327]
Loss:[0.1250]
Loss:[0.1324]
Loss:[0.1248]
Loss:[0.1246]
Loss:[0.1204]
Loss:[0.1237]
Loss:[0.1176]
Loss:[0.1187]
Loss:[0.1125]
Loss:[0.1163]
Loss:[0.1124]
Loss:[0.1103]
Loss:[0.1045]
Loss:[0.1049]
Loss:[0.1028]
Loss:[0.1130]
Loss:[0.0973]
Loss:[0.1083]
Loss:[0.1069]
Loss:[0.1207]
Loss:[0.0935]
Loss:[0.1148]
Loss:[0.1126]
Loss:[0.1041]
Loss:[0.1289]
Loss:[0.1135]
Loss:[0.1284]
Loss:[0.1205]
Loss:[0.0863]
Loss:[0.0944]
Loss:[0.0969]
Loss:[0.1223]
Loss:[0.0830]
Loss:[0.1140]
Loss:[0.0798]
Loss:[0.0983]
Loss:[0.0855]
Loss:[0.0920]
Loss:[0.0753]
Loss:[0.0881]
Loss:[0.0837]
Loss:[0.0916]
Loss:[0.0743]
Loss:[0.0722]
Loss:[0.0791]
Loss:[0.0791]
Loss:[0.0678]
Loss:[0.0755]
Loss:[0.0724]
Loss:[0.0763]
Loss:[0.0710]
Loss:[0.0715]
Loss:[0.0685]
Loss:[0.0690]
Loss:[0.0780]
Loss:[0.0653]
Loss:[0.0650]
Loss:[0.0656]
Loss:[0.0635]
Loss:[0.0638]
Loss:[0.0614]
Loss:[0.0688]
Loss:[0.0573]
Loss:[0.0616]
Loss:[0.0672]
Loss:[0.0533]
Loss:[0.0595]
Loss:[0.0596]
Loss:[0.0567]
Loss:[0.0653]
Loss:[0.0577]
Loss:[0.0590]
Loss:[0.0616]
Loss:[0.0534]
Loss:[0.0567]
Loss:[0.0559]
Loss:[0.0556]
Loss:[0.0582]
Loss:[0.0579]
Loss:[0.0574]
Loss:[0.0483]
Loss:[0.0537]
Loss:[0.0441]
Loss:[0.0576]
Loss:[0.0481]
Loss:[0.0641]
Loss:[0.0511]
Loss:[0.0518]
Loss:[0.0550]
Loss:[0.0464]
Loss:[0.0529]
Loss:[0.0550]
Loss:[0.0479]
Loss:[0.0450]
Loss:[0.0466]
Loss:[0.0422]
Loss:[0.0532]
Loss:[0.0554]
Loss:[0.0489]
Loss:[0.0494]
Loss:[0.0432]
Loss:[0.0466]
Loss:[0.0414]
Loss:[0.0476]
Loss:[0.0368]
Loss:[0.0397]
Loss:[0.0366]
Loss:[0.0410]
Loss:[0.0406]
Loss:[0.0432]
Loss:[0.0414]
Loss:[0.0475]
Loss:[0.0450]
Loss:[0.0439]
Loss:[0.0402]
Loss:[0.0421]
Loss:[0.0396]
Loss:[0.0346]
Loss:[0.0443]
Loss:[0.0405]
Loss:[0.0418]
Loss:[0.0353]
Loss:[0.0366]
Loss:[0.0379]
Loss:[0.0426]
Loss:[0.0388]
Loss:[0.0363]
Loss:[0.0337]
Loss:[0.0298]
Loss:[0.0354]
Loss:[0.0379]
Loss:[0.0372]
Loss:[0.0393]
Loss:[0.0359]
Loss:[0.0375]
Loss:[0.0310]
Loss:[0.0357]
Loss:[0.0329]
Loss:[0.0350]
Loss:[0.0314]
Loss:[0.0428]
Loss:[0.0290]
Loss:[0.0346]
Loss:[0.0309]
Loss:[0.0352]
Loss:[0.0274]
Loss:[0.0311]
Loss:[0.0290]
Loss:[0.0284]
Loss:[0.0286]
Loss:[0.0361]
Loss:[0.0323]
Loss:[0.0292]
Loss:[0.0353]
Loss:[0.0322]
Loss:[0.0341]
Loss:[0.0332]
Loss:[0.0277]
Loss:[0.0381]
Loss:[0.0301]
Loss:[0.0244]
Loss:[0.0344]
Loss:[0.0255]
Loss:[0.0272]
Loss:[0.0294]
Loss:[0.0301]
Loss:[0.0298]
Loss:[0.0318]
Loss:[0.0354]
Loss:[0.0273]
Loss:[0.0232]
Loss:[0.0240]
Loss:[0.0317]
Loss:[0.0237]
Loss:[0.0236]
Loss:[0.0288]
Loss:[0.0317]
Loss:[0.0294]
Loss:[0.0294]
Loss:[0.0237]
Loss:[0.0231]
Loss:[0.0276]
Loss:[0.0274]
Loss:[0.0292]
Loss:[0.0233]
Loss:[0.0257]
Loss:[0.0288]
Loss:[0.0248]
Loss:[0.0247]
Loss:[0.0250]
Loss:[0.0283]
Loss:[0.0213]
Loss:[0.0249]
Loss:[0.0245]
Loss:[0.0239]
Loss:[0.0221]
Loss:[0.0298]
Loss:[0.0255]
Loss:[0.0263]
Loss:[0.0257]
Loss:[0.0236]
Loss:[0.0215]
Loss:[0.0218]
Loss:[0.0233]
Loss:[0.0224]
Loss:[0.0237]
Loss:[0.0262]
Loss:[0.0230]
Loss:[0.0254]
Loss:[0.0288]
Loss:[0.0215]
Loss:[0.0257]
Early stopping!
Loading 270th epoch
acc:[0.8130]
acc:[0.8130]
acc:[0.8160]
acc:[0.8140]
acc:[0.8120]
acc:[0.8130]
acc:[0.8140]
acc:[0.8130]
acc:[0.8150]
acc:[0.8150]
acc:[0.8150]
acc:[0.8140]
acc:[0.8150]
acc:[0.8140]
acc:[0.8130]
acc:[0.8150]
acc:[0.8120]
acc:[0.8140]
acc:[0.8150]
acc:[0.8130]
acc:[0.8140]
acc:[0.8130]
acc:[0.8150]
acc:[0.8150]
acc:[0.8160]
acc:[0.8130]
acc:[0.8150]
acc:[0.8130]
acc:[0.8140]
acc:[0.8140]
acc:[0.8140]
acc:[0.8140]
acc:[0.8130]
acc:[0.8170]
acc:[0.8160]
acc:[0.8160]
acc:[0.8140]
acc:[0.8130]
acc:[0.8150]
acc:[0.8180]
acc:[0.8160]
acc:[0.8140]
acc:[0.8160]
acc:[0.8150]
acc:[0.8140]
acc:[0.8130]
acc:[0.8130]
acc:[0.8150]
acc:[0.8140]
acc:[0.8140]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8143]
Mean:[81.4280]
Std :[0.1262]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='cora', drop_percent=0.4, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=39)
----------------------------------------------------------------------------------------------------
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.7000]
Loss:[0.6857]
Loss:[0.6900]
Loss:[0.6860]
Loss:[0.6778]
Loss:[0.6742]
Loss:[0.6731]
Loss:[0.6628]
Loss:[0.6573]
Loss:[0.6523]
Loss:[0.6374]
Loss:[0.6347]
Loss:[0.6209]
Loss:[0.6094]
Loss:[0.6003]
Loss:[0.5825]
Loss:[0.5729]
Loss:[0.5559]
Loss:[0.5440]
Loss:[0.5242]
Loss:[0.5138]
Loss:[0.4968]
Loss:[0.4833]
Loss:[0.4683]
Loss:[0.4550]
Loss:[0.4402]
Loss:[0.4253]
Loss:[0.4147]
Loss:[0.3960]
Loss:[0.3822]
Loss:[0.3715]
Loss:[0.3574]
Loss:[0.3419]
Loss:[0.3275]
Loss:[0.3205]
Loss:[0.3176]
Loss:[0.3066]
Loss:[0.2938]
Loss:[0.2802]
Loss:[0.2855]
Loss:[0.2707]
Loss:[0.2678]
Loss:[0.2574]
Loss:[0.2560]
Loss:[0.2442]
Loss:[0.2257]
Loss:[0.2335]
Loss:[0.2214]
Loss:[0.2224]
Loss:[0.2120]
Loss:[0.2115]
Loss:[0.2075]
Loss:[0.2046]
Loss:[0.1923]
Loss:[0.2000]
Loss:[0.1907]
Loss:[0.1789]
Loss:[0.1820]
Loss:[0.1636]
Loss:[0.1697]
Loss:[0.1912]
Loss:[0.1745]
Loss:[0.1579]
Loss:[0.1570]
Loss:[0.1497]
Loss:[0.1561]
Loss:[0.1463]
Loss:[0.1443]
Loss:[0.1539]
Loss:[0.1467]
Loss:[0.1477]
Loss:[0.1306]
Loss:[0.1369]
Loss:[0.1378]
Loss:[0.1366]
Loss:[0.1336]
Loss:[0.1385]
Loss:[0.1328]
Loss:[0.1249]
Loss:[0.1324]
Loss:[0.1250]
Loss:[0.1251]
Loss:[0.1205]
Loss:[0.1237]
Loss:[0.1176]
Loss:[0.1188]
Loss:[0.1125]
Loss:[0.1163]
Loss:[0.1126]
Loss:[0.1107]
Loss:[0.1044]
Loss:[0.1049]
Loss:[0.1031]
Loss:[0.1131]
Loss:[0.0971]
Loss:[0.1085]
Loss:[0.1076]
Loss:[0.1200]
Loss:[0.0931]
Loss:[0.1163]
Loss:[0.1136]
Loss:[0.1051]
Loss:[0.1314]
Loss:[0.1122]
Loss:[0.1314]
Loss:[0.1153]
Loss:[0.0894]
Loss:[0.0945]
Loss:[0.1026]
Loss:[0.1177]
Loss:[0.0846]
Loss:[0.1120]
Loss:[0.0821]
Loss:[0.0941]
Loss:[0.0843]
Loss:[0.0890]
Loss:[0.0763]
Loss:[0.0839]
Loss:[0.0826]
Loss:[0.0878]
Loss:[0.0739]
Loss:[0.0684]
Loss:[0.0775]
Loss:[0.0755]
Loss:[0.0679]
Loss:[0.0717]
Loss:[0.0721]
Loss:[0.0739]
Loss:[0.0711]
Loss:[0.0685]
Loss:[0.0683]
Loss:[0.0671]
Loss:[0.0778]
Loss:[0.0625]
Loss:[0.0653]
Loss:[0.0638]
Loss:[0.0637]
Loss:[0.0628]
Loss:[0.0614]
Loss:[0.0677]
Loss:[0.0577]
Loss:[0.0608]
Loss:[0.0674]
Loss:[0.0534]
Loss:[0.0590]
Loss:[0.0594]
Loss:[0.0559]
Loss:[0.0648]
Loss:[0.0577]
Loss:[0.0589]
Loss:[0.0614]
Loss:[0.0534]
Loss:[0.0560]
Loss:[0.0559]
Loss:[0.0554]
Loss:[0.0579]
Loss:[0.0577]
Loss:[0.0572]
Loss:[0.0481]
Loss:[0.0535]
Loss:[0.0439]
Loss:[0.0573]
Loss:[0.0473]
Loss:[0.0643]
Loss:[0.0515]
Loss:[0.0515]
Loss:[0.0550]
Loss:[0.0462]
Loss:[0.0526]
Loss:[0.0549]
Loss:[0.0473]
Loss:[0.0448]
Loss:[0.0461]
Loss:[0.0421]
Loss:[0.0529]
Loss:[0.0553]
Loss:[0.0488]
Loss:[0.0493]
Loss:[0.0430]
Loss:[0.0463]
Loss:[0.0414]
Loss:[0.0474]
Loss:[0.0365]
Loss:[0.0394]
Loss:[0.0363]
Loss:[0.0408]
Loss:[0.0404]
Loss:[0.0430]
Loss:[0.0410]
Loss:[0.0472]
Loss:[0.0446]
Loss:[0.0436]
Loss:[0.0399]
Loss:[0.0418]
Loss:[0.0397]
Loss:[0.0342]
Loss:[0.0445]
Loss:[0.0401]
Loss:[0.0418]
Loss:[0.0352]
Loss:[0.0367]
Loss:[0.0377]
Loss:[0.0426]
Loss:[0.0388]
Loss:[0.0362]
Loss:[0.0338]
Loss:[0.0296]
Loss:[0.0359]
Loss:[0.0377]
Loss:[0.0376]
Loss:[0.0391]
Loss:[0.0357]
Loss:[0.0372]
Loss:[0.0314]
Loss:[0.0354]
Loss:[0.0327]
Loss:[0.0348]
Loss:[0.0315]
Loss:[0.0430]
Loss:[0.0286]
Loss:[0.0344]
Loss:[0.0308]
Loss:[0.0351]
Loss:[0.0271]
Loss:[0.0308]
Loss:[0.0289]
Loss:[0.0281]
Loss:[0.0283]
Loss:[0.0357]
Loss:[0.0321]
Loss:[0.0290]
Loss:[0.0350]
Loss:[0.0320]
Loss:[0.0339]
Loss:[0.0330]
Loss:[0.0274]
Loss:[0.0377]
Loss:[0.0301]
Loss:[0.0245]
Loss:[0.0345]
Loss:[0.0257]
Loss:[0.0271]
Loss:[0.0297]
Loss:[0.0300]
Loss:[0.0297]
Loss:[0.0316]
Loss:[0.0355]
Loss:[0.0269]
Loss:[0.0233]
Loss:[0.0236]
Loss:[0.0317]
Loss:[0.0231]
Loss:[0.0234]
Loss:[0.0284]
Loss:[0.0313]
Loss:[0.0289]
Loss:[0.0290]
Loss:[0.0232]
Loss:[0.0228]
Loss:[0.0271]
Loss:[0.0271]
Loss:[0.0287]
Loss:[0.0229]
Loss:[0.0253]
Loss:[0.0284]
Loss:[0.0244]
Loss:[0.0243]
Loss:[0.0247]
Loss:[0.0280]
Loss:[0.0210]
Loss:[0.0245]
Loss:[0.0244]
Loss:[0.0238]
Loss:[0.0219]
Loss:[0.0295]
Loss:[0.0252]
Loss:[0.0261]
Loss:[0.0253]
Loss:[0.0233]
Loss:[0.0211]
Loss:[0.0215]
Loss:[0.0231]
Loss:[0.0219]
Loss:[0.0233]
Loss:[0.0260]
Loss:[0.0226]
Loss:[0.0253]
Loss:[0.0285]
Loss:[0.0212]
Loss:[0.0256]
Early stopping!
Loading 270th epoch
acc:[0.8140]
acc:[0.8120]
acc:[0.8150]
acc:[0.8140]
acc:[0.8120]
acc:[0.8120]
acc:[0.8130]
acc:[0.8110]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8130]
acc:[0.8120]
acc:[0.8120]
acc:[0.8130]
acc:[0.8140]
acc:[0.8120]
acc:[0.8130]
acc:[0.8140]
acc:[0.8120]
acc:[0.8150]
acc:[0.8110]
acc:[0.8150]
acc:[0.8120]
acc:[0.8160]
acc:[0.8130]
acc:[0.8150]
acc:[0.8120]
acc:[0.8130]
acc:[0.8120]
acc:[0.8130]
acc:[0.8140]
acc:[0.8140]
acc:[0.8180]
acc:[0.8170]
acc:[0.8150]
acc:[0.8130]
acc:[0.8140]
acc:[0.8150]
acc:[0.8170]
acc:[0.8140]
acc:[0.8120]
acc:[0.8150]
acc:[0.8150]
acc:[0.8140]
acc:[0.8130]
acc:[0.8140]
acc:[0.8140]
acc:[0.8140]
acc:[0.8120]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8136]
Mean:[81.3640]
Std :[0.1535]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='cora', drop_percent=0.5, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=39)
----------------------------------------------------------------------------------------------------
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.7000]
Loss:[0.6857]
Loss:[0.6900]
Loss:[0.6860]
Loss:[0.6778]
Loss:[0.6742]
Loss:[0.6731]
Loss:[0.6628]
Loss:[0.6573]
Loss:[0.6523]
Loss:[0.6374]
Loss:[0.6347]
Loss:[0.6209]
Loss:[0.6094]
Loss:[0.6003]
Loss:[0.5825]
Loss:[0.5729]
Loss:[0.5559]
Loss:[0.5440]
Loss:[0.5242]
Loss:[0.5138]
Loss:[0.4968]
Loss:[0.4833]
Loss:[0.4683]
Loss:[0.4550]
Loss:[0.4402]
Loss:[0.4253]
Loss:[0.4146]
Loss:[0.3960]
Loss:[0.3822]
Loss:[0.3715]
Loss:[0.3573]
Loss:[0.3419]
Loss:[0.3275]
Loss:[0.3204]
Loss:[0.3174]
Loss:[0.3066]
Loss:[0.2940]
Loss:[0.2802]
Loss:[0.2853]
Loss:[0.2708]
Loss:[0.2680]
Loss:[0.2574]
Loss:[0.2560]
Loss:[0.2443]
Loss:[0.2257]
Loss:[0.2335]
Loss:[0.2213]
Loss:[0.2224]
Loss:[0.2120]
Loss:[0.2115]
Loss:[0.2076]
Loss:[0.2046]
Loss:[0.1923]
Loss:[0.2000]
Loss:[0.1907]
Loss:[0.1789]
Loss:[0.1820]
Loss:[0.1636]
Loss:[0.1697]
Loss:[0.1910]
Loss:[0.1742]
Loss:[0.1578]
Loss:[0.1572]
Loss:[0.1499]
Loss:[0.1561]
Loss:[0.1462]
Loss:[0.1445]
Loss:[0.1541]
Loss:[0.1467]
Loss:[0.1478]
Loss:[0.1309]
Loss:[0.1373]
Loss:[0.1380]
Loss:[0.1365]
Loss:[0.1334]
Loss:[0.1383]
Loss:[0.1329]
Loss:[0.1251]
Loss:[0.1325]
Loss:[0.1247]
Loss:[0.1243]
Loss:[0.1202]
Loss:[0.1237]
Loss:[0.1175]
Loss:[0.1187]
Loss:[0.1124]
Loss:[0.1163]
Loss:[0.1124]
Loss:[0.1101]
Loss:[0.1044]
Loss:[0.1049]
Loss:[0.1027]
Loss:[0.1130]
Loss:[0.0974]
Loss:[0.1081]
Loss:[0.1066]
Loss:[0.1205]
Loss:[0.0938]
Loss:[0.1135]
Loss:[0.1109]
Loss:[0.1033]
Loss:[0.1248]
Loss:[0.1124]
Loss:[0.1232]
Loss:[0.1255]
Loss:[0.0861]
Loss:[0.0957]
Loss:[0.0922]
Loss:[0.1278]
Loss:[0.0868]
Loss:[0.1202]
Loss:[0.0803]
Loss:[0.1053]
Loss:[0.0914]
Loss:[0.1000]
Loss:[0.0757]
Loss:[0.0981]
Loss:[0.0868]
Loss:[0.1018]
Loss:[0.0738]
Loss:[0.0842]
Loss:[0.0798]
Loss:[0.0879]
Loss:[0.0684]
Loss:[0.0821]
Loss:[0.0719]
Loss:[0.0796]
Loss:[0.0739]
Loss:[0.0721]
Loss:[0.0699]
Loss:[0.0684]
Loss:[0.0828]
Loss:[0.0614]
Loss:[0.0677]
Loss:[0.0632]
Loss:[0.0660]
Loss:[0.0628]
Loss:[0.0647]
Loss:[0.0672]
Loss:[0.0582]
Loss:[0.0615]
Loss:[0.0675]
Loss:[0.0563]
Loss:[0.0603]
Loss:[0.0626]
Loss:[0.0563]
Loss:[0.0660]
Loss:[0.0582]
Loss:[0.0605]
Loss:[0.0623]
Loss:[0.0556]
Loss:[0.0568]
Loss:[0.0568]
Loss:[0.0562]
Loss:[0.0588]
Loss:[0.0588]
Loss:[0.0576]
Loss:[0.0488]
Loss:[0.0540]
Loss:[0.0444]
Loss:[0.0577]
Loss:[0.0470]
Loss:[0.0638]
Loss:[0.0523]
Loss:[0.0523]
Loss:[0.0558]
Loss:[0.0468]
Loss:[0.0527]
Loss:[0.0551]
Loss:[0.0475]
Loss:[0.0455]
Loss:[0.0468]
Loss:[0.0424]
Loss:[0.0535]
Loss:[0.0556]
Loss:[0.0501]
Loss:[0.0502]
Loss:[0.0439]
Loss:[0.0468]
Loss:[0.0424]
Loss:[0.0482]
Loss:[0.0371]
Loss:[0.0405]
Loss:[0.0370]
Loss:[0.0415]
Loss:[0.0411]
Loss:[0.0438]
Loss:[0.0420]
Loss:[0.0476]
Loss:[0.0454]
Loss:[0.0444]
Loss:[0.0406]
Loss:[0.0424]
Loss:[0.0396]
Loss:[0.0351]
Loss:[0.0442]
Loss:[0.0409]
Loss:[0.0417]
Loss:[0.0359]
Loss:[0.0363]
Loss:[0.0387]
Loss:[0.0425]
Loss:[0.0394]
Loss:[0.0368]
Loss:[0.0343]
Loss:[0.0299]
Loss:[0.0353]
Loss:[0.0383]
Loss:[0.0374]
Loss:[0.0399]
Loss:[0.0363]
Loss:[0.0378]
Loss:[0.0313]
Loss:[0.0364]
Loss:[0.0336]
Loss:[0.0359]
Loss:[0.0319]
Loss:[0.0436]
Loss:[0.0294]
Loss:[0.0354]
Loss:[0.0313]
Loss:[0.0360]
Loss:[0.0275]
Loss:[0.0317]
Loss:[0.0295]
Loss:[0.0287]
Loss:[0.0291]
Loss:[0.0368]
Loss:[0.0334]
Loss:[0.0296]
Loss:[0.0363]
Loss:[0.0328]
Loss:[0.0350]
Loss:[0.0339]
Loss:[0.0283]
Loss:[0.0393]
Loss:[0.0303]
Loss:[0.0249]
Loss:[0.0342]
Loss:[0.0258]
Loss:[0.0271]
Loss:[0.0298]
Loss:[0.0301]
Loss:[0.0304]
Loss:[0.0315]
Loss:[0.0358]
Loss:[0.0280]
Loss:[0.0231]
Loss:[0.0249]
Loss:[0.0321]
Loss:[0.0257]
Loss:[0.0241]
Loss:[0.0303]
Loss:[0.0325]
Loss:[0.0315]
Loss:[0.0299]
Loss:[0.0249]
Loss:[0.0240]
Loss:[0.0288]
Loss:[0.0289]
Loss:[0.0302]
Loss:[0.0243]
Loss:[0.0264]
Loss:[0.0294]
Loss:[0.0254]
Loss:[0.0256]
Loss:[0.0261]
Loss:[0.0291]
Early stopping!
Loading 249th epoch
acc:[0.8160]
acc:[0.8150]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
acc:[0.8150]
acc:[0.8150]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
acc:[0.8170]
acc:[0.8150]
acc:[0.8140]
acc:[0.8160]
acc:[0.8150]
acc:[0.8170]
acc:[0.8150]
acc:[0.8150]
acc:[0.8150]
acc:[0.8150]
acc:[0.8160]
acc:[0.8160]
acc:[0.8180]
acc:[0.8160]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
acc:[0.8160]
acc:[0.8150]
acc:[0.8170]
acc:[0.8140]
acc:[0.8150]
acc:[0.8160]
acc:[0.8160]
acc:[0.8170]
acc:[0.8170]
acc:[0.8160]
acc:[0.8150]
acc:[0.8160]
acc:[0.8170]
acc:[0.8150]
acc:[0.8150]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
acc:[0.8160]
acc:[0.8160]
acc:[0.8180]
acc:[0.8160]
acc:[0.8160]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8159]
Mean:[81.5920]
Std :[0.0900]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='cora', drop_percent=0.6, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=39)
----------------------------------------------------------------------------------------------------
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.7000]
Loss:[0.6857]
Loss:[0.6900]
Loss:[0.6860]
Loss:[0.6778]
Loss:[0.6742]
Loss:[0.6731]
Loss:[0.6628]
Loss:[0.6573]
Loss:[0.6523]
Loss:[0.6374]
Loss:[0.6347]
Loss:[0.6209]
Loss:[0.6094]
Loss:[0.6003]
Loss:[0.5825]
Loss:[0.5729]
Loss:[0.5559]
Loss:[0.5440]
Loss:[0.5242]
Loss:[0.5138]
Loss:[0.4968]
Loss:[0.4833]
Loss:[0.4683]
Loss:[0.4550]
Loss:[0.4402]
Loss:[0.4253]
Loss:[0.4146]
Loss:[0.3960]
Loss:[0.3822]
Loss:[0.3714]
Loss:[0.3572]
Loss:[0.3419]
Loss:[0.3276]
Loss:[0.3203]
Loss:[0.3172]
Loss:[0.3065]
Loss:[0.2942]
Loss:[0.2802]
Loss:[0.2850]
Loss:[0.2708]
Loss:[0.2682]
Loss:[0.2574]
Loss:[0.2560]
Loss:[0.2445]
Loss:[0.2258]
Loss:[0.2335]
Loss:[0.2213]
Loss:[0.2225]
Loss:[0.2120]
Loss:[0.2115]
Loss:[0.2077]
Loss:[0.2047]
Loss:[0.1924]
Loss:[0.2000]
Loss:[0.1908]
Loss:[0.1789]
Loss:[0.1822]
Loss:[0.1638]
Loss:[0.1699]
Loss:[0.1907]
Loss:[0.1738]
Loss:[0.1577]
Loss:[0.1575]
Loss:[0.1501]
Loss:[0.1560]
Loss:[0.1463]
Loss:[0.1449]
Loss:[0.1544]
Loss:[0.1468]
Loss:[0.1479]
Loss:[0.1312]
Loss:[0.1379]
Loss:[0.1381]
Loss:[0.1361]
Loss:[0.1331]
Loss:[0.1383]
Loss:[0.1329]
Loss:[0.1252]
Loss:[0.1323]
Loss:[0.1240]
Loss:[0.1232]
Loss:[0.1201]
Loss:[0.1238]
Loss:[0.1175]
Loss:[0.1184]
Loss:[0.1124]
Loss:[0.1163]
Loss:[0.1120]
Loss:[0.1095]
Loss:[0.1045]
Loss:[0.1051]
Loss:[0.1022]
Loss:[0.1126]
Loss:[0.0976]
Loss:[0.1077]
Loss:[0.1046]
Loss:[0.1191]
Loss:[0.0955]
Loss:[0.1082]
Loss:[0.1033]
Loss:[0.1012]
Loss:[0.1058]
Loss:[0.0997]
Loss:[0.1032]
Loss:[0.1256]
Loss:[0.1096]
Loss:[0.0968]
Loss:[0.1055]
Loss:[0.1024]
Loss:[0.0957]
Loss:[0.1010]
Loss:[0.0987]
Loss:[0.0817]
Loss:[0.0950]
Loss:[0.0808]
Loss:[0.0867]
Loss:[0.0789]
Loss:[0.0922]
Loss:[0.0829]
Loss:[0.0875]
Loss:[0.0688]
Loss:[0.0902]
Loss:[0.0726]
Loss:[0.0826]
Loss:[0.0736]
Loss:[0.0861]
Loss:[0.0740]
Loss:[0.0835]
Loss:[0.0707]
Loss:[0.0813]
Loss:[0.0657]
Loss:[0.0888]
Loss:[0.0608]
Loss:[0.0761]
Loss:[0.0626]
Loss:[0.0752]
Loss:[0.0631]
Loss:[0.0678]
Loss:[0.0673]
Loss:[0.0665]
Loss:[0.0613]
Loss:[0.0744]
Loss:[0.0530]
Loss:[0.0614]
Loss:[0.0585]
Loss:[0.0560]
Loss:[0.0666]
Loss:[0.0594]
Loss:[0.0594]
Loss:[0.0627]
Loss:[0.0536]
Loss:[0.0553]
Loss:[0.0569]
Loss:[0.0556]
Loss:[0.0589]
Loss:[0.0571]
Loss:[0.0584]
Loss:[0.0479]
Loss:[0.0546]
Loss:[0.0437]
Loss:[0.0573]
Loss:[0.0475]
Loss:[0.0645]
Loss:[0.0508]
Loss:[0.0518]
Loss:[0.0543]
Loss:[0.0460]
Loss:[0.0529]
Loss:[0.0544]
Loss:[0.0481]
Loss:[0.0444]
Loss:[0.0470]
Loss:[0.0419]
Loss:[0.0533]
Loss:[0.0551]
Loss:[0.0485]
Loss:[0.0494]
Loss:[0.0429]
Loss:[0.0461]
Loss:[0.0408]
Loss:[0.0473]
Loss:[0.0366]
Loss:[0.0398]
Loss:[0.0364]
Loss:[0.0411]
Loss:[0.0405]
Loss:[0.0431]
Loss:[0.0414]
Loss:[0.0471]
Loss:[0.0450]
Loss:[0.0437]
Loss:[0.0401]
Loss:[0.0423]
Loss:[0.0391]
Loss:[0.0349]
Loss:[0.0439]
Loss:[0.0405]
Loss:[0.0410]
Loss:[0.0352]
Loss:[0.0359]
Loss:[0.0380]
Loss:[0.0418]
Loss:[0.0391]
Loss:[0.0359]
Loss:[0.0335]
Loss:[0.0294]
Loss:[0.0346]
Loss:[0.0378]
Loss:[0.0370]
Loss:[0.0394]
Loss:[0.0359]
Loss:[0.0373]
Loss:[0.0309]
Loss:[0.0357]
Loss:[0.0329]
Loss:[0.0353]
Loss:[0.0312]
Loss:[0.0433]
Loss:[0.0292]
Loss:[0.0348]
Loss:[0.0309]
Loss:[0.0356]
Loss:[0.0271]
Loss:[0.0311]
Loss:[0.0291]
Loss:[0.0282]
Loss:[0.0286]
Loss:[0.0364]
Loss:[0.0325]
Loss:[0.0292]
Loss:[0.0358]
Loss:[0.0321]
Loss:[0.0345]
Loss:[0.0335]
Loss:[0.0279]
Loss:[0.0386]
Loss:[0.0301]
Loss:[0.0244]
Loss:[0.0339]
Loss:[0.0253]
Loss:[0.0269]
Loss:[0.0292]
Loss:[0.0299]
Loss:[0.0300]
Loss:[0.0316]
Loss:[0.0353]
Loss:[0.0275]
Loss:[0.0228]
Loss:[0.0246]
Loss:[0.0315]
Loss:[0.0246]
Loss:[0.0238]
Loss:[0.0295]
Loss:[0.0320]
Loss:[0.0303]
Loss:[0.0297]
Loss:[0.0244]
Loss:[0.0234]
Loss:[0.0284]
Loss:[0.0281]
Loss:[0.0297]
Loss:[0.0237]
Loss:[0.0261]
Loss:[0.0290]
Loss:[0.0250]
Loss:[0.0253]
Loss:[0.0256]
Loss:[0.0289]
Early stopping!
Loading 249th epoch
acc:[0.8160]
acc:[0.8160]
acc:[0.8160]
acc:[0.8160]
acc:[0.8170]
acc:[0.8180]
acc:[0.8160]
acc:[0.8190]
acc:[0.8180]
acc:[0.8180]
acc:[0.8190]
acc:[0.8160]
acc:[0.8170]
acc:[0.8180]
acc:[0.8150]
acc:[0.8150]
acc:[0.8160]
acc:[0.8170]
acc:[0.8190]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
acc:[0.8160]
acc:[0.8130]
acc:[0.8140]
acc:[0.8170]
acc:[0.8170]
acc:[0.8170]
acc:[0.8180]
acc:[0.8170]
acc:[0.8170]
acc:[0.8140]
acc:[0.8150]
acc:[0.8160]
acc:[0.8180]
acc:[0.8170]
acc:[0.8150]
acc:[0.8160]
acc:[0.8160]
acc:[0.8160]
acc:[0.8140]
acc:[0.8160]
acc:[0.8170]
acc:[0.8150]
acc:[0.8160]
acc:[0.8190]
acc:[0.8160]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8164]
Mean:[81.6440]
Std :[0.1343]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='cora', drop_percent=0.02, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=39)
----------------------------------------------------------------------------------------------------
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.7000]
Loss:[0.6857]
Loss:[0.6900]
Loss:[0.6860]
Loss:[0.6778]
Loss:[0.6742]
Loss:[0.6731]
Loss:[0.6628]
Loss:[0.6573]
Loss:[0.6523]
Loss:[0.6374]
Loss:[0.6347]
Loss:[0.6209]
Loss:[0.6094]
Loss:[0.6003]
Loss:[0.5825]
Loss:[0.5729]
Loss:[0.5558]
Loss:[0.5440]
Loss:[0.5242]
Loss:[0.5138]
Loss:[0.4968]
Loss:[0.4833]
Loss:[0.4683]
Loss:[0.4550]
Loss:[0.4401]
Loss:[0.4254]
Loss:[0.4148]
Loss:[0.3960]
Loss:[0.3821]
Loss:[0.3717]
Loss:[0.3575]
Loss:[0.3418]
Loss:[0.3274]
Loss:[0.3205]
Loss:[0.3178]
Loss:[0.3066]
Loss:[0.2937]
Loss:[0.2802]
Loss:[0.2856]
Loss:[0.2706]
Loss:[0.2676]
Loss:[0.2574]
Loss:[0.2560]
Loss:[0.2441]
Loss:[0.2256]
Loss:[0.2336]
Loss:[0.2214]
Loss:[0.2224]
Loss:[0.2120]
Loss:[0.2114]
Loss:[0.2075]
Loss:[0.2046]
Loss:[0.1923]
Loss:[0.2000]
Loss:[0.1907]
Loss:[0.1789]
Loss:[0.1820]
Loss:[0.1634]
Loss:[0.1695]
Loss:[0.1914]
Loss:[0.1748]
Loss:[0.1579]
Loss:[0.1569]
Loss:[0.1495]
Loss:[0.1561]
Loss:[0.1463]
Loss:[0.1442]
Loss:[0.1537]
Loss:[0.1466]
Loss:[0.1477]
Loss:[0.1305]
Loss:[0.1365]
Loss:[0.1377]
Loss:[0.1367]
Loss:[0.1339]
Loss:[0.1387]
Loss:[0.1328]
Loss:[0.1248]
Loss:[0.1323]
Loss:[0.1253]
Loss:[0.1258]
Loss:[0.1208]
Loss:[0.1237]
Loss:[0.1177]
Loss:[0.1190]
Loss:[0.1125]
Loss:[0.1163]
Loss:[0.1127]
Loss:[0.1109]
Loss:[0.1043]
Loss:[0.1048]
Loss:[0.1037]
Loss:[0.1132]
Loss:[0.0969]
Loss:[0.1091]
Loss:[0.1081]
Loss:[0.1188]
Loss:[0.0928]
Loss:[0.1172]
Loss:[0.1140]
Loss:[0.1059]
Loss:[0.1322]
Loss:[0.1107]
Loss:[0.1322]
Loss:[0.1120]
Loss:[0.0922]
Loss:[0.0949]
Loss:[0.1064]
Loss:[0.1153]
Loss:[0.0875]
Loss:[0.1121]
Loss:[0.0851]
Loss:[0.0921]
Loss:[0.0862]
Loss:[0.0886]
Loss:[0.0786]
Loss:[0.0819]
Loss:[0.0840]
Loss:[0.0869]
Loss:[0.0748]
Loss:[0.0671]
Loss:[0.0775]
Loss:[0.0744]
Loss:[0.0684]
Loss:[0.0704]
Loss:[0.0722]
Loss:[0.0734]
Loss:[0.0716]
Loss:[0.0677]
Loss:[0.0680]
Loss:[0.0667]
Loss:[0.0782]
Loss:[0.0615]
Loss:[0.0651]
Loss:[0.0633]
Loss:[0.0635]
Loss:[0.0628]
Loss:[0.0616]
Loss:[0.0673]
Loss:[0.0575]
Loss:[0.0607]
Loss:[0.0673]
Loss:[0.0538]
Loss:[0.0594]
Loss:[0.0598]
Loss:[0.0564]
Loss:[0.0646]
Loss:[0.0579]
Loss:[0.0589]
Loss:[0.0612]
Loss:[0.0536]
Loss:[0.0563]
Loss:[0.0560]
Loss:[0.0554]
Loss:[0.0581]
Loss:[0.0579]
Loss:[0.0575]
Loss:[0.0481]
Loss:[0.0536]
Loss:[0.0442]
Loss:[0.0575]
Loss:[0.0478]
Loss:[0.0646]
Loss:[0.0513]
Loss:[0.0515]
Loss:[0.0551]
Loss:[0.0464]
Loss:[0.0528]
Loss:[0.0550]
Loss:[0.0476]
Loss:[0.0449]
Loss:[0.0462]
Loss:[0.0421]
Loss:[0.0531]
Loss:[0.0552]
Loss:[0.0487]
Loss:[0.0494]
Loss:[0.0430]
Loss:[0.0464]
Loss:[0.0414]
Loss:[0.0476]
Loss:[0.0366]
Loss:[0.0394]
Loss:[0.0364]
Loss:[0.0410]
Loss:[0.0404]
Loss:[0.0430]
Loss:[0.0412]
Loss:[0.0473]
Loss:[0.0448]
Loss:[0.0436]
Loss:[0.0401]
Loss:[0.0418]
Loss:[0.0397]
Loss:[0.0342]
Loss:[0.0447]
Loss:[0.0402]
Loss:[0.0420]
Loss:[0.0354]
Loss:[0.0368]
Loss:[0.0377]
Loss:[0.0427]
Loss:[0.0390]
Loss:[0.0362]
Loss:[0.0339]
Loss:[0.0295]
Loss:[0.0362]
Loss:[0.0377]
Loss:[0.0377]
Loss:[0.0390]
Loss:[0.0360]
Loss:[0.0372]
Loss:[0.0315]
Loss:[0.0356]
Loss:[0.0328]
Loss:[0.0348]
Loss:[0.0315]
Loss:[0.0431]
Loss:[0.0286]
Loss:[0.0343]
Loss:[0.0308]
Loss:[0.0350]
Loss:[0.0271]
Loss:[0.0308]
Loss:[0.0289]
Loss:[0.0281]
Loss:[0.0283]
Loss:[0.0358]
Loss:[0.0321]
Loss:[0.0290]
Loss:[0.0351]
Loss:[0.0319]
Loss:[0.0341]
Loss:[0.0332]
Loss:[0.0275]
Loss:[0.0376]
Loss:[0.0302]
Loss:[0.0245]
Loss:[0.0345]
Loss:[0.0256]
Loss:[0.0270]
Loss:[0.0298]
Loss:[0.0298]
Loss:[0.0298]
Loss:[0.0315]
Loss:[0.0357]
Loss:[0.0269]
Loss:[0.0233]
Loss:[0.0237]
Loss:[0.0319]
Loss:[0.0231]
Loss:[0.0235]
Loss:[0.0284]
Loss:[0.0313]
Loss:[0.0289]
Loss:[0.0291]
Loss:[0.0233]
Loss:[0.0228]
Loss:[0.0271]
Loss:[0.0271]
Loss:[0.0288]
Loss:[0.0230]
Loss:[0.0252]
Loss:[0.0285]
Loss:[0.0245]
Loss:[0.0242]
Loss:[0.0247]
Loss:[0.0280]
Loss:[0.0209]
Loss:[0.0245]
Loss:[0.0243]
Loss:[0.0236]
Loss:[0.0219]
Loss:[0.0294]
Loss:[0.0251]
Loss:[0.0260]
Loss:[0.0252]
Loss:[0.0233]
Loss:[0.0211]
Loss:[0.0216]
Loss:[0.0231]
Loss:[0.0221]
Loss:[0.0233]
Loss:[0.0259]
Loss:[0.0226]
Loss:[0.0252]
Loss:[0.0285]
Loss:[0.0212]
Loss:[0.0255]
Early stopping!
Loading 270th epoch
acc:[0.8140]
acc:[0.8140]
acc:[0.8130]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8140]
acc:[0.8160]
acc:[0.8150]
acc:[0.8170]
acc:[0.8140]
acc:[0.8140]
acc:[0.8140]
acc:[0.8160]
acc:[0.8140]
acc:[0.8150]
acc:[0.8150]
acc:[0.8120]
acc:[0.8150]
acc:[0.8150]
acc:[0.8170]
acc:[0.8130]
acc:[0.8140]
acc:[0.8150]
acc:[0.8160]
acc:[0.8140]
acc:[0.8130]
acc:[0.8150]
acc:[0.8140]
acc:[0.8140]
acc:[0.8130]
acc:[0.8140]
acc:[0.8150]
acc:[0.8160]
acc:[0.8150]
acc:[0.8130]
acc:[0.8150]
acc:[0.8160]
acc:[0.8130]
acc:[0.8150]
acc:[0.8140]
acc:[0.8150]
acc:[0.8130]
acc:[0.8150]
acc:[0.8150]
acc:[0.8160]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8160]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8145]
Mean:[81.4540]
Std :[0.1092]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='cora', drop_percent=0.05, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=39)
----------------------------------------------------------------------------------------------------
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.7000]
Loss:[0.6857]
Loss:[0.6900]
Loss:[0.6860]
Loss:[0.6778]
Loss:[0.6742]
Loss:[0.6731]
Loss:[0.6628]
Loss:[0.6573]
Loss:[0.6523]
Loss:[0.6374]
Loss:[0.6347]
Loss:[0.6209]
Loss:[0.6094]
Loss:[0.6003]
Loss:[0.5825]
Loss:[0.5729]
Loss:[0.5558]
Loss:[0.5440]
Loss:[0.5242]
Loss:[0.5138]
Loss:[0.4968]
Loss:[0.4833]
Loss:[0.4683]
Loss:[0.4550]
Loss:[0.4401]
Loss:[0.4254]
Loss:[0.4148]
Loss:[0.3960]
Loss:[0.3821]
Loss:[0.3717]
Loss:[0.3575]
Loss:[0.3418]
Loss:[0.3274]
Loss:[0.3205]
Loss:[0.3178]
Loss:[0.3066]
Loss:[0.2937]
Loss:[0.2802]
Loss:[0.2856]
Loss:[0.2706]
Loss:[0.2676]
Loss:[0.2574]
Loss:[0.2560]
Loss:[0.2441]
Loss:[0.2256]
Loss:[0.2336]
Loss:[0.2214]
Loss:[0.2224]
Loss:[0.2120]
Loss:[0.2114]
Loss:[0.2075]
Loss:[0.2046]
Loss:[0.1923]
Loss:[0.2000]
Loss:[0.1907]
Loss:[0.1789]
Loss:[0.1820]
Loss:[0.1634]
Loss:[0.1695]
Loss:[0.1914]
Loss:[0.1748]
Loss:[0.1579]
Loss:[0.1569]
Loss:[0.1495]
Loss:[0.1561]
Loss:[0.1463]
Loss:[0.1442]
Loss:[0.1537]
Loss:[0.1466]
Loss:[0.1477]
Loss:[0.1305]
Loss:[0.1365]
Loss:[0.1377]
Loss:[0.1367]
Loss:[0.1339]
Loss:[0.1387]
Loss:[0.1328]
Loss:[0.1248]
Loss:[0.1323]
Loss:[0.1253]
Loss:[0.1258]
Loss:[0.1208]
Loss:[0.1237]
Loss:[0.1177]
Loss:[0.1190]
Loss:[0.1125]
Loss:[0.1163]
Loss:[0.1127]
Loss:[0.1109]
Loss:[0.1043]
Loss:[0.1048]
Loss:[0.1037]
Loss:[0.1132]
Loss:[0.0969]
Loss:[0.1091]
Loss:[0.1081]
Loss:[0.1188]
Loss:[0.0928]
Loss:[0.1172]
Loss:[0.1140]
Loss:[0.1059]
Loss:[0.1322]
Loss:[0.1107]
Loss:[0.1322]
Loss:[0.1120]
Loss:[0.0922]
Loss:[0.0949]
Loss:[0.1064]
Loss:[0.1153]
Loss:[0.0875]
Loss:[0.1121]
Loss:[0.0851]
Loss:[0.0921]
Loss:[0.0862]
Loss:[0.0886]
Loss:[0.0786]
Loss:[0.0819]
Loss:[0.0840]
Loss:[0.0869]
Loss:[0.0748]
Loss:[0.0671]
Loss:[0.0775]
Loss:[0.0744]
Loss:[0.0684]
Loss:[0.0704]
Loss:[0.0722]
Loss:[0.0734]
Loss:[0.0716]
Loss:[0.0677]
Loss:[0.0680]
Loss:[0.0667]
Loss:[0.0782]
Loss:[0.0615]
Loss:[0.0651]
Loss:[0.0633]
Loss:[0.0635]
Loss:[0.0628]
Loss:[0.0616]
Loss:[0.0673]
Loss:[0.0575]
Loss:[0.0607]
Loss:[0.0673]
Loss:[0.0538]
Loss:[0.0594]
Loss:[0.0598]
Loss:[0.0564]
Loss:[0.0646]
Loss:[0.0579]
Loss:[0.0589]
Loss:[0.0612]
Loss:[0.0536]
Loss:[0.0563]
Loss:[0.0560]
Loss:[0.0554]
Loss:[0.0581]
Loss:[0.0579]
Loss:[0.0575]
Loss:[0.0481]
Loss:[0.0536]
Loss:[0.0442]
Loss:[0.0575]
Loss:[0.0478]
Loss:[0.0646]
Loss:[0.0513]
Loss:[0.0515]
Loss:[0.0551]
Loss:[0.0464]
Loss:[0.0528]
Loss:[0.0550]
Loss:[0.0476]
Loss:[0.0449]
Loss:[0.0462]
Loss:[0.0421]
Loss:[0.0531]
Loss:[0.0552]
Loss:[0.0487]
Loss:[0.0494]
Loss:[0.0430]
Loss:[0.0464]
Loss:[0.0414]
Loss:[0.0476]
Loss:[0.0366]
Loss:[0.0394]
Loss:[0.0364]
Loss:[0.0410]
Loss:[0.0404]
Loss:[0.0430]
Loss:[0.0412]
Loss:[0.0473]
Loss:[0.0448]
Loss:[0.0436]
Loss:[0.0401]
Loss:[0.0418]
Loss:[0.0397]
Loss:[0.0342]
Loss:[0.0447]
Loss:[0.0402]
Loss:[0.0420]
Loss:[0.0354]
Loss:[0.0368]
Loss:[0.0377]
Loss:[0.0427]
Loss:[0.0390]
Loss:[0.0362]
Loss:[0.0339]
Loss:[0.0295]
Loss:[0.0362]
Loss:[0.0377]
Loss:[0.0377]
Loss:[0.0390]
Loss:[0.0360]
Loss:[0.0372]
Loss:[0.0315]
Loss:[0.0356]
Loss:[0.0328]
Loss:[0.0348]
Loss:[0.0315]
Loss:[0.0431]
Loss:[0.0286]
Loss:[0.0343]
Loss:[0.0308]
Loss:[0.0350]
Loss:[0.0271]
Loss:[0.0308]
Loss:[0.0289]
Loss:[0.0281]
Loss:[0.0283]
Loss:[0.0358]
Loss:[0.0321]
Loss:[0.0290]
Loss:[0.0351]
Loss:[0.0319]
Loss:[0.0341]
Loss:[0.0332]
Loss:[0.0275]
Loss:[0.0376]
Loss:[0.0302]
Loss:[0.0245]
Loss:[0.0345]
Loss:[0.0256]
Loss:[0.0270]
Loss:[0.0298]
Loss:[0.0298]
Loss:[0.0298]
Loss:[0.0315]
Loss:[0.0357]
Loss:[0.0269]
Loss:[0.0233]
Loss:[0.0237]
Loss:[0.0319]
Loss:[0.0231]
Loss:[0.0235]
Loss:[0.0284]
Loss:[0.0313]
Loss:[0.0289]
Loss:[0.0291]
Loss:[0.0233]
Loss:[0.0228]
Loss:[0.0271]
Loss:[0.0271]
Loss:[0.0288]
Loss:[0.0230]
Loss:[0.0252]
Loss:[0.0285]
Loss:[0.0245]
Loss:[0.0242]
Loss:[0.0247]
Loss:[0.0280]
Loss:[0.0209]
Loss:[0.0245]
Loss:[0.0243]
Loss:[0.0236]
Loss:[0.0219]
Loss:[0.0294]
Loss:[0.0251]
Loss:[0.0260]
Loss:[0.0252]
Loss:[0.0233]
Loss:[0.0211]
Loss:[0.0216]
Loss:[0.0231]
Loss:[0.0221]
Loss:[0.0233]
Loss:[0.0259]
Loss:[0.0226]
Loss:[0.0252]
Loss:[0.0285]
Loss:[0.0212]
Loss:[0.0255]
Early stopping!
Loading 270th epoch
acc:[0.8140]
acc:[0.8140]
acc:[0.8130]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8140]
acc:[0.8160]
acc:[0.8150]
acc:[0.8170]
acc:[0.8140]
acc:[0.8140]
acc:[0.8140]
acc:[0.8160]
acc:[0.8140]
acc:[0.8150]
acc:[0.8150]
acc:[0.8120]
acc:[0.8150]
acc:[0.8150]
acc:[0.8170]
acc:[0.8130]
acc:[0.8140]
acc:[0.8150]
acc:[0.8160]
acc:[0.8140]
acc:[0.8130]
acc:[0.8150]
acc:[0.8140]
acc:[0.8140]
acc:[0.8130]
acc:[0.8140]
acc:[0.8150]
acc:[0.8160]
acc:[0.8150]
acc:[0.8130]
acc:[0.8150]
acc:[0.8160]
acc:[0.8130]
acc:[0.8150]
acc:[0.8140]
acc:[0.8150]
acc:[0.8130]
acc:[0.8150]
acc:[0.8150]
acc:[0.8160]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8160]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8145]
Mean:[81.4540]
Std :[0.1092]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='cora', drop_percent=0.08, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=39)
----------------------------------------------------------------------------------------------------
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.7000]
Loss:[0.6857]
Loss:[0.6900]
Loss:[0.6860]
Loss:[0.6778]
Loss:[0.6742]
Loss:[0.6731]
Loss:[0.6628]
Loss:[0.6573]
Loss:[0.6523]
Loss:[0.6374]
Loss:[0.6347]
Loss:[0.6209]
Loss:[0.6094]
Loss:[0.6003]
Loss:[0.5825]
Loss:[0.5729]
Loss:[0.5558]
Loss:[0.5440]
Loss:[0.5242]
Loss:[0.5138]
Loss:[0.4968]
Loss:[0.4833]
Loss:[0.4683]
Loss:[0.4550]
Loss:[0.4401]
Loss:[0.4254]
Loss:[0.4148]
Loss:[0.3960]
Loss:[0.3821]
Loss:[0.3717]
Loss:[0.3575]
Loss:[0.3418]
Loss:[0.3274]
Loss:[0.3205]
Loss:[0.3178]
Loss:[0.3066]
Loss:[0.2937]
Loss:[0.2802]
Loss:[0.2856]
Loss:[0.2706]
Loss:[0.2676]
Loss:[0.2574]
Loss:[0.2560]
Loss:[0.2441]
Loss:[0.2256]
Loss:[0.2336]
Loss:[0.2214]
Loss:[0.2224]
Loss:[0.2120]
Loss:[0.2114]
Loss:[0.2075]
Loss:[0.2046]
Loss:[0.1923]
Loss:[0.2000]
Loss:[0.1907]
Loss:[0.1789]
Loss:[0.1820]
Loss:[0.1634]
Loss:[0.1695]
Loss:[0.1914]
Loss:[0.1748]
Loss:[0.1579]
Loss:[0.1569]
Loss:[0.1495]
Loss:[0.1561]
Loss:[0.1463]
Loss:[0.1442]
Loss:[0.1537]
Loss:[0.1466]
Loss:[0.1477]
Loss:[0.1305]
Loss:[0.1365]
Loss:[0.1377]
Loss:[0.1367]
Loss:[0.1339]
Loss:[0.1387]
Loss:[0.1328]
Loss:[0.1248]
Loss:[0.1323]
Loss:[0.1253]
Loss:[0.1258]
Loss:[0.1208]
Loss:[0.1237]
Loss:[0.1177]
Loss:[0.1190]
Loss:[0.1125]
Loss:[0.1163]
Loss:[0.1127]
Loss:[0.1109]
Loss:[0.1043]
Loss:[0.1048]
Loss:[0.1037]
Loss:[0.1132]
Loss:[0.0969]
Loss:[0.1091]
Loss:[0.1081]
Loss:[0.1188]
Loss:[0.0928]
Loss:[0.1172]
Loss:[0.1140]
Loss:[0.1059]
Loss:[0.1322]
Loss:[0.1107]
Loss:[0.1322]
Loss:[0.1120]
Loss:[0.0922]
Loss:[0.0949]
Loss:[0.1064]
Loss:[0.1153]
Loss:[0.0875]
Loss:[0.1121]
Loss:[0.0851]
Loss:[0.0921]
Loss:[0.0862]
Loss:[0.0886]
Loss:[0.0786]
Loss:[0.0819]
Loss:[0.0840]
Loss:[0.0869]
Loss:[0.0748]
Loss:[0.0671]
Loss:[0.0775]
Loss:[0.0744]
Loss:[0.0684]
Loss:[0.0704]
Loss:[0.0722]
Loss:[0.0734]
Loss:[0.0716]
Loss:[0.0677]
Loss:[0.0680]
Loss:[0.0667]
Loss:[0.0782]
Loss:[0.0615]
Loss:[0.0651]
Loss:[0.0633]
Loss:[0.0635]
Loss:[0.0628]
Loss:[0.0616]
Loss:[0.0673]
Loss:[0.0575]
Loss:[0.0607]
Loss:[0.0673]
Loss:[0.0538]
Loss:[0.0594]
Loss:[0.0598]
Loss:[0.0564]
Loss:[0.0646]
Loss:[0.0579]
Loss:[0.0589]
Loss:[0.0612]
Loss:[0.0536]
Loss:[0.0563]
Loss:[0.0560]
Loss:[0.0554]
Loss:[0.0581]
Loss:[0.0579]
Loss:[0.0575]
Loss:[0.0481]
Loss:[0.0536]
Loss:[0.0442]
Loss:[0.0575]
Loss:[0.0478]
Loss:[0.0646]
Loss:[0.0513]
Loss:[0.0515]
Loss:[0.0551]
Loss:[0.0464]
Loss:[0.0528]
Loss:[0.0550]
Loss:[0.0476]
Loss:[0.0449]
Loss:[0.0462]
Loss:[0.0421]
Loss:[0.0531]
Loss:[0.0552]
Loss:[0.0487]
Loss:[0.0494]
Loss:[0.0430]
Loss:[0.0464]
Loss:[0.0414]
Loss:[0.0476]
Loss:[0.0366]
Loss:[0.0394]
Loss:[0.0364]
Loss:[0.0410]
Loss:[0.0404]
Loss:[0.0430]
Loss:[0.0412]
Loss:[0.0473]
Loss:[0.0448]
Loss:[0.0436]
Loss:[0.0401]
Loss:[0.0418]
Loss:[0.0397]
Loss:[0.0342]
Loss:[0.0447]
Loss:[0.0402]
Loss:[0.0420]
Loss:[0.0354]
Loss:[0.0368]
Loss:[0.0377]
Loss:[0.0427]
Loss:[0.0390]
Loss:[0.0362]
Loss:[0.0339]
Loss:[0.0295]
Loss:[0.0362]
Loss:[0.0377]
Loss:[0.0377]
Loss:[0.0390]
Loss:[0.0360]
Loss:[0.0372]
Loss:[0.0315]
Loss:[0.0356]
Loss:[0.0328]
Loss:[0.0348]
Loss:[0.0315]
Loss:[0.0431]
Loss:[0.0286]
Loss:[0.0343]
Loss:[0.0308]
Loss:[0.0350]
Loss:[0.0271]
Loss:[0.0308]
Loss:[0.0289]
Loss:[0.0281]
Loss:[0.0283]
Loss:[0.0358]
Loss:[0.0321]
Loss:[0.0290]
Loss:[0.0351]
Loss:[0.0319]
Loss:[0.0341]
Loss:[0.0332]
Loss:[0.0275]
Loss:[0.0376]
Loss:[0.0302]
Loss:[0.0245]
Loss:[0.0345]
Loss:[0.0256]
Loss:[0.0270]
Loss:[0.0298]
Loss:[0.0298]
Loss:[0.0298]
Loss:[0.0315]
Loss:[0.0357]
Loss:[0.0269]
Loss:[0.0233]
Loss:[0.0237]
Loss:[0.0319]
Loss:[0.0231]
Loss:[0.0235]
Loss:[0.0284]
Loss:[0.0313]
Loss:[0.0289]
Loss:[0.0291]
Loss:[0.0233]
Loss:[0.0228]
Loss:[0.0271]
Loss:[0.0271]
Loss:[0.0288]
Loss:[0.0230]
Loss:[0.0252]
Loss:[0.0285]
Loss:[0.0245]
Loss:[0.0242]
Loss:[0.0247]
Loss:[0.0280]
Loss:[0.0209]
Loss:[0.0245]
Loss:[0.0243]
Loss:[0.0236]
Loss:[0.0219]
Loss:[0.0294]
Loss:[0.0251]
Loss:[0.0260]
Loss:[0.0252]
Loss:[0.0233]
Loss:[0.0211]
Loss:[0.0216]
Loss:[0.0231]
Loss:[0.0221]
Loss:[0.0233]
Loss:[0.0259]
Loss:[0.0226]
Loss:[0.0252]
Loss:[0.0285]
Loss:[0.0212]
Loss:[0.0255]
Early stopping!
Loading 270th epoch
acc:[0.8140]
acc:[0.8140]
acc:[0.8130]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8140]
acc:[0.8160]
acc:[0.8150]
acc:[0.8170]
acc:[0.8140]
acc:[0.8140]
acc:[0.8140]
acc:[0.8160]
acc:[0.8140]
acc:[0.8150]
acc:[0.8150]
acc:[0.8120]
acc:[0.8150]
acc:[0.8150]
acc:[0.8170]
acc:[0.8130]
acc:[0.8140]
acc:[0.8150]
acc:[0.8160]
acc:[0.8140]
acc:[0.8130]
acc:[0.8150]
acc:[0.8140]
acc:[0.8140]
acc:[0.8130]
acc:[0.8140]
acc:[0.8150]
acc:[0.8160]
acc:[0.8150]
acc:[0.8130]
acc:[0.8150]
acc:[0.8160]
acc:[0.8130]
acc:[0.8150]
acc:[0.8140]
acc:[0.8150]
acc:[0.8130]
acc:[0.8150]
acc:[0.8150]
acc:[0.8160]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8160]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8145]
Mean:[81.4540]
Std :[0.1092]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='cora', drop_percent=0.1, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=39)
----------------------------------------------------------------------------------------------------
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.7000]
Loss:[0.6857]
Loss:[0.6900]
Loss:[0.6860]
Loss:[0.6778]
Loss:[0.6742]
Loss:[0.6731]
Loss:[0.6628]
Loss:[0.6573]
Loss:[0.6523]
Loss:[0.6374]
Loss:[0.6347]
Loss:[0.6209]
Loss:[0.6094]
Loss:[0.6003]
Loss:[0.5825]
Loss:[0.5729]
Loss:[0.5558]
Loss:[0.5440]
Loss:[0.5242]
Loss:[0.5138]
Loss:[0.4968]
Loss:[0.4833]
Loss:[0.4683]
Loss:[0.4550]
Loss:[0.4401]
Loss:[0.4254]
Loss:[0.4148]
Loss:[0.3960]
Loss:[0.3821]
Loss:[0.3716]
Loss:[0.3574]
Loss:[0.3418]
Loss:[0.3274]
Loss:[0.3205]
Loss:[0.3177]
Loss:[0.3066]
Loss:[0.2938]
Loss:[0.2802]
Loss:[0.2855]
Loss:[0.2707]
Loss:[0.2678]
Loss:[0.2574]
Loss:[0.2560]
Loss:[0.2442]
Loss:[0.2257]
Loss:[0.2336]
Loss:[0.2214]
Loss:[0.2224]
Loss:[0.2120]
Loss:[0.2114]
Loss:[0.2075]
Loss:[0.2046]
Loss:[0.1923]
Loss:[0.2000]
Loss:[0.1908]
Loss:[0.1789]
Loss:[0.1820]
Loss:[0.1636]
Loss:[0.1697]
Loss:[0.1912]
Loss:[0.1745]
Loss:[0.1578]
Loss:[0.1571]
Loss:[0.1497]
Loss:[0.1561]
Loss:[0.1463]
Loss:[0.1444]
Loss:[0.1540]
Loss:[0.1467]
Loss:[0.1478]
Loss:[0.1307]
Loss:[0.1369]
Loss:[0.1379]
Loss:[0.1366]
Loss:[0.1336]
Loss:[0.1385]
Loss:[0.1329]
Loss:[0.1250]
Loss:[0.1325]
Loss:[0.1249]
Loss:[0.1250]
Loss:[0.1205]
Loss:[0.1238]
Loss:[0.1176]
Loss:[0.1188]
Loss:[0.1125]
Loss:[0.1164]
Loss:[0.1125]
Loss:[0.1105]
Loss:[0.1044]
Loss:[0.1049]
Loss:[0.1031]
Loss:[0.1131]
Loss:[0.0973]
Loss:[0.1085]
Loss:[0.1075]
Loss:[0.1205]
Loss:[0.0932]
Loss:[0.1159]
Loss:[0.1138]
Loss:[0.1049]
Loss:[0.1313]
Loss:[0.1132]
Loss:[0.1311]
Loss:[0.1166]
Loss:[0.0887]
Loss:[0.0945]
Loss:[0.1015]
Loss:[0.1186]
Loss:[0.0841]
Loss:[0.1121]
Loss:[0.0817]
Loss:[0.0947]
Loss:[0.0842]
Loss:[0.0892]
Loss:[0.0761]
Loss:[0.0843]
Loss:[0.0825]
Loss:[0.0882]
Loss:[0.0740]
Loss:[0.0688]
Loss:[0.0777]
Loss:[0.0758]
Loss:[0.0680]
Loss:[0.0720]
Loss:[0.0721]
Loss:[0.0742]
Loss:[0.0711]
Loss:[0.0688]
Loss:[0.0684]
Loss:[0.0673]
Loss:[0.0779]
Loss:[0.0628]
Loss:[0.0655]
Loss:[0.0641]
Loss:[0.0638]
Loss:[0.0630]
Loss:[0.0615]
Loss:[0.0679]
Loss:[0.0577]
Loss:[0.0609]
Loss:[0.0676]
Loss:[0.0534]
Loss:[0.0591]
Loss:[0.0596]
Loss:[0.0560]
Loss:[0.0650]
Loss:[0.0577]
Loss:[0.0590]
Loss:[0.0615]
Loss:[0.0535]
Loss:[0.0562]
Loss:[0.0559]
Loss:[0.0554]
Loss:[0.0581]
Loss:[0.0578]
Loss:[0.0573]
Loss:[0.0481]
Loss:[0.0536]
Loss:[0.0440]
Loss:[0.0575]
Loss:[0.0474]
Loss:[0.0644]
Loss:[0.0515]
Loss:[0.0516]
Loss:[0.0551]
Loss:[0.0464]
Loss:[0.0527]
Loss:[0.0549]
Loss:[0.0474]
Loss:[0.0448]
Loss:[0.0461]
Loss:[0.0422]
Loss:[0.0531]
Loss:[0.0552]
Loss:[0.0489]
Loss:[0.0494]
Loss:[0.0431]
Loss:[0.0464]
Loss:[0.0415]
Loss:[0.0476]
Loss:[0.0367]
Loss:[0.0395]
Loss:[0.0364]
Loss:[0.0410]
Loss:[0.0405]
Loss:[0.0430]
Loss:[0.0412]
Loss:[0.0474]
Loss:[0.0448]
Loss:[0.0437]
Loss:[0.0400]
Loss:[0.0419]
Loss:[0.0397]
Loss:[0.0343]
Loss:[0.0447]
Loss:[0.0402]
Loss:[0.0420]
Loss:[0.0353]
Loss:[0.0368]
Loss:[0.0378]
Loss:[0.0427]
Loss:[0.0389]
Loss:[0.0363]
Loss:[0.0339]
Loss:[0.0297]
Loss:[0.0361]
Loss:[0.0378]
Loss:[0.0376]
Loss:[0.0391]
Loss:[0.0360]
Loss:[0.0374]
Loss:[0.0315]
Loss:[0.0356]
Loss:[0.0329]
Loss:[0.0348]
Loss:[0.0315]
Loss:[0.0431]
Loss:[0.0286]
Loss:[0.0343]
Loss:[0.0308]
Loss:[0.0351]
Loss:[0.0272]
Loss:[0.0309]
Loss:[0.0289]
Loss:[0.0281]
Loss:[0.0284]
Loss:[0.0360]
Loss:[0.0322]
Loss:[0.0290]
Loss:[0.0351]
Loss:[0.0320]
Loss:[0.0342]
Loss:[0.0331]
Loss:[0.0276]
Loss:[0.0377]
Loss:[0.0302]
Loss:[0.0245]
Loss:[0.0345]
Loss:[0.0256]
Loss:[0.0272]
Loss:[0.0296]
Loss:[0.0299]
Loss:[0.0297]
Loss:[0.0317]
Loss:[0.0355]
Loss:[0.0271]
Loss:[0.0234]
Loss:[0.0238]
Loss:[0.0319]
Loss:[0.0232]
Loss:[0.0234]
Loss:[0.0285]
Loss:[0.0313]
Loss:[0.0290]
Loss:[0.0291]
Loss:[0.0233]
Loss:[0.0229]
Loss:[0.0272]
Loss:[0.0271]
Loss:[0.0288]
Loss:[0.0230]
Loss:[0.0252]
Loss:[0.0285]
Loss:[0.0245]
Loss:[0.0243]
Loss:[0.0248]
Loss:[0.0281]
Loss:[0.0210]
Loss:[0.0245]
Loss:[0.0244]
Loss:[0.0237]
Loss:[0.0219]
Loss:[0.0296]
Loss:[0.0253]
Loss:[0.0261]
Loss:[0.0253]
Loss:[0.0234]
Loss:[0.0211]
Loss:[0.0217]
Loss:[0.0231]
Loss:[0.0221]
Loss:[0.0234]
Loss:[0.0260]
Loss:[0.0226]
Loss:[0.0254]
Loss:[0.0285]
Loss:[0.0212]
Loss:[0.0256]
Early stopping!
Loading 270th epoch
acc:[0.8110]
acc:[0.8130]
acc:[0.8140]
acc:[0.8130]
acc:[0.8110]
acc:[0.8120]
acc:[0.8130]
acc:[0.8130]
acc:[0.8140]
acc:[0.8120]
acc:[0.8130]
acc:[0.8120]
acc:[0.8130]
acc:[0.8120]
acc:[0.8120]
acc:[0.8130]
acc:[0.8110]
acc:[0.8120]
acc:[0.8160]
acc:[0.8100]
acc:[0.8130]
acc:[0.8120]
acc:[0.8130]
acc:[0.8130]
acc:[0.8130]
acc:[0.8110]
acc:[0.8110]
acc:[0.8110]
acc:[0.8120]
acc:[0.8140]
acc:[0.8130]
acc:[0.8120]
acc:[0.8120]
acc:[0.8140]
acc:[0.8130]
acc:[0.8130]
acc:[0.8140]
acc:[0.8110]
acc:[0.8130]
acc:[0.8150]
acc:[0.8140]
acc:[0.8120]
acc:[0.8110]
acc:[0.8140]
acc:[0.8130]
acc:[0.8100]
acc:[0.8130]
acc:[0.8150]
acc:[0.8130]
acc:[0.8130]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8126]
Mean:[81.2620]
Std :[0.1260]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='cora', drop_percent=0.2, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=39)
----------------------------------------------------------------------------------------------------
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.7000]
Loss:[0.6857]
Loss:[0.6900]
Loss:[0.6860]
Loss:[0.6778]
Loss:[0.6742]
Loss:[0.6731]
Loss:[0.6628]
Loss:[0.6573]
Loss:[0.6523]
Loss:[0.6374]
Loss:[0.6347]
Loss:[0.6209]
Loss:[0.6094]
Loss:[0.6003]
Loss:[0.5825]
Loss:[0.5729]
Loss:[0.5559]
Loss:[0.5440]
Loss:[0.5242]
Loss:[0.5138]
Loss:[0.4968]
Loss:[0.4833]
Loss:[0.4683]
Loss:[0.4550]
Loss:[0.4401]
Loss:[0.4254]
Loss:[0.4147]
Loss:[0.3960]
Loss:[0.3822]
Loss:[0.3715]
Loss:[0.3574]
Loss:[0.3419]
Loss:[0.3275]
Loss:[0.3204]
Loss:[0.3176]
Loss:[0.3066]
Loss:[0.2939]
Loss:[0.2802]
Loss:[0.2854]
Loss:[0.2707]
Loss:[0.2679]
Loss:[0.2574]
Loss:[0.2560]
Loss:[0.2443]
Loss:[0.2258]
Loss:[0.2336]
Loss:[0.2214]
Loss:[0.2225]
Loss:[0.2120]
Loss:[0.2115]
Loss:[0.2076]
Loss:[0.2047]
Loss:[0.1923]
Loss:[0.2000]
Loss:[0.1907]
Loss:[0.1789]
Loss:[0.1821]
Loss:[0.1637]
Loss:[0.1697]
Loss:[0.1910]
Loss:[0.1743]
Loss:[0.1578]
Loss:[0.1572]
Loss:[0.1498]
Loss:[0.1561]
Loss:[0.1463]
Loss:[0.1445]
Loss:[0.1540]
Loss:[0.1468]
Loss:[0.1478]
Loss:[0.1308]
Loss:[0.1371]
Loss:[0.1379]
Loss:[0.1366]
Loss:[0.1336]
Loss:[0.1385]
Loss:[0.1328]
Loss:[0.1251]
Loss:[0.1325]
Loss:[0.1248]
Loss:[0.1246]
Loss:[0.1204]
Loss:[0.1238]
Loss:[0.1177]
Loss:[0.1188]
Loss:[0.1126]
Loss:[0.1163]
Loss:[0.1126]
Loss:[0.1104]
Loss:[0.1044]
Loss:[0.1049]
Loss:[0.1029]
Loss:[0.1131]
Loss:[0.0973]
Loss:[0.1084]
Loss:[0.1071]
Loss:[0.1205]
Loss:[0.0935]
Loss:[0.1150]
Loss:[0.1128]
Loss:[0.1042]
Loss:[0.1290]
Loss:[0.1132]
Loss:[0.1282]
Loss:[0.1202]
Loss:[0.0864]
Loss:[0.0944]
Loss:[0.0971]
Loss:[0.1223]
Loss:[0.0830]
Loss:[0.1141]
Loss:[0.0798]
Loss:[0.0983]
Loss:[0.0856]
Loss:[0.0921]
Loss:[0.0754]
Loss:[0.0881]
Loss:[0.0838]
Loss:[0.0916]
Loss:[0.0745]
Loss:[0.0723]
Loss:[0.0792]
Loss:[0.0792]
Loss:[0.0679]
Loss:[0.0756]
Loss:[0.0725]
Loss:[0.0763]
Loss:[0.0711]
Loss:[0.0717]
Loss:[0.0686]
Loss:[0.0692]
Loss:[0.0781]
Loss:[0.0656]
Loss:[0.0652]
Loss:[0.0657]
Loss:[0.0636]
Loss:[0.0640]
Loss:[0.0615]
Loss:[0.0689]
Loss:[0.0573]
Loss:[0.0617]
Loss:[0.0671]
Loss:[0.0534]
Loss:[0.0594]
Loss:[0.0596]
Loss:[0.0568]
Loss:[0.0655]
Loss:[0.0578]
Loss:[0.0591]
Loss:[0.0615]
Loss:[0.0535]
Loss:[0.0568]
Loss:[0.0560]
Loss:[0.0557]
Loss:[0.0583]
Loss:[0.0578]
Loss:[0.0574]
Loss:[0.0484]
Loss:[0.0538]
Loss:[0.0441]
Loss:[0.0577]
Loss:[0.0481]
Loss:[0.0642]
Loss:[0.0511]
Loss:[0.0518]
Loss:[0.0550]
Loss:[0.0464]
Loss:[0.0530]
Loss:[0.0552]
Loss:[0.0479]
Loss:[0.0451]
Loss:[0.0467]
Loss:[0.0424]
Loss:[0.0533]
Loss:[0.0554]
Loss:[0.0490]
Loss:[0.0495]
Loss:[0.0433]
Loss:[0.0466]
Loss:[0.0414]
Loss:[0.0477]
Loss:[0.0369]
Loss:[0.0398]
Loss:[0.0367]
Loss:[0.0411]
Loss:[0.0407]
Loss:[0.0433]
Loss:[0.0415]
Loss:[0.0475]
Loss:[0.0450]
Loss:[0.0440]
Loss:[0.0402]
Loss:[0.0422]
Loss:[0.0397]
Loss:[0.0347]
Loss:[0.0445]
Loss:[0.0404]
Loss:[0.0418]
Loss:[0.0353]
Loss:[0.0367]
Loss:[0.0378]
Loss:[0.0427]
Loss:[0.0388]
Loss:[0.0365]
Loss:[0.0337]
Loss:[0.0299]
Loss:[0.0355]
Loss:[0.0379]
Loss:[0.0373]
Loss:[0.0394]
Loss:[0.0360]
Loss:[0.0376]
Loss:[0.0311]
Loss:[0.0357]
Loss:[0.0330]
Loss:[0.0352]
Loss:[0.0315]
Loss:[0.0430]
Loss:[0.0291]
Loss:[0.0347]
Loss:[0.0310]
Loss:[0.0353]
Loss:[0.0275]
Loss:[0.0312]
Loss:[0.0291]
Loss:[0.0284]
Loss:[0.0287]
Loss:[0.0362]
Loss:[0.0324]
Loss:[0.0293]
Loss:[0.0353]
Loss:[0.0322]
Loss:[0.0342]
Loss:[0.0334]
Loss:[0.0277]
Loss:[0.0382]
Loss:[0.0302]
Loss:[0.0245]
Loss:[0.0343]
Loss:[0.0255]
Loss:[0.0273]
Loss:[0.0295]
Loss:[0.0300]
Loss:[0.0298]
Loss:[0.0317]
Loss:[0.0354]
Loss:[0.0273]
Loss:[0.0231]
Loss:[0.0241]
Loss:[0.0318]
Loss:[0.0238]
Loss:[0.0236]
Loss:[0.0288]
Loss:[0.0317]
Loss:[0.0297]
Loss:[0.0296]
Loss:[0.0238]
Loss:[0.0232]
Loss:[0.0278]
Loss:[0.0274]
Loss:[0.0294]
Loss:[0.0234]
Loss:[0.0256]
Loss:[0.0287]
Loss:[0.0248]
Loss:[0.0247]
Loss:[0.0252]
Loss:[0.0284]
Early stopping!
Loading 249th epoch
acc:[0.8150]
acc:[0.8160]
acc:[0.8160]
acc:[0.8160]
acc:[0.8120]
acc:[0.8160]
acc:[0.8150]
acc:[0.8140]
acc:[0.8140]
acc:[0.8140]
acc:[0.8160]
acc:[0.8150]
acc:[0.8120]
acc:[0.8140]
acc:[0.8120]
acc:[0.8160]
acc:[0.8100]
acc:[0.8110]
acc:[0.8160]
acc:[0.8110]
acc:[0.8150]
acc:[0.8110]
acc:[0.8150]
acc:[0.8120]
acc:[0.8160]
acc:[0.8130]
acc:[0.8140]
acc:[0.8130]
acc:[0.8150]
acc:[0.8150]
acc:[0.8140]
acc:[0.8160]
acc:[0.8140]
acc:[0.8130]
acc:[0.8160]
acc:[0.8140]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8170]
acc:[0.8150]
acc:[0.8150]
acc:[0.8140]
acc:[0.8180]
acc:[0.8150]
acc:[0.8120]
acc:[0.8130]
acc:[0.8160]
acc:[0.8140]
acc:[0.8160]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8143]
Mean:[81.4300]
Std :[0.1729]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='cora', drop_percent=0.3, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=39)
----------------------------------------------------------------------------------------------------
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.7000]
Loss:[0.6857]
Loss:[0.6900]
Loss:[0.6860]
Loss:[0.6778]
Loss:[0.6742]
Loss:[0.6731]
Loss:[0.6628]
Loss:[0.6573]
Loss:[0.6523]
Loss:[0.6374]
Loss:[0.6347]
Loss:[0.6209]
Loss:[0.6094]
Loss:[0.6003]
Loss:[0.5825]
Loss:[0.5729]
Loss:[0.5558]
Loss:[0.5440]
Loss:[0.5242]
Loss:[0.5138]
Loss:[0.4968]
Loss:[0.4833]
Loss:[0.4683]
Loss:[0.4550]
Loss:[0.4401]
Loss:[0.4254]
Loss:[0.4147]
Loss:[0.3960]
Loss:[0.3822]
Loss:[0.3716]
Loss:[0.3574]
Loss:[0.3418]
Loss:[0.3274]
Loss:[0.3205]
Loss:[0.3176]
Loss:[0.3066]
Loss:[0.2939]
Loss:[0.2802]
Loss:[0.2854]
Loss:[0.2707]
Loss:[0.2679]
Loss:[0.2574]
Loss:[0.2560]
Loss:[0.2443]
Loss:[0.2258]
Loss:[0.2336]
Loss:[0.2214]
Loss:[0.2224]
Loss:[0.2120]
Loss:[0.2115]
Loss:[0.2075]
Loss:[0.2046]
Loss:[0.1923]
Loss:[0.2000]
Loss:[0.1907]
Loss:[0.1789]
Loss:[0.1821]
Loss:[0.1636]
Loss:[0.1697]
Loss:[0.1912]
Loss:[0.1744]
Loss:[0.1578]
Loss:[0.1571]
Loss:[0.1498]
Loss:[0.1561]
Loss:[0.1463]
Loss:[0.1444]
Loss:[0.1540]
Loss:[0.1467]
Loss:[0.1478]
Loss:[0.1307]
Loss:[0.1371]
Loss:[0.1379]
Loss:[0.1366]
Loss:[0.1336]
Loss:[0.1384]
Loss:[0.1329]
Loss:[0.1250]
Loss:[0.1325]
Loss:[0.1250]
Loss:[0.1248]
Loss:[0.1204]
Loss:[0.1238]
Loss:[0.1177]
Loss:[0.1188]
Loss:[0.1125]
Loss:[0.1163]
Loss:[0.1126]
Loss:[0.1105]
Loss:[0.1044]
Loss:[0.1049]
Loss:[0.1031]
Loss:[0.1131]
Loss:[0.0972]
Loss:[0.1086]
Loss:[0.1074]
Loss:[0.1203]
Loss:[0.0932]
Loss:[0.1160]
Loss:[0.1137]
Loss:[0.1048]
Loss:[0.1311]
Loss:[0.1129]
Loss:[0.1309]
Loss:[0.1167]
Loss:[0.0885]
Loss:[0.0945]
Loss:[0.1013]
Loss:[0.1188]
Loss:[0.0838]
Loss:[0.1122]
Loss:[0.0813]
Loss:[0.0950]
Loss:[0.0842]
Loss:[0.0895]
Loss:[0.0760]
Loss:[0.0846]
Loss:[0.0825]
Loss:[0.0884]
Loss:[0.0739]
Loss:[0.0690]
Loss:[0.0778]
Loss:[0.0762]
Loss:[0.0679]
Loss:[0.0724]
Loss:[0.0722]
Loss:[0.0743]
Loss:[0.0712]
Loss:[0.0690]
Loss:[0.0684]
Loss:[0.0674]
Loss:[0.0778]
Loss:[0.0631]
Loss:[0.0655]
Loss:[0.0642]
Loss:[0.0639]
Loss:[0.0630]
Loss:[0.0615]
Loss:[0.0679]
Loss:[0.0577]
Loss:[0.0611]
Loss:[0.0675]
Loss:[0.0534]
Loss:[0.0591]
Loss:[0.0595]
Loss:[0.0559]
Loss:[0.0651]
Loss:[0.0577]
Loss:[0.0590]
Loss:[0.0614]
Loss:[0.0535]
Loss:[0.0561]
Loss:[0.0560]
Loss:[0.0555]
Loss:[0.0580]
Loss:[0.0577]
Loss:[0.0573]
Loss:[0.0481]
Loss:[0.0536]
Loss:[0.0440]
Loss:[0.0575]
Loss:[0.0475]
Loss:[0.0644]
Loss:[0.0514]
Loss:[0.0516]
Loss:[0.0551]
Loss:[0.0464]
Loss:[0.0527]
Loss:[0.0549]
Loss:[0.0474]
Loss:[0.0449]
Loss:[0.0462]
Loss:[0.0423]
Loss:[0.0532]
Loss:[0.0553]
Loss:[0.0490]
Loss:[0.0493]
Loss:[0.0431]
Loss:[0.0464]
Loss:[0.0415]
Loss:[0.0476]
Loss:[0.0366]
Loss:[0.0396]
Loss:[0.0364]
Loss:[0.0411]
Loss:[0.0405]
Loss:[0.0431]
Loss:[0.0411]
Loss:[0.0474]
Loss:[0.0449]
Loss:[0.0437]
Loss:[0.0400]
Loss:[0.0419]
Loss:[0.0397]
Loss:[0.0343]
Loss:[0.0447]
Loss:[0.0402]
Loss:[0.0420]
Loss:[0.0353]
Loss:[0.0368]
Loss:[0.0377]
Loss:[0.0427]
Loss:[0.0389]
Loss:[0.0363]
Loss:[0.0338]
Loss:[0.0297]
Loss:[0.0361]
Loss:[0.0377]
Loss:[0.0376]
Loss:[0.0391]
Loss:[0.0358]
Loss:[0.0374]
Loss:[0.0314]
Loss:[0.0356]
Loss:[0.0329]
Loss:[0.0349]
Loss:[0.0315]
Loss:[0.0432]
Loss:[0.0287]
Loss:[0.0345]
Loss:[0.0309]
Loss:[0.0352]
Loss:[0.0273]
Loss:[0.0309]
Loss:[0.0289]
Loss:[0.0282]
Loss:[0.0285]
Loss:[0.0360]
Loss:[0.0322]
Loss:[0.0291]
Loss:[0.0352]
Loss:[0.0320]
Loss:[0.0341]
Loss:[0.0332]
Loss:[0.0276]
Loss:[0.0378]
Loss:[0.0302]
Loss:[0.0245]
Loss:[0.0344]
Loss:[0.0257]
Loss:[0.0271]
Loss:[0.0296]
Loss:[0.0300]
Loss:[0.0298]
Loss:[0.0316]
Loss:[0.0355]
Loss:[0.0271]
Loss:[0.0234]
Loss:[0.0238]
Loss:[0.0318]
Loss:[0.0232]
Loss:[0.0235]
Loss:[0.0285]
Loss:[0.0314]
Loss:[0.0291]
Loss:[0.0293]
Loss:[0.0235]
Loss:[0.0230]
Loss:[0.0272]
Loss:[0.0271]
Loss:[0.0289]
Loss:[0.0231]
Loss:[0.0253]
Loss:[0.0286]
Loss:[0.0246]
Loss:[0.0243]
Loss:[0.0248]
Loss:[0.0281]
Loss:[0.0210]
Loss:[0.0245]
Loss:[0.0243]
Loss:[0.0238]
Loss:[0.0219]
Loss:[0.0297]
Loss:[0.0253]
Loss:[0.0261]
Loss:[0.0253]
Loss:[0.0234]
Loss:[0.0211]
Loss:[0.0218]
Loss:[0.0232]
Loss:[0.0221]
Loss:[0.0235]
Loss:[0.0260]
Loss:[0.0227]
Loss:[0.0253]
Loss:[0.0284]
Loss:[0.0213]
Loss:[0.0256]
Early stopping!
Loading 270th epoch
acc:[0.8140]
acc:[0.8130]
acc:[0.8150]
acc:[0.8140]
acc:[0.8120]
acc:[0.8150]
acc:[0.8140]
acc:[0.8130]
acc:[0.8140]
acc:[0.8150]
acc:[0.8160]
acc:[0.8140]
acc:[0.8160]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8150]
acc:[0.8120]
acc:[0.8130]
acc:[0.8150]
acc:[0.8160]
acc:[0.8130]
acc:[0.8140]
acc:[0.8160]
acc:[0.8140]
acc:[0.8160]
acc:[0.8160]
acc:[0.8150]
acc:[0.8150]
acc:[0.8130]
acc:[0.8150]
acc:[0.8120]
acc:[0.8140]
acc:[0.8160]
acc:[0.8140]
acc:[0.8140]
acc:[0.8130]
acc:[0.8140]
acc:[0.8150]
acc:[0.8150]
acc:[0.8130]
acc:[0.8150]
acc:[0.8120]
acc:[0.8150]
acc:[0.8140]
acc:[0.8150]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8140]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8143]
Mean:[81.4280]
Std :[0.1126]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='cora', drop_percent=0.4, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=39)
----------------------------------------------------------------------------------------------------
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.7000]
Loss:[0.6857]
Loss:[0.6900]
Loss:[0.6860]
Loss:[0.6778]
Loss:[0.6742]
Loss:[0.6731]
Loss:[0.6628]
Loss:[0.6573]
Loss:[0.6523]
Loss:[0.6374]
Loss:[0.6347]
Loss:[0.6209]
Loss:[0.6094]
Loss:[0.6003]
Loss:[0.5825]
Loss:[0.5729]
Loss:[0.5558]
Loss:[0.5440]
Loss:[0.5242]
Loss:[0.5138]
Loss:[0.4968]
Loss:[0.4833]
Loss:[0.4683]
Loss:[0.4550]
Loss:[0.4401]
Loss:[0.4254]
Loss:[0.4147]
Loss:[0.3960]
Loss:[0.3821]
Loss:[0.3716]
Loss:[0.3574]
Loss:[0.3418]
Loss:[0.3274]
Loss:[0.3205]
Loss:[0.3177]
Loss:[0.3066]
Loss:[0.2937]
Loss:[0.2802]
Loss:[0.2855]
Loss:[0.2707]
Loss:[0.2678]
Loss:[0.2574]
Loss:[0.2560]
Loss:[0.2442]
Loss:[0.2257]
Loss:[0.2336]
Loss:[0.2214]
Loss:[0.2224]
Loss:[0.2120]
Loss:[0.2115]
Loss:[0.2075]
Loss:[0.2046]
Loss:[0.1923]
Loss:[0.2000]
Loss:[0.1907]
Loss:[0.1789]
Loss:[0.1820]
Loss:[0.1635]
Loss:[0.1695]
Loss:[0.1913]
Loss:[0.1747]
Loss:[0.1579]
Loss:[0.1570]
Loss:[0.1496]
Loss:[0.1561]
Loss:[0.1463]
Loss:[0.1443]
Loss:[0.1538]
Loss:[0.1466]
Loss:[0.1478]
Loss:[0.1306]
Loss:[0.1368]
Loss:[0.1377]
Loss:[0.1366]
Loss:[0.1338]
Loss:[0.1386]
Loss:[0.1329]
Loss:[0.1249]
Loss:[0.1324]
Loss:[0.1251]
Loss:[0.1253]
Loss:[0.1206]
Loss:[0.1237]
Loss:[0.1177]
Loss:[0.1189]
Loss:[0.1126]
Loss:[0.1163]
Loss:[0.1126]
Loss:[0.1107]
Loss:[0.1044]
Loss:[0.1048]
Loss:[0.1032]
Loss:[0.1132]
Loss:[0.0971]
Loss:[0.1087]
Loss:[0.1077]
Loss:[0.1198]
Loss:[0.0929]
Loss:[0.1166]
Loss:[0.1140]
Loss:[0.1055]
Loss:[0.1321]
Loss:[0.1121]
Loss:[0.1322]
Loss:[0.1139]
Loss:[0.0908]
Loss:[0.0948]
Loss:[0.1047]
Loss:[0.1164]
Loss:[0.0861]
Loss:[0.1120]
Loss:[0.0835]
Loss:[0.0928]
Loss:[0.0852]
Loss:[0.0885]
Loss:[0.0775]
Loss:[0.0826]
Loss:[0.0831]
Loss:[0.0870]
Loss:[0.0742]
Loss:[0.0675]
Loss:[0.0773]
Loss:[0.0748]
Loss:[0.0681]
Loss:[0.0709]
Loss:[0.0720]
Loss:[0.0735]
Loss:[0.0713]
Loss:[0.0680]
Loss:[0.0680]
Loss:[0.0668]
Loss:[0.0779]
Loss:[0.0618]
Loss:[0.0652]
Loss:[0.0634]
Loss:[0.0635]
Loss:[0.0627]
Loss:[0.0615]
Loss:[0.0674]
Loss:[0.0575]
Loss:[0.0607]
Loss:[0.0674]
Loss:[0.0537]
Loss:[0.0591]
Loss:[0.0596]
Loss:[0.0561]
Loss:[0.0647]
Loss:[0.0577]
Loss:[0.0590]
Loss:[0.0613]
Loss:[0.0535]
Loss:[0.0561]
Loss:[0.0559]
Loss:[0.0553]
Loss:[0.0580]
Loss:[0.0578]
Loss:[0.0574]
Loss:[0.0481]
Loss:[0.0536]
Loss:[0.0441]
Loss:[0.0575]
Loss:[0.0476]
Loss:[0.0644]
Loss:[0.0514]
Loss:[0.0516]
Loss:[0.0550]
Loss:[0.0464]
Loss:[0.0528]
Loss:[0.0549]
Loss:[0.0475]
Loss:[0.0448]
Loss:[0.0462]
Loss:[0.0421]
Loss:[0.0531]
Loss:[0.0553]
Loss:[0.0488]
Loss:[0.0493]
Loss:[0.0430]
Loss:[0.0464]
Loss:[0.0415]
Loss:[0.0475]
Loss:[0.0366]
Loss:[0.0394]
Loss:[0.0363]
Loss:[0.0409]
Loss:[0.0404]
Loss:[0.0429]
Loss:[0.0410]
Loss:[0.0472]
Loss:[0.0447]
Loss:[0.0435]
Loss:[0.0399]
Loss:[0.0417]
Loss:[0.0396]
Loss:[0.0342]
Loss:[0.0447]
Loss:[0.0401]
Loss:[0.0418]
Loss:[0.0354]
Loss:[0.0368]
Loss:[0.0378]
Loss:[0.0426]
Loss:[0.0390]
Loss:[0.0361]
Loss:[0.0339]
Loss:[0.0295]
Loss:[0.0361]
Loss:[0.0377]
Loss:[0.0377]
Loss:[0.0390]
Loss:[0.0359]
Loss:[0.0373]
Loss:[0.0314]
Loss:[0.0355]
Loss:[0.0328]
Loss:[0.0348]
Loss:[0.0316]
Loss:[0.0432]
Loss:[0.0286]
Loss:[0.0343]
Loss:[0.0308]
Loss:[0.0351]
Loss:[0.0271]
Loss:[0.0308]
Loss:[0.0288]
Loss:[0.0281]
Loss:[0.0283]
Loss:[0.0357]
Loss:[0.0321]
Loss:[0.0290]
Loss:[0.0350]
Loss:[0.0319]
Loss:[0.0340]
Loss:[0.0331]
Loss:[0.0274]
Loss:[0.0376]
Loss:[0.0301]
Loss:[0.0245]
Loss:[0.0343]
Loss:[0.0256]
Loss:[0.0269]
Loss:[0.0297]
Loss:[0.0298]
Loss:[0.0298]
Loss:[0.0315]
Loss:[0.0355]
Loss:[0.0269]
Loss:[0.0233]
Loss:[0.0236]
Loss:[0.0319]
Loss:[0.0231]
Loss:[0.0234]
Loss:[0.0285]
Loss:[0.0313]
Loss:[0.0292]
Loss:[0.0290]
Loss:[0.0233]
Loss:[0.0230]
Loss:[0.0272]
Loss:[0.0270]
Loss:[0.0288]
Loss:[0.0230]
Loss:[0.0253]
Loss:[0.0284]
Loss:[0.0244]
Loss:[0.0241]
Loss:[0.0247]
Loss:[0.0280]
Loss:[0.0209]
Loss:[0.0245]
Loss:[0.0244]
Loss:[0.0237]
Loss:[0.0219]
Loss:[0.0295]
Loss:[0.0252]
Loss:[0.0261]
Loss:[0.0252]
Loss:[0.0233]
Loss:[0.0210]
Loss:[0.0216]
Loss:[0.0230]
Loss:[0.0220]
Loss:[0.0234]
Loss:[0.0259]
Loss:[0.0226]
Loss:[0.0253]
Loss:[0.0283]
Loss:[0.0213]
Loss:[0.0255]
Early stopping!
Loading 270th epoch
acc:[0.8120]
acc:[0.8130]
acc:[0.8120]
acc:[0.8120]
acc:[0.8120]
acc:[0.8130]
acc:[0.8120]
acc:[0.8120]
acc:[0.8140]
acc:[0.8120]
acc:[0.8120]
acc:[0.8150]
acc:[0.8110]
acc:[0.8110]
acc:[0.8110]
acc:[0.8140]
acc:[0.8110]
acc:[0.8110]
acc:[0.8130]
acc:[0.8120]
acc:[0.8140]
acc:[0.8110]
acc:[0.8140]
acc:[0.8130]
acc:[0.8130]
acc:[0.8150]
acc:[0.8130]
acc:[0.8110]
acc:[0.8110]
acc:[0.8120]
acc:[0.8130]
acc:[0.8100]
acc:[0.8130]
acc:[0.8140]
acc:[0.8130]
acc:[0.8130]
acc:[0.8110]
acc:[0.8130]
acc:[0.8120]
acc:[0.8150]
acc:[0.8120]
acc:[0.8120]
acc:[0.8120]
acc:[0.8130]
acc:[0.8140]
acc:[0.8110]
acc:[0.8110]
acc:[0.8150]
acc:[0.8130]
acc:[0.8130]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8125]
Mean:[81.2500]
Std :[0.1249]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='cora', drop_percent=0.5, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=39)
----------------------------------------------------------------------------------------------------
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.7000]
Loss:[0.6857]
Loss:[0.6900]
Loss:[0.6860]
Loss:[0.6778]
Loss:[0.6742]
Loss:[0.6731]
Loss:[0.6628]
Loss:[0.6573]
Loss:[0.6522]
Loss:[0.6374]
Loss:[0.6347]
Loss:[0.6209]
Loss:[0.6094]
Loss:[0.6002]
Loss:[0.5825]
Loss:[0.5728]
Loss:[0.5558]
Loss:[0.5439]
Loss:[0.5242]
Loss:[0.5138]
Loss:[0.4967]
Loss:[0.4833]
Loss:[0.4682]
Loss:[0.4549]
Loss:[0.4401]
Loss:[0.4252]
Loss:[0.4145]
Loss:[0.3960]
Loss:[0.3822]
Loss:[0.3713]
Loss:[0.3571]
Loss:[0.3419]
Loss:[0.3276]
Loss:[0.3202]
Loss:[0.3170]
Loss:[0.3064]
Loss:[0.2943]
Loss:[0.2802]
Loss:[0.2848]
Loss:[0.2708]
Loss:[0.2684]
Loss:[0.2573]
Loss:[0.2559]
Loss:[0.2445]
Loss:[0.2259]
Loss:[0.2335]
Loss:[0.2213]
Loss:[0.2224]
Loss:[0.2119]
Loss:[0.2115]
Loss:[0.2077]
Loss:[0.2047]
Loss:[0.1924]
Loss:[0.2000]
Loss:[0.1907]
Loss:[0.1789]
Loss:[0.1822]
Loss:[0.1639]
Loss:[0.1698]
Loss:[0.1905]
Loss:[0.1736]
Loss:[0.1577]
Loss:[0.1576]
Loss:[0.1501]
Loss:[0.1560]
Loss:[0.1463]
Loss:[0.1451]
Loss:[0.1545]
Loss:[0.1469]
Loss:[0.1479]
Loss:[0.1312]
Loss:[0.1380]
Loss:[0.1381]
Loss:[0.1361]
Loss:[0.1331]
Loss:[0.1384]
Loss:[0.1330]
Loss:[0.1252]
Loss:[0.1324]
Loss:[0.1240]
Loss:[0.1230]
Loss:[0.1201]
Loss:[0.1239]
Loss:[0.1176]
Loss:[0.1184]
Loss:[0.1124]
Loss:[0.1164]
Loss:[0.1120]
Loss:[0.1093]
Loss:[0.1045]
Loss:[0.1053]
Loss:[0.1023]
Loss:[0.1125]
Loss:[0.0976]
Loss:[0.1079]
Loss:[0.1044]
Loss:[0.1189]
Loss:[0.0958]
Loss:[0.1078]
Loss:[0.1025]
Loss:[0.1012]
Loss:[0.1038]
Loss:[0.0977]
Loss:[0.1018]
Loss:[0.1217]
Loss:[0.1103]
Loss:[0.0929]
Loss:[0.1099]
Loss:[0.0986]
Loss:[0.0939]
Loss:[0.0954]
Loss:[0.0988]
Loss:[0.0864]
Loss:[0.0956]
Loss:[0.0841]
Loss:[0.0829]
Loss:[0.0828]
Loss:[0.0908]
Loss:[0.0859]
Loss:[0.0815]
Loss:[0.0706]
Loss:[0.0859]
Loss:[0.0747]
Loss:[0.0758]
Loss:[0.0743]
Loss:[0.0807]
Loss:[0.0766]
Loss:[0.0756]
Loss:[0.0704]
Loss:[0.0743]
Loss:[0.0675]
Loss:[0.0805]
Loss:[0.0611]
Loss:[0.0692]
Loss:[0.0637]
Loss:[0.0672]
Loss:[0.0646]
Loss:[0.0631]
Loss:[0.0679]
Loss:[0.0604]
Loss:[0.0628]
Loss:[0.0708]
Loss:[0.0554]
Loss:[0.0597]
Loss:[0.0597]
Loss:[0.0553]
Loss:[0.0645]
Loss:[0.0592]
Loss:[0.0591]
Loss:[0.0624]
Loss:[0.0537]
Loss:[0.0553]
Loss:[0.0557]
Loss:[0.0558]
Loss:[0.0578]
Loss:[0.0581]
Loss:[0.0571]
Loss:[0.0481]
Loss:[0.0529]
Loss:[0.0439]
Loss:[0.0571]
Loss:[0.0459]
Loss:[0.0629]
Loss:[0.0526]
Loss:[0.0509]
Loss:[0.0550]
Loss:[0.0464]
Loss:[0.0521]
Loss:[0.0545]
Loss:[0.0466]
Loss:[0.0445]
Loss:[0.0456]
Loss:[0.0418]
Loss:[0.0530]
Loss:[0.0548]
Loss:[0.0488]
Loss:[0.0494]
Loss:[0.0430]
Loss:[0.0459]
Loss:[0.0413]
Loss:[0.0476]
Loss:[0.0363]
Loss:[0.0393]
Loss:[0.0363]
Loss:[0.0410]
Loss:[0.0400]
Loss:[0.0425]
Loss:[0.0412]
Loss:[0.0469]
Loss:[0.0445]
Loss:[0.0434]
Loss:[0.0398]
Loss:[0.0419]
Loss:[0.0389]
Loss:[0.0344]
Loss:[0.0438]
Loss:[0.0400]
Loss:[0.0410]
Loss:[0.0346]
Loss:[0.0361]
Loss:[0.0374]
Loss:[0.0420]
Loss:[0.0384]
Loss:[0.0359]
Loss:[0.0332]
Loss:[0.0294]
Loss:[0.0347]
Loss:[0.0374]
Loss:[0.0368]
Loss:[0.0389]
Loss:[0.0355]
Loss:[0.0369]
Loss:[0.0308]
Loss:[0.0352]
Loss:[0.0324]
Loss:[0.0347]
Loss:[0.0309]
Loss:[0.0427]
Loss:[0.0290]
Loss:[0.0342]
Loss:[0.0305]
Loss:[0.0350]
Loss:[0.0270]
Loss:[0.0307]
Loss:[0.0287]
Loss:[0.0280]
Loss:[0.0284]
Loss:[0.0359]
Loss:[0.0317]
Loss:[0.0289]
Loss:[0.0349]
Loss:[0.0318]
Loss:[0.0339]
Loss:[0.0332]
Loss:[0.0274]
Loss:[0.0377]
Loss:[0.0298]
Loss:[0.0240]
Loss:[0.0339]
Loss:[0.0254]
Loss:[0.0270]
Loss:[0.0291]
Loss:[0.0296]
Loss:[0.0295]
Loss:[0.0317]
Loss:[0.0352]
Loss:[0.0272]
Loss:[0.0229]
Loss:[0.0239]
Loss:[0.0313]
Loss:[0.0233]
Loss:[0.0233]
Loss:[0.0283]
Loss:[0.0313]
Loss:[0.0288]
Loss:[0.0292]
Loss:[0.0235]
Loss:[0.0229]
Loss:[0.0276]
Loss:[0.0273]
Loss:[0.0290]
Loss:[0.0231]
Loss:[0.0255]
Loss:[0.0284]
Loss:[0.0246]
Loss:[0.0245]
Loss:[0.0248]
Loss:[0.0283]
Loss:[0.0211]
Loss:[0.0247]
Loss:[0.0244]
Loss:[0.0237]
Loss:[0.0221]
Loss:[0.0297]
Loss:[0.0253]
Loss:[0.0259]
Loss:[0.0255]
Loss:[0.0231]
Loss:[0.0211]
Loss:[0.0216]
Loss:[0.0235]
Loss:[0.0224]
Loss:[0.0234]
Loss:[0.0261]
Loss:[0.0230]
Loss:[0.0251]
Loss:[0.0284]
Loss:[0.0211]
Loss:[0.0256]
Loss:[0.0203]
Loss:[0.0228]
Loss:[0.0248]
Loss:[0.0211]
Loss:[0.0204]
Loss:[0.0201]
Loss:[0.0187]
Loss:[0.0233]
Loss:[0.0158]
Loss:[0.0181]
Loss:[0.0207]
Loss:[0.0206]
Loss:[0.0215]
Loss:[0.0193]
Loss:[0.0248]
Loss:[0.0177]
Loss:[0.0213]
Loss:[0.0206]
Loss:[0.0180]
Loss:[0.0183]
Loss:[0.0201]
Loss:[0.0244]
Loss:[0.0153]
Loss:[0.0206]
Loss:[0.0162]
Loss:[0.0213]
Loss:[0.0172]
Loss:[0.0211]
Loss:[0.0177]
Loss:[0.0195]
Loss:[0.0172]
Loss:[0.0156]
Loss:[0.0189]
Loss:[0.0160]
Loss:[0.0180]
Loss:[0.0151]
Loss:[0.0197]
Loss:[0.0208]
Loss:[0.0181]
Loss:[0.0136]
Loss:[0.0152]
Loss:[0.0188]
Loss:[0.0134]
Loss:[0.0182]
Loss:[0.0180]
Loss:[0.0130]
Loss:[0.0174]
Loss:[0.0157]
Loss:[0.0150]
Loss:[0.0172]
Loss:[0.0225]
Loss:[0.0174]
Loss:[0.0173]
Loss:[0.0144]
Loss:[0.0180]
Loss:[0.0193]
Loss:[0.0156]
Loss:[0.0163]
Loss:[0.0167]
Loss:[0.0144]
Loss:[0.0166]
Loss:[0.0142]
Loss:[0.0140]
Loss:[0.0133]
Loss:[0.0142]
Loss:[0.0124]
Loss:[0.0167]
Loss:[0.0133]
Loss:[0.0108]
Loss:[0.0164]
Loss:[0.0129]
Loss:[0.0127]
Loss:[0.0121]
Loss:[0.0124]
Loss:[0.0128]
Loss:[0.0149]
Loss:[0.0148]
Loss:[0.0119]
Loss:[0.0125]
Loss:[0.0130]
Loss:[0.0119]
Loss:[0.0136]
Loss:[0.0105]
Loss:[0.0116]
Loss:[0.0128]
Loss:[0.0124]
Loss:[0.0125]
Loss:[0.0150]
Loss:[0.0112]
Loss:[0.0105]
Loss:[0.0143]
Loss:[0.0116]
Loss:[0.0143]
Loss:[0.0109]
Loss:[0.0119]
Loss:[0.0141]
Loss:[0.0095]
Loss:[0.0120]
Loss:[0.0092]
Loss:[0.0126]
Loss:[0.0097]
Loss:[0.0133]
Loss:[0.0098]
Loss:[0.0129]
Loss:[0.0119]
Loss:[0.0114]
Loss:[0.0108]
Loss:[0.0085]
Loss:[0.0113]
Loss:[0.0100]
Loss:[0.0130]
Loss:[0.0103]
Loss:[0.0106]
Loss:[0.0072]
Loss:[0.0092]
Loss:[0.0101]
Loss:[0.0114]
Loss:[0.0106]
Loss:[0.0116]
Loss:[0.0154]
Loss:[0.0119]
Loss:[0.0100]
Loss:[0.0168]
Loss:[0.0119]
Loss:[0.0098]
Loss:[0.0106]
Loss:[0.0102]
Loss:[0.0148]
Loss:[0.0127]
Loss:[0.0092]
Loss:[0.0083]
Loss:[0.0141]
Loss:[0.0094]
Loss:[0.0179]
Early stopping!
Loading 404th epoch
acc:[0.8130]
acc:[0.8150]
acc:[0.8110]
acc:[0.8140]
acc:[0.8100]
acc:[0.8120]
acc:[0.8120]
acc:[0.8140]
acc:[0.8120]
acc:[0.8140]
acc:[0.8140]
acc:[0.8130]
acc:[0.8140]
acc:[0.8120]
acc:[0.8100]
acc:[0.8130]
acc:[0.8160]
acc:[0.8120]
acc:[0.8150]
acc:[0.8110]
acc:[0.8120]
acc:[0.8070]
acc:[0.8130]
acc:[0.8110]
acc:[0.8100]
acc:[0.8130]
acc:[0.8120]
acc:[0.8080]
acc:[0.8090]
acc:[0.8130]
acc:[0.8100]
acc:[0.8130]
acc:[0.8150]
acc:[0.8150]
acc:[0.8170]
acc:[0.8100]
acc:[0.8120]
acc:[0.8110]
acc:[0.8100]
acc:[0.8130]
acc:[0.8160]
acc:[0.8130]
acc:[0.8130]
acc:[0.8130]
acc:[0.8140]
acc:[0.8110]
acc:[0.8130]
acc:[0.8100]
acc:[0.8120]
acc:[0.8140]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8124]
Mean:[81.2400]
Std :[0.2060]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='cora', drop_percent=0.6, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=39)
----------------------------------------------------------------------------------------------------
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.7000]
Loss:[0.6857]
Loss:[0.6900]
Loss:[0.6860]
Loss:[0.6778]
Loss:[0.6742]
Loss:[0.6731]
Loss:[0.6628]
Loss:[0.6573]
Loss:[0.6523]
Loss:[0.6374]
Loss:[0.6347]
Loss:[0.6209]
Loss:[0.6094]
Loss:[0.6003]
Loss:[0.5825]
Loss:[0.5729]
Loss:[0.5558]
Loss:[0.5440]
Loss:[0.5242]
Loss:[0.5138]
Loss:[0.4968]
Loss:[0.4833]
Loss:[0.4683]
Loss:[0.4550]
Loss:[0.4401]
Loss:[0.4254]
Loss:[0.4147]
Loss:[0.3960]
Loss:[0.3822]
Loss:[0.3715]
Loss:[0.3573]
Loss:[0.3419]
Loss:[0.3275]
Loss:[0.3204]
Loss:[0.3175]
Loss:[0.3066]
Loss:[0.2939]
Loss:[0.2802]
Loss:[0.2854]
Loss:[0.2707]
Loss:[0.2679]
Loss:[0.2574]
Loss:[0.2560]
Loss:[0.2443]
Loss:[0.2257]
Loss:[0.2335]
Loss:[0.2214]
Loss:[0.2224]
Loss:[0.2120]
Loss:[0.2115]
Loss:[0.2076]
Loss:[0.2046]
Loss:[0.1923]
Loss:[0.1999]
Loss:[0.1907]
Loss:[0.1789]
Loss:[0.1821]
Loss:[0.1636]
Loss:[0.1697]
Loss:[0.1910]
Loss:[0.1743]
Loss:[0.1578]
Loss:[0.1572]
Loss:[0.1498]
Loss:[0.1561]
Loss:[0.1462]
Loss:[0.1444]
Loss:[0.1540]
Loss:[0.1467]
Loss:[0.1478]
Loss:[0.1308]
Loss:[0.1371]
Loss:[0.1379]
Loss:[0.1366]
Loss:[0.1336]
Loss:[0.1384]
Loss:[0.1328]
Loss:[0.1250]
Loss:[0.1325]
Loss:[0.1248]
Loss:[0.1246]
Loss:[0.1203]
Loss:[0.1238]
Loss:[0.1177]
Loss:[0.1188]
Loss:[0.1125]
Loss:[0.1163]
Loss:[0.1124]
Loss:[0.1104]
Loss:[0.1044]
Loss:[0.1049]
Loss:[0.1029]
Loss:[0.1131]
Loss:[0.0973]
Loss:[0.1083]
Loss:[0.1069]
Loss:[0.1205]
Loss:[0.0935]
Loss:[0.1148]
Loss:[0.1123]
Loss:[0.1041]
Loss:[0.1282]
Loss:[0.1129]
Loss:[0.1274]
Loss:[0.1208]
Loss:[0.0861]
Loss:[0.0944]
Loss:[0.0960]
Loss:[0.1231]
Loss:[0.0831]
Loss:[0.1146]
Loss:[0.0796]
Loss:[0.0992]
Loss:[0.0863]
Loss:[0.0930]
Loss:[0.0755]
Loss:[0.0892]
Loss:[0.0844]
Loss:[0.0929]
Loss:[0.0745]
Loss:[0.0736]
Loss:[0.0795]
Loss:[0.0804]
Loss:[0.0678]
Loss:[0.0768]
Loss:[0.0724]
Loss:[0.0771]
Loss:[0.0711]
Loss:[0.0726]
Loss:[0.0685]
Loss:[0.0696]
Loss:[0.0783]
Loss:[0.0661]
Loss:[0.0649]
Loss:[0.0660]
Loss:[0.0635]
Loss:[0.0640]
Loss:[0.0615]
Loss:[0.0688]
Loss:[0.0573]
Loss:[0.0616]
Loss:[0.0671]
Loss:[0.0534]
Loss:[0.0597]
Loss:[0.0595]
Loss:[0.0572]
Loss:[0.0653]
Loss:[0.0578]
Loss:[0.0589]
Loss:[0.0616]
Loss:[0.0536]
Loss:[0.0570]
Loss:[0.0559]
Loss:[0.0558]
Loss:[0.0582]
Loss:[0.0579]
Loss:[0.0574]
Loss:[0.0485]
Loss:[0.0537]
Loss:[0.0441]
Loss:[0.0577]
Loss:[0.0482]
Loss:[0.0639]
Loss:[0.0511]
Loss:[0.0518]
Loss:[0.0551]
Loss:[0.0465]
Loss:[0.0530]
Loss:[0.0553]
Loss:[0.0480]
Loss:[0.0451]
Loss:[0.0469]
Loss:[0.0423]
Loss:[0.0533]
Loss:[0.0555]
Loss:[0.0489]
Loss:[0.0497]
Loss:[0.0432]
Loss:[0.0466]
Loss:[0.0414]
Loss:[0.0478]
Loss:[0.0368]
Loss:[0.0398]
Loss:[0.0367]
Loss:[0.0411]
Loss:[0.0407]
Loss:[0.0433]
Loss:[0.0414]
Loss:[0.0475]
Loss:[0.0449]
Loss:[0.0440]
Loss:[0.0403]
Loss:[0.0423]
Loss:[0.0397]
Loss:[0.0348]
Loss:[0.0443]
Loss:[0.0405]
Loss:[0.0417]
Loss:[0.0353]
Loss:[0.0365]
Loss:[0.0379]
Loss:[0.0425]
Loss:[0.0389]
Loss:[0.0364]
Loss:[0.0337]
Loss:[0.0299]
Loss:[0.0352]
Loss:[0.0380]
Loss:[0.0372]
Loss:[0.0395]
Loss:[0.0359]
Loss:[0.0376]
Loss:[0.0311]
Loss:[0.0358]
Loss:[0.0330]
Loss:[0.0352]
Loss:[0.0315]
Loss:[0.0431]
Loss:[0.0292]
Loss:[0.0347]
Loss:[0.0308]
Loss:[0.0354]
Loss:[0.0275]
Loss:[0.0312]
Loss:[0.0291]
Loss:[0.0284]
Loss:[0.0287]
Loss:[0.0363]
Loss:[0.0324]
Loss:[0.0292]
Loss:[0.0354]
Loss:[0.0323]
Loss:[0.0342]
Loss:[0.0334]
Loss:[0.0278]
Loss:[0.0382]
Loss:[0.0301]
Loss:[0.0245]
Loss:[0.0344]
Loss:[0.0255]
Loss:[0.0273]
Loss:[0.0294]
Loss:[0.0301]
Loss:[0.0298]
Loss:[0.0319]
Loss:[0.0354]
Loss:[0.0273]
Loss:[0.0231]
Loss:[0.0241]
Loss:[0.0319]
Loss:[0.0239]
Loss:[0.0237]
Loss:[0.0290]
Loss:[0.0318]
Loss:[0.0298]
Loss:[0.0297]
Loss:[0.0239]
Loss:[0.0232]
Loss:[0.0279]
Loss:[0.0276]
Loss:[0.0294]
Loss:[0.0235]
Loss:[0.0258]
Loss:[0.0289]
Loss:[0.0248]
Loss:[0.0248]
Loss:[0.0253]
Loss:[0.0285]
Early stopping!
Loading 249th epoch
acc:[0.8160]
acc:[0.8140]
acc:[0.8150]
acc:[0.8150]
acc:[0.8150]
acc:[0.8150]
acc:[0.8160]
acc:[0.8140]
acc:[0.8160]
acc:[0.8150]
acc:[0.8170]
acc:[0.8150]
acc:[0.8140]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8130]
acc:[0.8160]
acc:[0.8140]
acc:[0.8170]
acc:[0.8140]
acc:[0.8160]
acc:[0.8150]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8150]
acc:[0.8140]
acc:[0.8150]
acc:[0.8150]
acc:[0.8160]
acc:[0.8170]
acc:[0.8150]
acc:[0.8150]
acc:[0.8140]
acc:[0.8160]
acc:[0.8180]
acc:[0.8140]
acc:[0.8150]
acc:[0.8120]
acc:[0.8160]
acc:[0.8160]
acc:[0.8140]
acc:[0.8160]
acc:[0.8160]
acc:[0.8150]
acc:[0.8140]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8150]
Mean:[81.4980]
Std :[0.1116]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='cora', drop_percent=0.02, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=31)
----------------------------------------------------------------------------------------------------
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6996]
Loss:[0.6861]
Loss:[0.6905]
Loss:[0.6863]
Loss:[0.6782]
Loss:[0.6766]
Loss:[0.6737]
Loss:[0.6636]
Loss:[0.6587]
Loss:[0.6539]
Loss:[0.6406]
Loss:[0.6350]
Loss:[0.6246]
Loss:[0.6099]
Loss:[0.6024]
Loss:[0.5843]
Loss:[0.5788]
Loss:[0.5599]
Loss:[0.5454]
Loss:[0.5318]
Loss:[0.5144]
Loss:[0.5033]
Loss:[0.4803]
Loss:[0.4733]
Loss:[0.4612]
Loss:[0.4360]
Loss:[0.4330]
Loss:[0.4155]
Loss:[0.3938]
Loss:[0.3847]
Loss:[0.3817]
Loss:[0.3637]
Loss:[0.3501]
Loss:[0.3406]
Loss:[0.3207]
Loss:[0.3146]
Loss:[0.3017]
Loss:[0.2942]
Loss:[0.2827]
Loss:[0.2776]
Loss:[0.2657]
Loss:[0.2608]
Loss:[0.2564]
Loss:[0.2476]
Loss:[0.2400]
Loss:[0.2330]
Loss:[0.2336]
Loss:[0.2267]
Loss:[0.2192]
Loss:[0.2100]
Loss:[0.2074]
Loss:[0.2053]
Loss:[0.2097]
Loss:[0.1970]
Loss:[0.1874]
Loss:[0.2010]
Loss:[0.1753]
Loss:[0.1874]
Loss:[0.1808]
Loss:[0.1713]
Loss:[0.1738]
Loss:[0.1653]
Loss:[0.1538]
Loss:[0.1597]
Loss:[0.1716]
Loss:[0.1563]
Loss:[0.1571]
Loss:[0.1520]
Loss:[0.1532]
Loss:[0.1368]
Loss:[0.1406]
Loss:[0.1400]
Loss:[0.1446]
Loss:[0.1376]
Loss:[0.1358]
Loss:[0.1422]
Loss:[0.1500]
Loss:[0.1286]
Loss:[0.1278]
Loss:[0.1239]
Loss:[0.1251]
Loss:[0.1163]
Loss:[0.1191]
Loss:[0.1170]
Loss:[0.1161]
Loss:[0.1198]
Loss:[0.1126]
Loss:[0.1252]
Loss:[0.0999]
Loss:[0.1136]
Loss:[0.0995]
Loss:[0.1080]
Loss:[0.1164]
Loss:[0.1029]
Loss:[0.1116]
Loss:[0.1150]
Loss:[0.1038]
Loss:[0.0993]
Loss:[0.1182]
Loss:[0.1104]
Loss:[0.0973]
Loss:[0.1085]
Loss:[0.0966]
Loss:[0.0955]
Loss:[0.0977]
Loss:[0.0967]
Loss:[0.0982]
Loss:[0.0833]
Loss:[0.0938]
Loss:[0.0824]
Loss:[0.0992]
Loss:[0.0859]
Loss:[0.0852]
Loss:[0.0850]
Loss:[0.0739]
Loss:[0.0810]
Loss:[0.0761]
Loss:[0.0779]
Loss:[0.0810]
Loss:[0.0674]
Loss:[0.0869]
Loss:[0.0786]
Loss:[0.0772]
Loss:[0.0786]
Loss:[0.0666]
Loss:[0.0766]
Loss:[0.0695]
Loss:[0.0802]
Loss:[0.0829]
Loss:[0.0809]
Loss:[0.0702]
Loss:[0.0802]
Loss:[0.0831]
Loss:[0.0663]
Loss:[0.0685]
Loss:[0.0630]
Loss:[0.0732]
Loss:[0.0683]
Loss:[0.0670]
Loss:[0.0738]
Loss:[0.0668]
Loss:[0.0539]
Loss:[0.0664]
Loss:[0.0624]
Loss:[0.0688]
Loss:[0.0674]
Loss:[0.0723]
Loss:[0.0698]
Loss:[0.0546]
Loss:[0.0533]
Loss:[0.0602]
Loss:[0.0594]
Loss:[0.0572]
Loss:[0.0603]
Loss:[0.0563]
Loss:[0.0723]
Loss:[0.0627]
Loss:[0.0635]
Loss:[0.0612]
Loss:[0.0548]
Loss:[0.0577]
Loss:[0.0546]
Loss:[0.0514]
Loss:[0.0518]
Loss:[0.0513]
Loss:[0.0453]
Loss:[0.0488]
Loss:[0.0546]
Loss:[0.0546]
Loss:[0.0558]
Loss:[0.0529]
Loss:[0.0467]
Loss:[0.0487]
Loss:[0.0493]
Loss:[0.0511]
Loss:[0.0500]
Loss:[0.0448]
Loss:[0.0474]
Loss:[0.0490]
Loss:[0.0446]
Loss:[0.0512]
Loss:[0.0475]
Loss:[0.0424]
Loss:[0.0431]
Loss:[0.0444]
Loss:[0.0447]
Loss:[0.0430]
Loss:[0.0459]
Loss:[0.0472]
Loss:[0.0411]
Loss:[0.0469]
Loss:[0.0370]
Loss:[0.0417]
Loss:[0.0502]
Loss:[0.0512]
Loss:[0.0413]
Loss:[0.0428]
Loss:[0.0445]
Loss:[0.0404]
Loss:[0.0377]
Loss:[0.0428]
Loss:[0.0371]
Loss:[0.0421]
Loss:[0.0346]
Loss:[0.0419]
Loss:[0.0395]
Loss:[0.0392]
Loss:[0.0361]
Loss:[0.0336]
Loss:[0.0377]
Loss:[0.0361]
Loss:[0.0343]
Loss:[0.0352]
Loss:[0.0368]
Loss:[0.0331]
Loss:[0.0370]
Loss:[0.0365]
Loss:[0.0378]
Loss:[0.0334]
Loss:[0.0361]
Loss:[0.0384]
Loss:[0.0372]
Loss:[0.0389]
Loss:[0.0345]
Loss:[0.0343]
Loss:[0.0314]
Loss:[0.0324]
Loss:[0.0352]
Loss:[0.0301]
Loss:[0.0295]
Loss:[0.0254]
Loss:[0.0320]
Loss:[0.0315]
Loss:[0.0324]
Loss:[0.0334]
Loss:[0.0421]
Loss:[0.0321]
Loss:[0.0354]
Loss:[0.0389]
Loss:[0.0300]
Loss:[0.0294]
Loss:[0.0342]
Loss:[0.0295]
Loss:[0.0321]
Loss:[0.0218]
Loss:[0.0362]
Loss:[0.0268]
Loss:[0.0258]
Loss:[0.0318]
Loss:[0.0268]
Loss:[0.0309]
Loss:[0.0265]
Loss:[0.0304]
Loss:[0.0344]
Loss:[0.0323]
Loss:[0.0292]
Loss:[0.0285]
Loss:[0.0310]
Loss:[0.0350]
Loss:[0.0273]
Loss:[0.0270]
Loss:[0.0252]
Loss:[0.0254]
Loss:[0.0252]
Loss:[0.0307]
Early stopping!
Loading 245th epoch
acc:[0.8140]
acc:[0.8180]
acc:[0.8160]
acc:[0.8190]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
acc:[0.8150]
acc:[0.8160]
acc:[0.8130]
acc:[0.8180]
acc:[0.8170]
acc:[0.8190]
acc:[0.8200]
acc:[0.8160]
acc:[0.8180]
acc:[0.8150]
acc:[0.8150]
acc:[0.8120]
acc:[0.8160]
acc:[0.8180]
acc:[0.8190]
acc:[0.8180]
acc:[0.8190]
acc:[0.8170]
acc:[0.8150]
acc:[0.8150]
acc:[0.8190]
acc:[0.8130]
acc:[0.8160]
acc:[0.8110]
acc:[0.8160]
acc:[0.8190]
acc:[0.8190]
acc:[0.8130]
acc:[0.8200]
acc:[0.8170]
acc:[0.8170]
acc:[0.8140]
acc:[0.8160]
acc:[0.8190]
acc:[0.8170]
acc:[0.8190]
acc:[0.8150]
acc:[0.8120]
acc:[0.8160]
acc:[0.8180]
acc:[0.8190]
acc:[0.8180]
acc:[0.8180]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8166]
Mean:[81.6560]
Std :[0.2224]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='cora', drop_percent=0.05, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=31)
----------------------------------------------------------------------------------------------------
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6996]
Loss:[0.6861]
Loss:[0.6905]
Loss:[0.6863]
Loss:[0.6782]
Loss:[0.6766]
Loss:[0.6737]
Loss:[0.6636]
Loss:[0.6587]
Loss:[0.6539]
Loss:[0.6406]
Loss:[0.6350]
Loss:[0.6246]
Loss:[0.6099]
Loss:[0.6024]
Loss:[0.5843]
Loss:[0.5788]
Loss:[0.5599]
Loss:[0.5454]
Loss:[0.5318]
Loss:[0.5144]
Loss:[0.5033]
Loss:[0.4803]
Loss:[0.4733]
Loss:[0.4612]
Loss:[0.4360]
Loss:[0.4330]
Loss:[0.4155]
Loss:[0.3938]
Loss:[0.3847]
Loss:[0.3817]
Loss:[0.3637]
Loss:[0.3500]
Loss:[0.3406]
Loss:[0.3207]
Loss:[0.3145]
Loss:[0.3016]
Loss:[0.2942]
Loss:[0.2826]
Loss:[0.2776]
Loss:[0.2657]
Loss:[0.2608]
Loss:[0.2564]
Loss:[0.2476]
Loss:[0.2400]
Loss:[0.2329]
Loss:[0.2336]
Loss:[0.2268]
Loss:[0.2193]
Loss:[0.2100]
Loss:[0.2074]
Loss:[0.2053]
Loss:[0.2097]
Loss:[0.1970]
Loss:[0.1875]
Loss:[0.2010]
Loss:[0.1752]
Loss:[0.1874]
Loss:[0.1808]
Loss:[0.1713]
Loss:[0.1737]
Loss:[0.1652]
Loss:[0.1538]
Loss:[0.1597]
Loss:[0.1716]
Loss:[0.1562]
Loss:[0.1572]
Loss:[0.1520]
Loss:[0.1532]
Loss:[0.1369]
Loss:[0.1407]
Loss:[0.1400]
Loss:[0.1446]
Loss:[0.1377]
Loss:[0.1358]
Loss:[0.1422]
Loss:[0.1500]
Loss:[0.1287]
Loss:[0.1279]
Loss:[0.1238]
Loss:[0.1252]
Loss:[0.1163]
Loss:[0.1190]
Loss:[0.1170]
Loss:[0.1161]
Loss:[0.1198]
Loss:[0.1125]
Loss:[0.1253]
Loss:[0.0999]
Loss:[0.1136]
Loss:[0.0996]
Loss:[0.1080]
Loss:[0.1164]
Loss:[0.1029]
Loss:[0.1116]
Loss:[0.1150]
Loss:[0.1037]
Loss:[0.0994]
Loss:[0.1181]
Loss:[0.1102]
Loss:[0.0973]
Loss:[0.1082]
Loss:[0.0965]
Loss:[0.0954]
Loss:[0.0977]
Loss:[0.0967]
Loss:[0.0981]
Loss:[0.0833]
Loss:[0.0937]
Loss:[0.0823]
Loss:[0.0991]
Loss:[0.0859]
Loss:[0.0851]
Loss:[0.0850]
Loss:[0.0739]
Loss:[0.0810]
Loss:[0.0761]
Loss:[0.0779]
Loss:[0.0810]
Loss:[0.0674]
Loss:[0.0869]
Loss:[0.0786]
Loss:[0.0771]
Loss:[0.0787]
Loss:[0.0666]
Loss:[0.0765]
Loss:[0.0694]
Loss:[0.0801]
Loss:[0.0829]
Loss:[0.0809]
Loss:[0.0702]
Loss:[0.0802]
Loss:[0.0832]
Loss:[0.0662]
Loss:[0.0685]
Loss:[0.0630]
Loss:[0.0733]
Loss:[0.0683]
Loss:[0.0669]
Loss:[0.0738]
Loss:[0.0667]
Loss:[0.0540]
Loss:[0.0663]
Loss:[0.0625]
Loss:[0.0686]
Loss:[0.0674]
Loss:[0.0721]
Loss:[0.0698]
Loss:[0.0547]
Loss:[0.0533]
Loss:[0.0602]
Loss:[0.0593]
Loss:[0.0573]
Loss:[0.0603]
Loss:[0.0565]
Loss:[0.0722]
Loss:[0.0627]
Loss:[0.0633]
Loss:[0.0612]
Loss:[0.0547]
Loss:[0.0576]
Loss:[0.0546]
Loss:[0.0514]
Loss:[0.0519]
Loss:[0.0512]
Loss:[0.0453]
Loss:[0.0486]
Loss:[0.0547]
Loss:[0.0544]
Loss:[0.0558]
Loss:[0.0529]
Loss:[0.0466]
Loss:[0.0487]
Loss:[0.0492]
Loss:[0.0510]
Loss:[0.0501]
Loss:[0.0448]
Loss:[0.0474]
Loss:[0.0490]
Loss:[0.0444]
Loss:[0.0511]
Loss:[0.0474]
Loss:[0.0423]
Loss:[0.0430]
Loss:[0.0443]
Loss:[0.0446]
Loss:[0.0430]
Loss:[0.0459]
Loss:[0.0471]
Loss:[0.0410]
Loss:[0.0468]
Loss:[0.0369]
Loss:[0.0416]
Loss:[0.0502]
Loss:[0.0512]
Loss:[0.0413]
Loss:[0.0428]
Loss:[0.0444]
Loss:[0.0403]
Loss:[0.0377]
Loss:[0.0428]
Loss:[0.0371]
Loss:[0.0421]
Loss:[0.0346]
Loss:[0.0419]
Loss:[0.0395]
Loss:[0.0391]
Loss:[0.0361]
Loss:[0.0335]
Loss:[0.0378]
Loss:[0.0361]
Loss:[0.0343]
Loss:[0.0352]
Loss:[0.0368]
Loss:[0.0331]
Loss:[0.0370]
Loss:[0.0367]
Loss:[0.0378]
Loss:[0.0334]
Loss:[0.0360]
Loss:[0.0383]
Loss:[0.0373]
Loss:[0.0389]
Loss:[0.0345]
Loss:[0.0343]
Loss:[0.0313]
Loss:[0.0324]
Loss:[0.0352]
Loss:[0.0301]
Loss:[0.0294]
Loss:[0.0255]
Loss:[0.0320]
Loss:[0.0315]
Loss:[0.0324]
Loss:[0.0334]
Loss:[0.0420]
Loss:[0.0322]
Loss:[0.0356]
Loss:[0.0389]
Loss:[0.0300]
Loss:[0.0294]
Loss:[0.0342]
Loss:[0.0295]
Loss:[0.0321]
Loss:[0.0218]
Loss:[0.0362]
Loss:[0.0268]
Loss:[0.0258]
Loss:[0.0318]
Loss:[0.0268]
Loss:[0.0308]
Loss:[0.0265]
Loss:[0.0303]
Loss:[0.0344]
Loss:[0.0324]
Loss:[0.0292]
Loss:[0.0285]
Loss:[0.0309]
Loss:[0.0351]
Loss:[0.0272]
Loss:[0.0270]
Loss:[0.0250]
Loss:[0.0253]
Loss:[0.0251]
Loss:[0.0306]
Early stopping!
Loading 245th epoch
acc:[0.8150]
acc:[0.8190]
acc:[0.8180]
acc:[0.8200]
acc:[0.8180]
acc:[0.8170]
acc:[0.8180]
acc:[0.8140]
acc:[0.8160]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
acc:[0.8200]
acc:[0.8180]
acc:[0.8180]
acc:[0.8160]
acc:[0.8180]
acc:[0.8180]
acc:[0.8190]
acc:[0.8180]
acc:[0.8190]
acc:[0.8200]
acc:[0.8170]
acc:[0.8190]
acc:[0.8160]
acc:[0.8190]
acc:[0.8160]
acc:[0.8220]
acc:[0.8170]
acc:[0.8170]
acc:[0.8120]
acc:[0.8180]
acc:[0.8190]
acc:[0.8200]
acc:[0.8190]
acc:[0.8210]
acc:[0.8200]
acc:[0.8200]
acc:[0.8160]
acc:[0.8170]
acc:[0.8200]
acc:[0.8180]
acc:[0.8220]
acc:[0.8160]
acc:[0.8180]
acc:[0.8180]
acc:[0.8190]
acc:[0.8200]
acc:[0.8190]
acc:[0.8190]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8180]
Mean:[81.8040]
Std :[0.1927]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='cora', drop_percent=0.08, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=31)
----------------------------------------------------------------------------------------------------
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6996]
Loss:[0.6861]
Loss:[0.6905]
Loss:[0.6863]
Loss:[0.6782]
Loss:[0.6766]
Loss:[0.6737]
Loss:[0.6636]
Loss:[0.6587]
Loss:[0.6539]
Loss:[0.6406]
Loss:[0.6350]
Loss:[0.6246]
Loss:[0.6099]
Loss:[0.6024]
Loss:[0.5843]
Loss:[0.5788]
Loss:[0.5599]
Loss:[0.5454]
Loss:[0.5318]
Loss:[0.5144]
Loss:[0.5033]
Loss:[0.4803]
Loss:[0.4733]
Loss:[0.4612]
Loss:[0.4360]
Loss:[0.4330]
Loss:[0.4155]
Loss:[0.3938]
Loss:[0.3847]
Loss:[0.3817]
Loss:[0.3637]
Loss:[0.3501]
Loss:[0.3406]
Loss:[0.3207]
Loss:[0.3146]
Loss:[0.3017]
Loss:[0.2942]
Loss:[0.2826]
Loss:[0.2776]
Loss:[0.2657]
Loss:[0.2608]
Loss:[0.2564]
Loss:[0.2476]
Loss:[0.2400]
Loss:[0.2329]
Loss:[0.2336]
Loss:[0.2267]
Loss:[0.2192]
Loss:[0.2100]
Loss:[0.2075]
Loss:[0.2053]
Loss:[0.2097]
Loss:[0.1968]
Loss:[0.1874]
Loss:[0.2011]
Loss:[0.1754]
Loss:[0.1875]
Loss:[0.1809]
Loss:[0.1713]
Loss:[0.1738]
Loss:[0.1654]
Loss:[0.1539]
Loss:[0.1596]
Loss:[0.1717]
Loss:[0.1564]
Loss:[0.1572]
Loss:[0.1521]
Loss:[0.1535]
Loss:[0.1368]
Loss:[0.1403]
Loss:[0.1401]
Loss:[0.1446]
Loss:[0.1375]
Loss:[0.1360]
Loss:[0.1425]
Loss:[0.1500]
Loss:[0.1283]
Loss:[0.1276]
Loss:[0.1240]
Loss:[0.1251]
Loss:[0.1163]
Loss:[0.1195]
Loss:[0.1170]
Loss:[0.1156]
Loss:[0.1200]
Loss:[0.1125]
Loss:[0.1240]
Loss:[0.0998]
Loss:[0.1133]
Loss:[0.0991]
Loss:[0.1080]
Loss:[0.1156]
Loss:[0.1022]
Loss:[0.1115]
Loss:[0.1133]
Loss:[0.1023]
Loss:[0.0986]
Loss:[0.1155]
Loss:[0.1105]
Loss:[0.0951]
Loss:[0.1097]
Loss:[0.0999]
Loss:[0.0951]
Loss:[0.1026]
Loss:[0.0952]
Loss:[0.0992]
Loss:[0.0833]
Loss:[0.0933]
Loss:[0.0838]
Loss:[0.0986]
Loss:[0.0879]
Loss:[0.0833]
Loss:[0.0853]
Loss:[0.0741]
Loss:[0.0808]
Loss:[0.0764]
Loss:[0.0775]
Loss:[0.0816]
Loss:[0.0665]
Loss:[0.0872]
Loss:[0.0791]
Loss:[0.0767]
Loss:[0.0795]
Loss:[0.0664]
Loss:[0.0770]
Loss:[0.0691]
Loss:[0.0803]
Loss:[0.0830]
Loss:[0.0806]
Loss:[0.0704]
Loss:[0.0798]
Loss:[0.0836]
Loss:[0.0655]
Loss:[0.0684]
Loss:[0.0626]
Loss:[0.0733]
Loss:[0.0683]
Loss:[0.0667]
Loss:[0.0739]
Loss:[0.0664]
Loss:[0.0540]
Loss:[0.0659]
Loss:[0.0625]
Loss:[0.0682]
Loss:[0.0672]
Loss:[0.0717]
Loss:[0.0694]
Loss:[0.0547]
Loss:[0.0531]
Loss:[0.0602]
Loss:[0.0589]
Loss:[0.0573]
Loss:[0.0600]
Loss:[0.0566]
Loss:[0.0719]
Loss:[0.0626]
Loss:[0.0629]
Loss:[0.0611]
Loss:[0.0544]
Loss:[0.0575]
Loss:[0.0545]
Loss:[0.0511]
Loss:[0.0520]
Loss:[0.0511]
Loss:[0.0455]
Loss:[0.0484]
Loss:[0.0549]
Loss:[0.0544]
Loss:[0.0559]
Loss:[0.0529]
Loss:[0.0466]
Loss:[0.0486]
Loss:[0.0492]
Loss:[0.0508]
Loss:[0.0499]
Loss:[0.0448]
Loss:[0.0474]
Loss:[0.0490]
Loss:[0.0444]
Loss:[0.0509]
Loss:[0.0473]
Loss:[0.0422]
Loss:[0.0430]
Loss:[0.0442]
Loss:[0.0446]
Loss:[0.0429]
Loss:[0.0459]
Loss:[0.0471]
Loss:[0.0409]
Loss:[0.0468]
Loss:[0.0370]
Loss:[0.0417]
Loss:[0.0502]
Loss:[0.0512]
Loss:[0.0412]
Loss:[0.0428]
Loss:[0.0444]
Loss:[0.0403]
Loss:[0.0376]
Loss:[0.0426]
Loss:[0.0371]
Loss:[0.0420]
Loss:[0.0345]
Loss:[0.0419]
Loss:[0.0395]
Loss:[0.0390]
Loss:[0.0361]
Loss:[0.0335]
Loss:[0.0378]
Loss:[0.0360]
Loss:[0.0342]
Loss:[0.0352]
Loss:[0.0369]
Loss:[0.0330]
Loss:[0.0370]
Loss:[0.0365]
Loss:[0.0377]
Loss:[0.0333]
Loss:[0.0360]
Loss:[0.0383]
Loss:[0.0371]
Loss:[0.0390]
Loss:[0.0345]
Loss:[0.0344]
Loss:[0.0313]
Loss:[0.0324]
Loss:[0.0352]
Loss:[0.0301]
Loss:[0.0293]
Loss:[0.0254]
Loss:[0.0318]
Loss:[0.0316]
Loss:[0.0324]
Loss:[0.0334]
Loss:[0.0419]
Loss:[0.0323]
Loss:[0.0356]
Loss:[0.0388]
Loss:[0.0303]
Loss:[0.0294]
Loss:[0.0343]
Loss:[0.0295]
Loss:[0.0320]
Loss:[0.0217]
Loss:[0.0363]
Loss:[0.0268]
Loss:[0.0258]
Loss:[0.0320]
Loss:[0.0267]
Loss:[0.0309]
Loss:[0.0264]
Loss:[0.0305]
Loss:[0.0342]
Loss:[0.0324]
Loss:[0.0291]
Loss:[0.0286]
Loss:[0.0308]
Loss:[0.0351]
Loss:[0.0272]
Loss:[0.0270]
Loss:[0.0250]
Loss:[0.0254]
Loss:[0.0251]
Loss:[0.0306]
Early stopping!
Loading 245th epoch
acc:[0.8180]
acc:[0.8190]
acc:[0.8180]
acc:[0.8170]
acc:[0.8200]
acc:[0.8190]
acc:[0.8180]
acc:[0.8180]
acc:[0.8200]
acc:[0.8190]
acc:[0.8180]
acc:[0.8190]
acc:[0.8200]
acc:[0.8210]
acc:[0.8230]
acc:[0.8190]
acc:[0.8210]
acc:[0.8180]
acc:[0.8180]
acc:[0.8180]
acc:[0.8190]
acc:[0.8200]
acc:[0.8210]
acc:[0.8170]
acc:[0.8170]
acc:[0.8160]
acc:[0.8190]
acc:[0.8180]
acc:[0.8200]
acc:[0.8190]
acc:[0.8130]
acc:[0.8180]
acc:[0.8210]
acc:[0.8190]
acc:[0.8170]
acc:[0.8190]
acc:[0.8200]
acc:[0.8190]
acc:[0.8180]
acc:[0.8180]
acc:[0.8210]
acc:[0.8190]
acc:[0.8170]
acc:[0.8160]
acc:[0.8170]
acc:[0.8180]
acc:[0.8190]
acc:[0.8210]
acc:[0.8190]
acc:[0.8230]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8188]
Mean:[81.8780]
Std :[0.1753]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='cora', drop_percent=0.1, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=31)
----------------------------------------------------------------------------------------------------
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6996]
Loss:[0.6861]
Loss:[0.6905]
Loss:[0.6863]
Loss:[0.6782]
Loss:[0.6766]
Loss:[0.6737]
Loss:[0.6636]
Loss:[0.6587]
Loss:[0.6539]
Loss:[0.6406]
Loss:[0.6350]
Loss:[0.6246]
Loss:[0.6099]
Loss:[0.6024]
Loss:[0.5843]
Loss:[0.5788]
Loss:[0.5599]
Loss:[0.5454]
Loss:[0.5318]
Loss:[0.5144]
Loss:[0.5034]
Loss:[0.4803]
Loss:[0.4733]
Loss:[0.4612]
Loss:[0.4360]
Loss:[0.4331]
Loss:[0.4155]
Loss:[0.3938]
Loss:[0.3847]
Loss:[0.3817]
Loss:[0.3637]
Loss:[0.3501]
Loss:[0.3406]
Loss:[0.3207]
Loss:[0.3146]
Loss:[0.3017]
Loss:[0.2941]
Loss:[0.2826]
Loss:[0.2777]
Loss:[0.2657]
Loss:[0.2608]
Loss:[0.2564]
Loss:[0.2477]
Loss:[0.2400]
Loss:[0.2329]
Loss:[0.2336]
Loss:[0.2268]
Loss:[0.2193]
Loss:[0.2099]
Loss:[0.2074]
Loss:[0.2053]
Loss:[0.2097]
Loss:[0.1969]
Loss:[0.1874]
Loss:[0.2011]
Loss:[0.1754]
Loss:[0.1875]
Loss:[0.1808]
Loss:[0.1713]
Loss:[0.1738]
Loss:[0.1653]
Loss:[0.1539]
Loss:[0.1596]
Loss:[0.1716]
Loss:[0.1563]
Loss:[0.1572]
Loss:[0.1520]
Loss:[0.1532]
Loss:[0.1368]
Loss:[0.1406]
Loss:[0.1400]
Loss:[0.1446]
Loss:[0.1376]
Loss:[0.1358]
Loss:[0.1422]
Loss:[0.1499]
Loss:[0.1286]
Loss:[0.1278]
Loss:[0.1239]
Loss:[0.1251]
Loss:[0.1162]
Loss:[0.1191]
Loss:[0.1170]
Loss:[0.1161]
Loss:[0.1198]
Loss:[0.1126]
Loss:[0.1251]
Loss:[0.0999]
Loss:[0.1136]
Loss:[0.0995]
Loss:[0.1079]
Loss:[0.1163]
Loss:[0.1028]
Loss:[0.1115]
Loss:[0.1147]
Loss:[0.1036]
Loss:[0.0992]
Loss:[0.1178]
Loss:[0.1104]
Loss:[0.0969]
Loss:[0.1086]
Loss:[0.0969]
Loss:[0.0955]
Loss:[0.0982]
Loss:[0.0964]
Loss:[0.0983]
Loss:[0.0832]
Loss:[0.0938]
Loss:[0.0825]
Loss:[0.0991]
Loss:[0.0861]
Loss:[0.0849]
Loss:[0.0850]
Loss:[0.0738]
Loss:[0.0810]
Loss:[0.0760]
Loss:[0.0779]
Loss:[0.0810]
Loss:[0.0673]
Loss:[0.0869]
Loss:[0.0786]
Loss:[0.0771]
Loss:[0.0787]
Loss:[0.0665]
Loss:[0.0766]
Loss:[0.0693]
Loss:[0.0802]
Loss:[0.0829]
Loss:[0.0809]
Loss:[0.0702]
Loss:[0.0802]
Loss:[0.0832]
Loss:[0.0662]
Loss:[0.0686]
Loss:[0.0630]
Loss:[0.0733]
Loss:[0.0683]
Loss:[0.0669]
Loss:[0.0738]
Loss:[0.0667]
Loss:[0.0540]
Loss:[0.0663]
Loss:[0.0625]
Loss:[0.0687]
Loss:[0.0674]
Loss:[0.0721]
Loss:[0.0696]
Loss:[0.0547]
Loss:[0.0533]
Loss:[0.0602]
Loss:[0.0593]
Loss:[0.0573]
Loss:[0.0603]
Loss:[0.0565]
Loss:[0.0722]
Loss:[0.0626]
Loss:[0.0633]
Loss:[0.0611]
Loss:[0.0546]
Loss:[0.0576]
Loss:[0.0546]
Loss:[0.0513]
Loss:[0.0519]
Loss:[0.0512]
Loss:[0.0453]
Loss:[0.0486]
Loss:[0.0548]
Loss:[0.0545]
Loss:[0.0558]
Loss:[0.0528]
Loss:[0.0467]
Loss:[0.0487]
Loss:[0.0493]
Loss:[0.0510]
Loss:[0.0500]
Loss:[0.0447]
Loss:[0.0473]
Loss:[0.0490]
Loss:[0.0445]
Loss:[0.0511]
Loss:[0.0474]
Loss:[0.0423]
Loss:[0.0430]
Loss:[0.0443]
Loss:[0.0447]
Loss:[0.0430]
Loss:[0.0459]
Loss:[0.0471]
Loss:[0.0410]
Loss:[0.0468]
Loss:[0.0370]
Loss:[0.0417]
Loss:[0.0502]
Loss:[0.0512]
Loss:[0.0413]
Loss:[0.0428]
Loss:[0.0444]
Loss:[0.0404]
Loss:[0.0376]
Loss:[0.0427]
Loss:[0.0371]
Loss:[0.0420]
Loss:[0.0346]
Loss:[0.0419]
Loss:[0.0394]
Loss:[0.0391]
Loss:[0.0362]
Loss:[0.0336]
Loss:[0.0377]
Loss:[0.0360]
Loss:[0.0343]
Loss:[0.0353]
Loss:[0.0369]
Loss:[0.0330]
Loss:[0.0370]
Loss:[0.0365]
Loss:[0.0379]
Loss:[0.0334]
Loss:[0.0360]
Loss:[0.0384]
Loss:[0.0372]
Loss:[0.0389]
Loss:[0.0345]
Loss:[0.0343]
Loss:[0.0312]
Loss:[0.0325]
Loss:[0.0352]
Loss:[0.0302]
Loss:[0.0293]
Loss:[0.0254]
Loss:[0.0319]
Loss:[0.0315]
Loss:[0.0323]
Loss:[0.0335]
Loss:[0.0421]
Loss:[0.0322]
Loss:[0.0356]
Loss:[0.0389]
Loss:[0.0301]
Loss:[0.0294]
Loss:[0.0342]
Loss:[0.0295]
Loss:[0.0320]
Loss:[0.0218]
Loss:[0.0362]
Loss:[0.0269]
Loss:[0.0258]
Loss:[0.0320]
Loss:[0.0268]
Loss:[0.0308]
Loss:[0.0264]
Loss:[0.0304]
Loss:[0.0343]
Loss:[0.0324]
Loss:[0.0291]
Loss:[0.0286]
Loss:[0.0308]
Loss:[0.0349]
Loss:[0.0272]
Loss:[0.0269]
Loss:[0.0250]
Loss:[0.0254]
Loss:[0.0252]
Loss:[0.0307]
Early stopping!
Loading 245th epoch
acc:[0.8150]
acc:[0.8190]
acc:[0.8180]
acc:[0.8180]
acc:[0.8170]
acc:[0.8170]
acc:[0.8130]
acc:[0.8160]
acc:[0.8170]
acc:[0.8140]
acc:[0.8170]
acc:[0.8170]
acc:[0.8190]
acc:[0.8180]
acc:[0.8190]
acc:[0.8160]
acc:[0.8180]
acc:[0.8200]
acc:[0.8160]
acc:[0.8160]
acc:[0.8180]
acc:[0.8200]
acc:[0.8180]
acc:[0.8170]
acc:[0.8150]
acc:[0.8160]
acc:[0.8160]
acc:[0.8200]
acc:[0.8130]
acc:[0.8170]
acc:[0.8120]
acc:[0.8190]
acc:[0.8180]
acc:[0.8190]
acc:[0.8180]
acc:[0.8200]
acc:[0.8180]
acc:[0.8180]
acc:[0.8160]
acc:[0.8160]
acc:[0.8200]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
acc:[0.8160]
acc:[0.8170]
acc:[0.8150]
acc:[0.8190]
acc:[0.8180]
acc:[0.8210]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8172]
Mean:[81.7180]
Std :[0.1935]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='cora', drop_percent=0.2, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=31)
----------------------------------------------------------------------------------------------------
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6996]
Loss:[0.6861]
Loss:[0.6905]
Loss:[0.6863]
Loss:[0.6782]
Loss:[0.6766]
Loss:[0.6737]
Loss:[0.6636]
Loss:[0.6587]
Loss:[0.6539]
Loss:[0.6406]
Loss:[0.6350]
Loss:[0.6246]
Loss:[0.6099]
Loss:[0.6024]
Loss:[0.5843]
Loss:[0.5788]
Loss:[0.5599]
Loss:[0.5454]
Loss:[0.5318]
Loss:[0.5144]
Loss:[0.5033]
Loss:[0.4803]
Loss:[0.4733]
Loss:[0.4612]
Loss:[0.4360]
Loss:[0.4330]
Loss:[0.4155]
Loss:[0.3938]
Loss:[0.3846]
Loss:[0.3817]
Loss:[0.3637]
Loss:[0.3500]
Loss:[0.3406]
Loss:[0.3207]
Loss:[0.3145]
Loss:[0.3016]
Loss:[0.2942]
Loss:[0.2826]
Loss:[0.2776]
Loss:[0.2657]
Loss:[0.2608]
Loss:[0.2564]
Loss:[0.2476]
Loss:[0.2400]
Loss:[0.2329]
Loss:[0.2336]
Loss:[0.2267]
Loss:[0.2192]
Loss:[0.2099]
Loss:[0.2074]
Loss:[0.2053]
Loss:[0.2097]
Loss:[0.1969]
Loss:[0.1874]
Loss:[0.2010]
Loss:[0.1752]
Loss:[0.1874]
Loss:[0.1808]
Loss:[0.1713]
Loss:[0.1737]
Loss:[0.1653]
Loss:[0.1539]
Loss:[0.1597]
Loss:[0.1716]
Loss:[0.1563]
Loss:[0.1572]
Loss:[0.1520]
Loss:[0.1531]
Loss:[0.1368]
Loss:[0.1407]
Loss:[0.1399]
Loss:[0.1445]
Loss:[0.1376]
Loss:[0.1358]
Loss:[0.1421]
Loss:[0.1499]
Loss:[0.1287]
Loss:[0.1279]
Loss:[0.1238]
Loss:[0.1252]
Loss:[0.1162]
Loss:[0.1189]
Loss:[0.1170]
Loss:[0.1163]
Loss:[0.1198]
Loss:[0.1126]
Loss:[0.1253]
Loss:[0.0999]
Loss:[0.1136]
Loss:[0.0997]
Loss:[0.1079]
Loss:[0.1164]
Loss:[0.1030]
Loss:[0.1116]
Loss:[0.1150]
Loss:[0.1038]
Loss:[0.0993]
Loss:[0.1182]
Loss:[0.1102]
Loss:[0.0974]
Loss:[0.1084]
Loss:[0.0964]
Loss:[0.0955]
Loss:[0.0976]
Loss:[0.0968]
Loss:[0.0981]
Loss:[0.0833]
Loss:[0.0938]
Loss:[0.0823]
Loss:[0.0992]
Loss:[0.0858]
Loss:[0.0852]
Loss:[0.0850]
Loss:[0.0738]
Loss:[0.0810]
Loss:[0.0760]
Loss:[0.0779]
Loss:[0.0809]
Loss:[0.0674]
Loss:[0.0868]
Loss:[0.0786]
Loss:[0.0771]
Loss:[0.0785]
Loss:[0.0666]
Loss:[0.0766]
Loss:[0.0694]
Loss:[0.0802]
Loss:[0.0829]
Loss:[0.0810]
Loss:[0.0702]
Loss:[0.0803]
Loss:[0.0831]
Loss:[0.0663]
Loss:[0.0685]
Loss:[0.0631]
Loss:[0.0733]
Loss:[0.0683]
Loss:[0.0670]
Loss:[0.0738]
Loss:[0.0668]
Loss:[0.0539]
Loss:[0.0664]
Loss:[0.0625]
Loss:[0.0688]
Loss:[0.0675]
Loss:[0.0723]
Loss:[0.0698]
Loss:[0.0547]
Loss:[0.0533]
Loss:[0.0602]
Loss:[0.0595]
Loss:[0.0573]
Loss:[0.0604]
Loss:[0.0564]
Loss:[0.0725]
Loss:[0.0628]
Loss:[0.0634]
Loss:[0.0612]
Loss:[0.0546]
Loss:[0.0577]
Loss:[0.0547]
Loss:[0.0514]
Loss:[0.0519]
Loss:[0.0513]
Loss:[0.0453]
Loss:[0.0487]
Loss:[0.0547]
Loss:[0.0546]
Loss:[0.0558]
Loss:[0.0528]
Loss:[0.0467]
Loss:[0.0487]
Loss:[0.0494]
Loss:[0.0511]
Loss:[0.0502]
Loss:[0.0447]
Loss:[0.0474]
Loss:[0.0490]
Loss:[0.0445]
Loss:[0.0511]
Loss:[0.0476]
Loss:[0.0423]
Loss:[0.0430]
Loss:[0.0444]
Loss:[0.0447]
Loss:[0.0431]
Loss:[0.0459]
Loss:[0.0472]
Loss:[0.0412]
Loss:[0.0469]
Loss:[0.0370]
Loss:[0.0418]
Loss:[0.0502]
Loss:[0.0512]
Loss:[0.0414]
Loss:[0.0428]
Loss:[0.0444]
Loss:[0.0404]
Loss:[0.0378]
Loss:[0.0427]
Loss:[0.0371]
Loss:[0.0421]
Loss:[0.0346]
Loss:[0.0418]
Loss:[0.0395]
Loss:[0.0392]
Loss:[0.0362]
Loss:[0.0335]
Loss:[0.0379]
Loss:[0.0361]
Loss:[0.0343]
Loss:[0.0353]
Loss:[0.0369]
Loss:[0.0330]
Loss:[0.0370]
Loss:[0.0366]
Loss:[0.0378]
Loss:[0.0334]
Loss:[0.0361]
Loss:[0.0384]
Loss:[0.0373]
Loss:[0.0389]
Loss:[0.0345]
Loss:[0.0343]
Loss:[0.0313]
Loss:[0.0325]
Loss:[0.0352]
Loss:[0.0302]
Loss:[0.0294]
Loss:[0.0254]
Loss:[0.0320]
Loss:[0.0314]
Loss:[0.0324]
Loss:[0.0334]
Loss:[0.0420]
Loss:[0.0322]
Loss:[0.0356]
Loss:[0.0388]
Loss:[0.0300]
Loss:[0.0294]
Loss:[0.0341]
Loss:[0.0295]
Loss:[0.0322]
Loss:[0.0218]
Loss:[0.0362]
Loss:[0.0269]
Loss:[0.0259]
Loss:[0.0318]
Loss:[0.0268]
Loss:[0.0308]
Loss:[0.0265]
Loss:[0.0304]
Loss:[0.0344]
Loss:[0.0323]
Loss:[0.0293]
Loss:[0.0285]
Loss:[0.0310]
Loss:[0.0349]
Loss:[0.0273]
Loss:[0.0270]
Loss:[0.0251]
Loss:[0.0253]
Loss:[0.0252]
Loss:[0.0306]
Early stopping!
Loading 245th epoch
acc:[0.8170]
acc:[0.8170]
acc:[0.8170]
acc:[0.8180]
acc:[0.8180]
acc:[0.8170]
acc:[0.8180]
acc:[0.8170]
acc:[0.8180]
acc:[0.8180]
acc:[0.8160]
acc:[0.8190]
acc:[0.8170]
acc:[0.8170]
acc:[0.8170]
acc:[0.8170]
acc:[0.8170]
acc:[0.8180]
acc:[0.8170]
acc:[0.8190]
acc:[0.8200]
acc:[0.8170]
acc:[0.8160]
acc:[0.8180]
acc:[0.8160]
acc:[0.8170]
acc:[0.8170]
acc:[0.8170]
acc:[0.8170]
acc:[0.8180]
acc:[0.8150]
acc:[0.8170]
acc:[0.8200]
acc:[0.8170]
acc:[0.8170]
acc:[0.8190]
acc:[0.8170]
acc:[0.8180]
acc:[0.8180]
acc:[0.8170]
acc:[0.8210]
acc:[0.8180]
acc:[0.8170]
acc:[0.8160]
acc:[0.8180]
acc:[0.8180]
acc:[0.8170]
acc:[0.8190]
acc:[0.8190]
acc:[0.8190]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8176]
Mean:[81.7580]
Std :[0.1126]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='cora', drop_percent=0.3, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=31)
----------------------------------------------------------------------------------------------------
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6996]
Loss:[0.6861]
Loss:[0.6905]
Loss:[0.6863]
Loss:[0.6782]
Loss:[0.6766]
Loss:[0.6737]
Loss:[0.6636]
Loss:[0.6587]
Loss:[0.6539]
Loss:[0.6406]
Loss:[0.6350]
Loss:[0.6246]
Loss:[0.6099]
Loss:[0.6024]
Loss:[0.5843]
Loss:[0.5788]
Loss:[0.5599]
Loss:[0.5454]
Loss:[0.5317]
Loss:[0.5144]
Loss:[0.5033]
Loss:[0.4803]
Loss:[0.4733]
Loss:[0.4612]
Loss:[0.4360]
Loss:[0.4329]
Loss:[0.4155]
Loss:[0.3938]
Loss:[0.3846]
Loss:[0.3817]
Loss:[0.3637]
Loss:[0.3499]
Loss:[0.3406]
Loss:[0.3207]
Loss:[0.3144]
Loss:[0.3016]
Loss:[0.2943]
Loss:[0.2827]
Loss:[0.2775]
Loss:[0.2657]
Loss:[0.2609]
Loss:[0.2564]
Loss:[0.2475]
Loss:[0.2400]
Loss:[0.2331]
Loss:[0.2336]
Loss:[0.2267]
Loss:[0.2192]
Loss:[0.2100]
Loss:[0.2075]
Loss:[0.2054]
Loss:[0.2096]
Loss:[0.1969]
Loss:[0.1874]
Loss:[0.2011]
Loss:[0.1752]
Loss:[0.1873]
Loss:[0.1808]
Loss:[0.1713]
Loss:[0.1738]
Loss:[0.1653]
Loss:[0.1538]
Loss:[0.1597]
Loss:[0.1716]
Loss:[0.1563]
Loss:[0.1571]
Loss:[0.1521]
Loss:[0.1534]
Loss:[0.1368]
Loss:[0.1405]
Loss:[0.1400]
Loss:[0.1446]
Loss:[0.1376]
Loss:[0.1359]
Loss:[0.1424]
Loss:[0.1500]
Loss:[0.1283]
Loss:[0.1277]
Loss:[0.1240]
Loss:[0.1251]
Loss:[0.1162]
Loss:[0.1194]
Loss:[0.1170]
Loss:[0.1158]
Loss:[0.1200]
Loss:[0.1125]
Loss:[0.1245]
Loss:[0.0998]
Loss:[0.1134]
Loss:[0.0993]
Loss:[0.1080]
Loss:[0.1159]
Loss:[0.1024]
Loss:[0.1115]
Loss:[0.1138]
Loss:[0.1028]
Loss:[0.0988]
Loss:[0.1164]
Loss:[0.1104]
Loss:[0.0958]
Loss:[0.1093]
Loss:[0.0987]
Loss:[0.0952]
Loss:[0.1009]
Loss:[0.0955]
Loss:[0.0988]
Loss:[0.0831]
Loss:[0.0935]
Loss:[0.0832]
Loss:[0.0988]
Loss:[0.0872]
Loss:[0.0838]
Loss:[0.0852]
Loss:[0.0739]
Loss:[0.0808]
Loss:[0.0763]
Loss:[0.0776]
Loss:[0.0813]
Loss:[0.0666]
Loss:[0.0870]
Loss:[0.0790]
Loss:[0.0767]
Loss:[0.0792]
Loss:[0.0663]
Loss:[0.0769]
Loss:[0.0691]
Loss:[0.0802]
Loss:[0.0830]
Loss:[0.0807]
Loss:[0.0704]
Loss:[0.0798]
Loss:[0.0835]
Loss:[0.0656]
Loss:[0.0684]
Loss:[0.0626]
Loss:[0.0732]
Loss:[0.0683]
Loss:[0.0667]
Loss:[0.0740]
Loss:[0.0663]
Loss:[0.0540]
Loss:[0.0658]
Loss:[0.0625]
Loss:[0.0679]
Loss:[0.0671]
Loss:[0.0715]
Loss:[0.0694]
Loss:[0.0549]
Loss:[0.0530]
Loss:[0.0602]
Loss:[0.0586]
Loss:[0.0575]
Loss:[0.0599]
Loss:[0.0569]
Loss:[0.0715]
Loss:[0.0625]
Loss:[0.0626]
Loss:[0.0611]
Loss:[0.0542]
Loss:[0.0574]
Loss:[0.0545]
Loss:[0.0510]
Loss:[0.0521]
Loss:[0.0509]
Loss:[0.0457]
Loss:[0.0481]
Loss:[0.0549]
Loss:[0.0543]
Loss:[0.0559]
Loss:[0.0530]
Loss:[0.0467]
Loss:[0.0487]
Loss:[0.0493]
Loss:[0.0509]
Loss:[0.0498]
Loss:[0.0450]
Loss:[0.0473]
Loss:[0.0491]
Loss:[0.0446]
Loss:[0.0509]
Loss:[0.0473]
Loss:[0.0422]
Loss:[0.0430]
Loss:[0.0442]
Loss:[0.0445]
Loss:[0.0429]
Loss:[0.0459]
Loss:[0.0470]
Loss:[0.0409]
Loss:[0.0469]
Loss:[0.0369]
Loss:[0.0417]
Loss:[0.0502]
Loss:[0.0512]
Loss:[0.0413]
Loss:[0.0428]
Loss:[0.0444]
Loss:[0.0405]
Loss:[0.0377]
Loss:[0.0428]
Loss:[0.0371]
Loss:[0.0421]
Loss:[0.0346]
Loss:[0.0418]
Loss:[0.0394]
Loss:[0.0391]
Loss:[0.0361]
Loss:[0.0335]
Loss:[0.0377]
Loss:[0.0360]
Loss:[0.0343]
Loss:[0.0351]
Loss:[0.0369]
Loss:[0.0330]
Loss:[0.0370]
Loss:[0.0366]
Loss:[0.0378]
Loss:[0.0333]
Loss:[0.0361]
Loss:[0.0383]
Loss:[0.0372]
Loss:[0.0391]
Loss:[0.0345]
Loss:[0.0343]
Loss:[0.0312]
Loss:[0.0325]
Loss:[0.0351]
Loss:[0.0301]
Loss:[0.0293]
Loss:[0.0254]
Loss:[0.0318]
Loss:[0.0316]
Loss:[0.0323]
Loss:[0.0333]
Loss:[0.0420]
Loss:[0.0323]
Loss:[0.0357]
Loss:[0.0388]
Loss:[0.0302]
Loss:[0.0293]
Loss:[0.0341]
Loss:[0.0295]
Loss:[0.0321]
Loss:[0.0217]
Loss:[0.0363]
Loss:[0.0268]
Loss:[0.0258]
Loss:[0.0319]
Loss:[0.0267]
Loss:[0.0309]
Loss:[0.0263]
Loss:[0.0306]
Loss:[0.0342]
Loss:[0.0323]
Loss:[0.0290]
Loss:[0.0285]
Loss:[0.0308]
Loss:[0.0351]
Loss:[0.0271]
Loss:[0.0270]
Loss:[0.0248]
Loss:[0.0254]
Loss:[0.0251]
Loss:[0.0305]
Early stopping!
Loading 245th epoch
acc:[0.8160]
acc:[0.8200]
acc:[0.8210]
acc:[0.8180]
acc:[0.8200]
acc:[0.8190]
acc:[0.8170]
acc:[0.8170]
acc:[0.8160]
acc:[0.8190]
acc:[0.8200]
acc:[0.8180]
acc:[0.8190]
acc:[0.8200]
acc:[0.8190]
acc:[0.8170]
acc:[0.8180]
acc:[0.8190]
acc:[0.8190]
acc:[0.8180]
acc:[0.8170]
acc:[0.8200]
acc:[0.8210]
acc:[0.8190]
acc:[0.8180]
acc:[0.8170]
acc:[0.8200]
acc:[0.8200]
acc:[0.8170]
acc:[0.8160]
acc:[0.8150]
acc:[0.8200]
acc:[0.8190]
acc:[0.8210]
acc:[0.8170]
acc:[0.8180]
acc:[0.8200]
acc:[0.8190]
acc:[0.8180]
acc:[0.8170]
acc:[0.8200]
acc:[0.8220]
acc:[0.8170]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
acc:[0.8180]
acc:[0.8180]
acc:[0.8180]
acc:[0.8200]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8184]
Mean:[81.8420]
Std :[0.1605]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='cora', drop_percent=0.4, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=31)
----------------------------------------------------------------------------------------------------
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6996]
Loss:[0.6861]
Loss:[0.6905]
Loss:[0.6863]
Loss:[0.6782]
Loss:[0.6766]
Loss:[0.6737]
Loss:[0.6636]
Loss:[0.6587]
Loss:[0.6539]
Loss:[0.6406]
Loss:[0.6350]
Loss:[0.6246]
Loss:[0.6099]
Loss:[0.6024]
Loss:[0.5843]
Loss:[0.5788]
Loss:[0.5599]
Loss:[0.5454]
Loss:[0.5317]
Loss:[0.5144]
Loss:[0.5033]
Loss:[0.4803]
Loss:[0.4733]
Loss:[0.4612]
Loss:[0.4360]
Loss:[0.4330]
Loss:[0.4155]
Loss:[0.3938]
Loss:[0.3846]
Loss:[0.3817]
Loss:[0.3637]
Loss:[0.3500]
Loss:[0.3406]
Loss:[0.3207]
Loss:[0.3145]
Loss:[0.3016]
Loss:[0.2942]
Loss:[0.2826]
Loss:[0.2776]
Loss:[0.2657]
Loss:[0.2608]
Loss:[0.2564]
Loss:[0.2476]
Loss:[0.2400]
Loss:[0.2330]
Loss:[0.2336]
Loss:[0.2267]
Loss:[0.2192]
Loss:[0.2099]
Loss:[0.2074]
Loss:[0.2054]
Loss:[0.2097]
Loss:[0.1969]
Loss:[0.1874]
Loss:[0.2011]
Loss:[0.1753]
Loss:[0.1874]
Loss:[0.1808]
Loss:[0.1713]
Loss:[0.1738]
Loss:[0.1653]
Loss:[0.1539]
Loss:[0.1596]
Loss:[0.1715]
Loss:[0.1562]
Loss:[0.1571]
Loss:[0.1519]
Loss:[0.1531]
Loss:[0.1369]
Loss:[0.1408]
Loss:[0.1399]
Loss:[0.1445]
Loss:[0.1377]
Loss:[0.1357]
Loss:[0.1420]
Loss:[0.1498]
Loss:[0.1288]
Loss:[0.1280]
Loss:[0.1238]
Loss:[0.1251]
Loss:[0.1163]
Loss:[0.1188]
Loss:[0.1170]
Loss:[0.1164]
Loss:[0.1198]
Loss:[0.1126]
Loss:[0.1257]
Loss:[0.0999]
Loss:[0.1136]
Loss:[0.0998]
Loss:[0.1079]
Loss:[0.1165]
Loss:[0.1031]
Loss:[0.1116]
Loss:[0.1154]
Loss:[0.1041]
Loss:[0.0996]
Loss:[0.1187]
Loss:[0.1102]
Loss:[0.0978]
Loss:[0.1079]
Loss:[0.0960]
Loss:[0.0955]
Loss:[0.0969]
Loss:[0.0972]
Loss:[0.0979]
Loss:[0.0834]
Loss:[0.0937]
Loss:[0.0822]
Loss:[0.0992]
Loss:[0.0855]
Loss:[0.0856]
Loss:[0.0849]
Loss:[0.0739]
Loss:[0.0810]
Loss:[0.0761]
Loss:[0.0780]
Loss:[0.0808]
Loss:[0.0676]
Loss:[0.0867]
Loss:[0.0785]
Loss:[0.0771]
Loss:[0.0784]
Loss:[0.0666]
Loss:[0.0765]
Loss:[0.0695]
Loss:[0.0801]
Loss:[0.0829]
Loss:[0.0809]
Loss:[0.0701]
Loss:[0.0804]
Loss:[0.0830]
Loss:[0.0665]
Loss:[0.0685]
Loss:[0.0632]
Loss:[0.0733]
Loss:[0.0683]
Loss:[0.0671]
Loss:[0.0737]
Loss:[0.0668]
Loss:[0.0539]
Loss:[0.0664]
Loss:[0.0625]
Loss:[0.0689]
Loss:[0.0675]
Loss:[0.0723]
Loss:[0.0697]
Loss:[0.0546]
Loss:[0.0533]
Loss:[0.0602]
Loss:[0.0594]
Loss:[0.0573]
Loss:[0.0603]
Loss:[0.0564]
Loss:[0.0725]
Loss:[0.0626]
Loss:[0.0635]
Loss:[0.0611]
Loss:[0.0547]
Loss:[0.0577]
Loss:[0.0547]
Loss:[0.0516]
Loss:[0.0518]
Loss:[0.0514]
Loss:[0.0452]
Loss:[0.0489]
Loss:[0.0547]
Loss:[0.0546]
Loss:[0.0558]
Loss:[0.0528]
Loss:[0.0467]
Loss:[0.0487]
Loss:[0.0494]
Loss:[0.0511]
Loss:[0.0501]
Loss:[0.0448]
Loss:[0.0474]
Loss:[0.0490]
Loss:[0.0445]
Loss:[0.0512]
Loss:[0.0475]
Loss:[0.0424]
Loss:[0.0431]
Loss:[0.0444]
Loss:[0.0447]
Loss:[0.0430]
Loss:[0.0460]
Loss:[0.0471]
Loss:[0.0411]
Loss:[0.0470]
Loss:[0.0369]
Loss:[0.0417]
Loss:[0.0502]
Loss:[0.0513]
Loss:[0.0413]
Loss:[0.0429]
Loss:[0.0445]
Loss:[0.0404]
Loss:[0.0377]
Loss:[0.0429]
Loss:[0.0371]
Loss:[0.0421]
Loss:[0.0346]
Loss:[0.0419]
Loss:[0.0395]
Loss:[0.0392]
Loss:[0.0361]
Loss:[0.0337]
Loss:[0.0379]
Loss:[0.0361]
Loss:[0.0344]
Loss:[0.0353]
Loss:[0.0370]
Loss:[0.0331]
Loss:[0.0370]
Loss:[0.0368]
Loss:[0.0379]
Loss:[0.0335]
Loss:[0.0361]
Loss:[0.0384]
Loss:[0.0373]
Loss:[0.0389]
Loss:[0.0346]
Loss:[0.0343]
Loss:[0.0313]
Loss:[0.0324]
Loss:[0.0352]
Loss:[0.0302]
Loss:[0.0295]
Loss:[0.0255]
Loss:[0.0321]
Loss:[0.0315]
Loss:[0.0323]
Loss:[0.0333]
Loss:[0.0421]
Loss:[0.0323]
Loss:[0.0355]
Loss:[0.0389]
Loss:[0.0301]
Loss:[0.0295]
Loss:[0.0342]
Loss:[0.0295]
Loss:[0.0321]
Loss:[0.0218]
Loss:[0.0362]
Loss:[0.0268]
Loss:[0.0258]
Loss:[0.0319]
Loss:[0.0267]
Loss:[0.0307]
Loss:[0.0265]
Loss:[0.0303]
Loss:[0.0344]
Loss:[0.0322]
Loss:[0.0293]
Loss:[0.0284]
Loss:[0.0311]
Loss:[0.0350]
Loss:[0.0274]
Loss:[0.0268]
Loss:[0.0252]
Loss:[0.0252]
Loss:[0.0253]
Loss:[0.0305]
Early stopping!
Loading 245th epoch
acc:[0.8150]
acc:[0.8150]
acc:[0.8170]
acc:[0.8180]
acc:[0.8190]
acc:[0.8160]
acc:[0.8160]
acc:[0.8160]
acc:[0.8160]
acc:[0.8170]
acc:[0.8190]
acc:[0.8160]
acc:[0.8190]
acc:[0.8180]
acc:[0.8200]
acc:[0.8130]
acc:[0.8170]
acc:[0.8170]
acc:[0.8170]
acc:[0.8170]
acc:[0.8170]
acc:[0.8190]
acc:[0.8170]
acc:[0.8160]
acc:[0.8120]
acc:[0.8140]
acc:[0.8180]
acc:[0.8200]
acc:[0.8150]
acc:[0.8170]
acc:[0.8140]
acc:[0.8140]
acc:[0.8190]
acc:[0.8170]
acc:[0.8150]
acc:[0.8190]
acc:[0.8170]
acc:[0.8160]
acc:[0.8190]
acc:[0.8160]
acc:[0.8200]
acc:[0.8190]
acc:[0.8160]
acc:[0.8140]
acc:[0.8160]
acc:[0.8150]
acc:[0.8150]
acc:[0.8190]
acc:[0.8170]
acc:[0.8200]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8168]
Mean:[81.6800]
Std :[0.1938]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='cora', drop_percent=0.5, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=31)
----------------------------------------------------------------------------------------------------
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6996]
Loss:[0.6861]
Loss:[0.6905]
Loss:[0.6863]
Loss:[0.6782]
Loss:[0.6766]
Loss:[0.6737]
Loss:[0.6636]
Loss:[0.6587]
Loss:[0.6539]
Loss:[0.6406]
Loss:[0.6350]
Loss:[0.6246]
Loss:[0.6099]
Loss:[0.6024]
Loss:[0.5843]
Loss:[0.5788]
Loss:[0.5599]
Loss:[0.5454]
Loss:[0.5317]
Loss:[0.5144]
Loss:[0.5033]
Loss:[0.4803]
Loss:[0.4732]
Loss:[0.4612]
Loss:[0.4360]
Loss:[0.4328]
Loss:[0.4155]
Loss:[0.3938]
Loss:[0.3844]
Loss:[0.3817]
Loss:[0.3637]
Loss:[0.3498]
Loss:[0.3406]
Loss:[0.3208]
Loss:[0.3143]
Loss:[0.3014]
Loss:[0.2943]
Loss:[0.2827]
Loss:[0.2773]
Loss:[0.2655]
Loss:[0.2609]
Loss:[0.2563]
Loss:[0.2473]
Loss:[0.2399]
Loss:[0.2332]
Loss:[0.2337]
Loss:[0.2266]
Loss:[0.2190]
Loss:[0.2099]
Loss:[0.2077]
Loss:[0.2054]
Loss:[0.2095]
Loss:[0.1967]
Loss:[0.1874]
Loss:[0.2012]
Loss:[0.1753]
Loss:[0.1874]
Loss:[0.1808]
Loss:[0.1713]
Loss:[0.1738]
Loss:[0.1653]
Loss:[0.1537]
Loss:[0.1597]
Loss:[0.1718]
Loss:[0.1563]
Loss:[0.1571]
Loss:[0.1524]
Loss:[0.1539]
Loss:[0.1366]
Loss:[0.1400]
Loss:[0.1403]
Loss:[0.1447]
Loss:[0.1375]
Loss:[0.1362]
Loss:[0.1429]
Loss:[0.1499]
Loss:[0.1277]
Loss:[0.1274]
Loss:[0.1243]
Loss:[0.1250]
Loss:[0.1164]
Loss:[0.1203]
Loss:[0.1170]
Loss:[0.1151]
Loss:[0.1203]
Loss:[0.1124]
Loss:[0.1226]
Loss:[0.0997]
Loss:[0.1131]
Loss:[0.0987]
Loss:[0.1081]
Loss:[0.1151]
Loss:[0.1016]
Loss:[0.1116]
Loss:[0.1120]
Loss:[0.1010]
Loss:[0.0982]
Loss:[0.1131]
Loss:[0.1097]
Loss:[0.0934]
Loss:[0.1091]
Loss:[0.1027]
Loss:[0.0941]
Loss:[0.1073]
Loss:[0.0961]
Loss:[0.1002]
Loss:[0.0854]
Loss:[0.0915]
Loss:[0.0855]
Loss:[0.0972]
Loss:[0.0901]
Loss:[0.0827]
Loss:[0.0852]
Loss:[0.0757]
Loss:[0.0801]
Loss:[0.0772]
Loss:[0.0770]
Loss:[0.0823]
Loss:[0.0661]
Loss:[0.0873]
Loss:[0.0805]
Loss:[0.0758]
Loss:[0.0803]
Loss:[0.0662]
Loss:[0.0773]
Loss:[0.0690]
Loss:[0.0803]
Loss:[0.0833]
Loss:[0.0802]
Loss:[0.0705]
Loss:[0.0793]
Loss:[0.0837]
Loss:[0.0650]
Loss:[0.0681]
Loss:[0.0624]
Loss:[0.0730]
Loss:[0.0683]
Loss:[0.0663]
Loss:[0.0740]
Loss:[0.0661]
Loss:[0.0539]
Loss:[0.0656]
Loss:[0.0625]
Loss:[0.0679]
Loss:[0.0669]
Loss:[0.0715]
Loss:[0.0690]
Loss:[0.0548]
Loss:[0.0529]
Loss:[0.0603]
Loss:[0.0585]
Loss:[0.0572]
Loss:[0.0596]
Loss:[0.0567]
Loss:[0.0718]
Loss:[0.0624]
Loss:[0.0628]
Loss:[0.0610]
Loss:[0.0543]
Loss:[0.0573]
Loss:[0.0545]
Loss:[0.0509]
Loss:[0.0519]
Loss:[0.0509]
Loss:[0.0455]
Loss:[0.0483]
Loss:[0.0549]
Loss:[0.0544]
Loss:[0.0559]
Loss:[0.0528]
Loss:[0.0466]
Loss:[0.0487]
Loss:[0.0493]
Loss:[0.0508]
Loss:[0.0499]
Loss:[0.0449]
Loss:[0.0473]
Loss:[0.0489]
Loss:[0.0445]
Loss:[0.0509]
Loss:[0.0473]
Loss:[0.0421]
Loss:[0.0430]
Loss:[0.0442]
Loss:[0.0446]
Loss:[0.0429]
Loss:[0.0459]
Loss:[0.0470]
Loss:[0.0409]
Loss:[0.0468]
Loss:[0.0370]
Loss:[0.0416]
Loss:[0.0501]
Loss:[0.0511]
Loss:[0.0413]
Loss:[0.0427]
Loss:[0.0445]
Loss:[0.0402]
Loss:[0.0377]
Loss:[0.0426]
Loss:[0.0371]
Loss:[0.0420]
Loss:[0.0345]
Loss:[0.0418]
Loss:[0.0393]
Loss:[0.0391]
Loss:[0.0360]
Loss:[0.0336]
Loss:[0.0378]
Loss:[0.0360]
Loss:[0.0343]
Loss:[0.0350]
Loss:[0.0369]
Loss:[0.0330]
Loss:[0.0370]
Loss:[0.0364]
Loss:[0.0377]
Loss:[0.0332]
Loss:[0.0360]
Loss:[0.0383]
Loss:[0.0373]
Loss:[0.0391]
Loss:[0.0344]
Loss:[0.0343]
Loss:[0.0312]
Loss:[0.0325]
Loss:[0.0352]
Loss:[0.0302]
Loss:[0.0294]
Loss:[0.0254]
Loss:[0.0318]
Loss:[0.0315]
Loss:[0.0323]
Loss:[0.0333]
Loss:[0.0419]
Loss:[0.0323]
Loss:[0.0357]
Loss:[0.0388]
Loss:[0.0302]
Loss:[0.0294]
Loss:[0.0342]
Loss:[0.0296]
Loss:[0.0321]
Loss:[0.0217]
Loss:[0.0364]
Loss:[0.0268]
Loss:[0.0259]
Loss:[0.0321]
Loss:[0.0267]
Loss:[0.0309]
Loss:[0.0264]
Loss:[0.0307]
Loss:[0.0342]
Loss:[0.0325]
Loss:[0.0290]
Loss:[0.0286]
Loss:[0.0307]
Loss:[0.0349]
Loss:[0.0270]
Loss:[0.0270]
Loss:[0.0248]
Loss:[0.0254]
Loss:[0.0251]
Loss:[0.0307]
Early stopping!
Loading 245th epoch
acc:[0.8170]
acc:[0.8200]
acc:[0.8210]
acc:[0.8190]
acc:[0.8210]
acc:[0.8190]
acc:[0.8160]
acc:[0.8180]
acc:[0.8180]
acc:[0.8170]
acc:[0.8190]
acc:[0.8200]
acc:[0.8200]
acc:[0.8190]
acc:[0.8190]
acc:[0.8160]
acc:[0.8190]
acc:[0.8190]
acc:[0.8190]
acc:[0.8180]
acc:[0.8210]
acc:[0.8180]
acc:[0.8220]
acc:[0.8190]
acc:[0.8190]
acc:[0.8140]
acc:[0.8200]
acc:[0.8190]
acc:[0.8140]
acc:[0.8160]
acc:[0.8150]
acc:[0.8160]
acc:[0.8200]
acc:[0.8170]
acc:[0.8170]
acc:[0.8190]
acc:[0.8180]
acc:[0.8180]
acc:[0.8170]
acc:[0.8160]
acc:[0.8210]
acc:[0.8200]
acc:[0.8190]
acc:[0.8180]
acc:[0.8200]
acc:[0.8200]
acc:[0.8200]
acc:[0.8200]
acc:[0.8170]
acc:[0.8210]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8185]
Mean:[81.8500]
Std :[0.1854]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='cora', drop_percent=0.6, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=31)
----------------------------------------------------------------------------------------------------
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6996]
Loss:[0.6861]
Loss:[0.6905]
Loss:[0.6863]
Loss:[0.6782]
Loss:[0.6766]
Loss:[0.6737]
Loss:[0.6636]
Loss:[0.6587]
Loss:[0.6539]
Loss:[0.6406]
Loss:[0.6350]
Loss:[0.6246]
Loss:[0.6099]
Loss:[0.6024]
Loss:[0.5843]
Loss:[0.5788]
Loss:[0.5599]
Loss:[0.5454]
Loss:[0.5318]
Loss:[0.5144]
Loss:[0.5033]
Loss:[0.4803]
Loss:[0.4733]
Loss:[0.4612]
Loss:[0.4360]
Loss:[0.4329]
Loss:[0.4155]
Loss:[0.3938]
Loss:[0.3846]
Loss:[0.3817]
Loss:[0.3637]
Loss:[0.3500]
Loss:[0.3406]
Loss:[0.3207]
Loss:[0.3145]
Loss:[0.3016]
Loss:[0.2942]
Loss:[0.2826]
Loss:[0.2775]
Loss:[0.2656]
Loss:[0.2609]
Loss:[0.2563]
Loss:[0.2475]
Loss:[0.2400]
Loss:[0.2330]
Loss:[0.2336]
Loss:[0.2267]
Loss:[0.2192]
Loss:[0.2100]
Loss:[0.2076]
Loss:[0.2054]
Loss:[0.2096]
Loss:[0.1968]
Loss:[0.1874]
Loss:[0.2011]
Loss:[0.1754]
Loss:[0.1874]
Loss:[0.1808]
Loss:[0.1712]
Loss:[0.1739]
Loss:[0.1654]
Loss:[0.1539]
Loss:[0.1596]
Loss:[0.1717]
Loss:[0.1564]
Loss:[0.1571]
Loss:[0.1521]
Loss:[0.1534]
Loss:[0.1367]
Loss:[0.1403]
Loss:[0.1401]
Loss:[0.1446]
Loss:[0.1375]
Loss:[0.1360]
Loss:[0.1425]
Loss:[0.1499]
Loss:[0.1281]
Loss:[0.1276]
Loss:[0.1240]
Loss:[0.1250]
Loss:[0.1163]
Loss:[0.1195]
Loss:[0.1170]
Loss:[0.1156]
Loss:[0.1200]
Loss:[0.1125]
Loss:[0.1243]
Loss:[0.0997]
Loss:[0.1134]
Loss:[0.0992]
Loss:[0.1079]
Loss:[0.1157]
Loss:[0.1023]
Loss:[0.1115]
Loss:[0.1136]
Loss:[0.1025]
Loss:[0.0986]
Loss:[0.1158]
Loss:[0.1101]
Loss:[0.0954]
Loss:[0.1092]
Loss:[0.0991]
Loss:[0.0950]
Loss:[0.1016]
Loss:[0.0953]
Loss:[0.0989]
Loss:[0.0832]
Loss:[0.0933]
Loss:[0.0836]
Loss:[0.0985]
Loss:[0.0876]
Loss:[0.0833]
Loss:[0.0852]
Loss:[0.0741]
Loss:[0.0806]
Loss:[0.0764]
Loss:[0.0774]
Loss:[0.0815]
Loss:[0.0664]
Loss:[0.0872]
Loss:[0.0793]
Loss:[0.0766]
Loss:[0.0795]
Loss:[0.0663]
Loss:[0.0770]
Loss:[0.0690]
Loss:[0.0802]
Loss:[0.0830]
Loss:[0.0805]
Loss:[0.0703]
Loss:[0.0795]
Loss:[0.0836]
Loss:[0.0653]
Loss:[0.0683]
Loss:[0.0625]
Loss:[0.0731]
Loss:[0.0683]
Loss:[0.0665]
Loss:[0.0740]
Loss:[0.0661]
Loss:[0.0539]
Loss:[0.0655]
Loss:[0.0626]
Loss:[0.0678]
Loss:[0.0670]
Loss:[0.0713]
Loss:[0.0692]
Loss:[0.0549]
Loss:[0.0529]
Loss:[0.0602]
Loss:[0.0584]
Loss:[0.0575]
Loss:[0.0595]
Loss:[0.0569]
Loss:[0.0713]
Loss:[0.0624]
Loss:[0.0624]
Loss:[0.0611]
Loss:[0.0541]
Loss:[0.0572]
Loss:[0.0545]
Loss:[0.0507]
Loss:[0.0520]
Loss:[0.0506]
Loss:[0.0458]
Loss:[0.0479]
Loss:[0.0550]
Loss:[0.0542]
Loss:[0.0560]
Loss:[0.0530]
Loss:[0.0465]
Loss:[0.0488]
Loss:[0.0492]
Loss:[0.0508]
Loss:[0.0498]
Loss:[0.0450]
Loss:[0.0473]
Loss:[0.0492]
Loss:[0.0446]
Loss:[0.0508]
Loss:[0.0472]
Loss:[0.0422]
Loss:[0.0429]
Loss:[0.0442]
Loss:[0.0445]
Loss:[0.0428]
Loss:[0.0458]
Loss:[0.0469]
Loss:[0.0408]
Loss:[0.0469]
Loss:[0.0369]
Loss:[0.0417]
Loss:[0.0502]
Loss:[0.0513]
Loss:[0.0413]
Loss:[0.0428]
Loss:[0.0443]
Loss:[0.0403]
Loss:[0.0377]
Loss:[0.0427]
Loss:[0.0372]
Loss:[0.0421]
Loss:[0.0345]
Loss:[0.0418]
Loss:[0.0394]
Loss:[0.0391]
Loss:[0.0360]
Loss:[0.0334]
Loss:[0.0378]
Loss:[0.0359]
Loss:[0.0342]
Loss:[0.0350]
Loss:[0.0368]
Loss:[0.0330]
Loss:[0.0370]
Loss:[0.0364]
Loss:[0.0378]
Loss:[0.0331]
Loss:[0.0360]
Loss:[0.0382]
Loss:[0.0371]
Loss:[0.0391]
Loss:[0.0345]
Loss:[0.0343]
Loss:[0.0311]
Loss:[0.0325]
Loss:[0.0352]
Loss:[0.0301]
Loss:[0.0292]
Loss:[0.0255]
Loss:[0.0317]
Loss:[0.0316]
Loss:[0.0323]
Loss:[0.0333]
Loss:[0.0419]
Loss:[0.0323]
Loss:[0.0358]
Loss:[0.0388]
Loss:[0.0303]
Loss:[0.0293]
Loss:[0.0341]
Loss:[0.0296]
Loss:[0.0320]
Loss:[0.0216]
Loss:[0.0363]
Loss:[0.0269]
Loss:[0.0258]
Loss:[0.0322]
Loss:[0.0267]
Loss:[0.0309]
Loss:[0.0262]
Loss:[0.0307]
Loss:[0.0341]
Loss:[0.0324]
Loss:[0.0289]
Loss:[0.0286]
Loss:[0.0306]
Loss:[0.0350]
Loss:[0.0270]
Loss:[0.0270]
Loss:[0.0248]
Loss:[0.0254]
Loss:[0.0250]
Loss:[0.0305]
Early stopping!
Loading 245th epoch
acc:[0.8160]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8180]
acc:[0.8140]
acc:[0.8160]
acc:[0.8140]
acc:[0.8140]
acc:[0.8150]
acc:[0.8160]
acc:[0.8160]
acc:[0.8180]
acc:[0.8200]
acc:[0.8140]
acc:[0.8160]
acc:[0.8180]
acc:[0.8170]
acc:[0.8160]
acc:[0.8170]
acc:[0.8170]
acc:[0.8150]
acc:[0.8210]
acc:[0.8160]
acc:[0.8150]
acc:[0.8140]
acc:[0.8140]
acc:[0.8120]
acc:[0.8150]
acc:[0.8140]
acc:[0.8140]
acc:[0.8140]
acc:[0.8160]
acc:[0.8170]
acc:[0.8130]
acc:[0.8160]
acc:[0.8160]
acc:[0.8150]
acc:[0.8160]
acc:[0.8130]
acc:[0.8180]
acc:[0.8140]
acc:[0.8150]
acc:[0.8150]
acc:[0.8170]
acc:[0.8150]
acc:[0.8140]
acc:[0.8180]
acc:[0.8150]
acc:[0.8180]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8156]
Mean:[81.5600]
Std :[0.1796]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='cora', drop_percent=0.02, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=31)
----------------------------------------------------------------------------------------------------
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6996]
Loss:[0.6861]
Loss:[0.6905]
Loss:[0.6863]
Loss:[0.6782]
Loss:[0.6766]
Loss:[0.6737]
Loss:[0.6636]
Loss:[0.6587]
Loss:[0.6539]
Loss:[0.6406]
Loss:[0.6350]
Loss:[0.6246]
Loss:[0.6099]
Loss:[0.6024]
Loss:[0.5843]
Loss:[0.5788]
Loss:[0.5599]
Loss:[0.5454]
Loss:[0.5318]
Loss:[0.5144]
Loss:[0.5033]
Loss:[0.4803]
Loss:[0.4733]
Loss:[0.4612]
Loss:[0.4360]
Loss:[0.4330]
Loss:[0.4155]
Loss:[0.3938]
Loss:[0.3847]
Loss:[0.3817]
Loss:[0.3637]
Loss:[0.3501]
Loss:[0.3406]
Loss:[0.3207]
Loss:[0.3145]
Loss:[0.3016]
Loss:[0.2942]
Loss:[0.2826]
Loss:[0.2776]
Loss:[0.2657]
Loss:[0.2608]
Loss:[0.2564]
Loss:[0.2476]
Loss:[0.2400]
Loss:[0.2330]
Loss:[0.2336]
Loss:[0.2267]
Loss:[0.2192]
Loss:[0.2099]
Loss:[0.2075]
Loss:[0.2053]
Loss:[0.2096]
Loss:[0.1968]
Loss:[0.1874]
Loss:[0.2011]
Loss:[0.1754]
Loss:[0.1874]
Loss:[0.1808]
Loss:[0.1713]
Loss:[0.1739]
Loss:[0.1654]
Loss:[0.1538]
Loss:[0.1596]
Loss:[0.1717]
Loss:[0.1564]
Loss:[0.1571]
Loss:[0.1522]
Loss:[0.1536]
Loss:[0.1367]
Loss:[0.1403]
Loss:[0.1401]
Loss:[0.1447]
Loss:[0.1375]
Loss:[0.1360]
Loss:[0.1426]
Loss:[0.1499]
Loss:[0.1280]
Loss:[0.1275]
Loss:[0.1241]
Loss:[0.1250]
Loss:[0.1163]
Loss:[0.1197]
Loss:[0.1169]
Loss:[0.1153]
Loss:[0.1202]
Loss:[0.1125]
Loss:[0.1237]
Loss:[0.0997]
Loss:[0.1132]
Loss:[0.0989]
Loss:[0.1080]
Loss:[0.1155]
Loss:[0.1021]
Loss:[0.1115]
Loss:[0.1129]
Loss:[0.1020]
Loss:[0.0984]
Loss:[0.1147]
Loss:[0.1104]
Loss:[0.0946]
Loss:[0.1098]
Loss:[0.1009]
Loss:[0.0949]
Loss:[0.1041]
Loss:[0.0953]
Loss:[0.0996]
Loss:[0.0837]
Loss:[0.0929]
Loss:[0.0843]
Loss:[0.0982]
Loss:[0.0885]
Loss:[0.0829]
Loss:[0.0854]
Loss:[0.0744]
Loss:[0.0806]
Loss:[0.0766]
Loss:[0.0774]
Loss:[0.0818]
Loss:[0.0662]
Loss:[0.0872]
Loss:[0.0794]
Loss:[0.0765]
Loss:[0.0797]
Loss:[0.0663]
Loss:[0.0771]
Loss:[0.0690]
Loss:[0.0803]
Loss:[0.0830]
Loss:[0.0805]
Loss:[0.0704]
Loss:[0.0797]
Loss:[0.0836]
Loss:[0.0655]
Loss:[0.0684]
Loss:[0.0627]
Loss:[0.0732]
Loss:[0.0683]
Loss:[0.0667]
Loss:[0.0739]
Loss:[0.0663]
Loss:[0.0540]
Loss:[0.0658]
Loss:[0.0626]
Loss:[0.0681]
Loss:[0.0672]
Loss:[0.0717]
Loss:[0.0693]
Loss:[0.0547]
Loss:[0.0530]
Loss:[0.0602]
Loss:[0.0589]
Loss:[0.0572]
Loss:[0.0600]
Loss:[0.0566]
Loss:[0.0720]
Loss:[0.0625]
Loss:[0.0629]
Loss:[0.0611]
Loss:[0.0544]
Loss:[0.0575]
Loss:[0.0545]
Loss:[0.0511]
Loss:[0.0519]
Loss:[0.0511]
Loss:[0.0454]
Loss:[0.0484]
Loss:[0.0548]
Loss:[0.0543]
Loss:[0.0558]
Loss:[0.0528]
Loss:[0.0466]
Loss:[0.0486]
Loss:[0.0492]
Loss:[0.0509]
Loss:[0.0499]
Loss:[0.0447]
Loss:[0.0473]
Loss:[0.0490]
Loss:[0.0444]
Loss:[0.0509]
Loss:[0.0473]
Loss:[0.0422]
Loss:[0.0429]
Loss:[0.0442]
Loss:[0.0446]
Loss:[0.0428]
Loss:[0.0458]
Loss:[0.0471]
Loss:[0.0409]
Loss:[0.0468]
Loss:[0.0370]
Loss:[0.0417]
Loss:[0.0502]
Loss:[0.0512]
Loss:[0.0413]
Loss:[0.0428]
Loss:[0.0444]
Loss:[0.0403]
Loss:[0.0377]
Loss:[0.0426]
Loss:[0.0372]
Loss:[0.0421]
Loss:[0.0346]
Loss:[0.0418]
Loss:[0.0394]
Loss:[0.0391]
Loss:[0.0361]
Loss:[0.0335]
Loss:[0.0378]
Loss:[0.0360]
Loss:[0.0343]
Loss:[0.0350]
Loss:[0.0368]
Loss:[0.0331]
Loss:[0.0369]
Loss:[0.0364]
Loss:[0.0377]
Loss:[0.0333]
Loss:[0.0359]
Loss:[0.0383]
Loss:[0.0371]
Loss:[0.0388]
Loss:[0.0344]
Loss:[0.0343]
Loss:[0.0313]
Loss:[0.0325]
Loss:[0.0352]
Loss:[0.0300]
Loss:[0.0293]
Loss:[0.0254]
Loss:[0.0318]
Loss:[0.0315]
Loss:[0.0323]
Loss:[0.0334]
Loss:[0.0420]
Loss:[0.0322]
Loss:[0.0357]
Loss:[0.0388]
Loss:[0.0302]
Loss:[0.0294]
Loss:[0.0342]
Loss:[0.0296]
Loss:[0.0320]
Loss:[0.0217]
Loss:[0.0363]
Loss:[0.0269]
Loss:[0.0258]
Loss:[0.0319]
Loss:[0.0268]
Loss:[0.0309]
Loss:[0.0264]
Loss:[0.0304]
Loss:[0.0342]
Loss:[0.0323]
Loss:[0.0291]
Loss:[0.0285]
Loss:[0.0307]
Loss:[0.0351]
Loss:[0.0272]
Loss:[0.0271]
Loss:[0.0250]
Loss:[0.0254]
Loss:[0.0251]
Loss:[0.0306]
Early stopping!
Loading 245th epoch
acc:[0.8180]
acc:[0.8230]
acc:[0.8200]
acc:[0.8190]
acc:[0.8210]
acc:[0.8180]
acc:[0.8190]
acc:[0.8160]
acc:[0.8190]
acc:[0.8180]
acc:[0.8190]
acc:[0.8200]
acc:[0.8210]
acc:[0.8200]
acc:[0.8210]
acc:[0.8190]
acc:[0.8200]
acc:[0.8210]
acc:[0.8190]
acc:[0.8210]
acc:[0.8190]
acc:[0.8180]
acc:[0.8210]
acc:[0.8220]
acc:[0.8200]
acc:[0.8180]
acc:[0.8180]
acc:[0.8180]
acc:[0.8190]
acc:[0.8180]
acc:[0.8140]
acc:[0.8190]
acc:[0.8200]
acc:[0.8200]
acc:[0.8190]
acc:[0.8210]
acc:[0.8200]
acc:[0.8200]
acc:[0.8190]
acc:[0.8180]
acc:[0.8220]
acc:[0.8200]
acc:[0.8200]
acc:[0.8170]
acc:[0.8210]
acc:[0.8200]
acc:[0.8200]
acc:[0.8210]
acc:[0.8230]
acc:[0.8220]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8196]
Mean:[81.9580]
Std :[0.1679]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='cora', drop_percent=0.05, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=31)
----------------------------------------------------------------------------------------------------
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6996]
Loss:[0.6861]
Loss:[0.6905]
Loss:[0.6863]
Loss:[0.6782]
Loss:[0.6766]
Loss:[0.6737]
Loss:[0.6636]
Loss:[0.6587]
Loss:[0.6539]
Loss:[0.6406]
Loss:[0.6350]
Loss:[0.6246]
Loss:[0.6099]
Loss:[0.6024]
Loss:[0.5843]
Loss:[0.5788]
Loss:[0.5599]
Loss:[0.5454]
Loss:[0.5318]
Loss:[0.5144]
Loss:[0.5033]
Loss:[0.4803]
Loss:[0.4733]
Loss:[0.4612]
Loss:[0.4360]
Loss:[0.4330]
Loss:[0.4155]
Loss:[0.3938]
Loss:[0.3847]
Loss:[0.3817]
Loss:[0.3637]
Loss:[0.3501]
Loss:[0.3406]
Loss:[0.3207]
Loss:[0.3145]
Loss:[0.3016]
Loss:[0.2942]
Loss:[0.2826]
Loss:[0.2776]
Loss:[0.2657]
Loss:[0.2608]
Loss:[0.2564]
Loss:[0.2476]
Loss:[0.2400]
Loss:[0.2330]
Loss:[0.2336]
Loss:[0.2267]
Loss:[0.2192]
Loss:[0.2099]
Loss:[0.2075]
Loss:[0.2053]
Loss:[0.2096]
Loss:[0.1968]
Loss:[0.1874]
Loss:[0.2011]
Loss:[0.1754]
Loss:[0.1874]
Loss:[0.1808]
Loss:[0.1713]
Loss:[0.1739]
Loss:[0.1654]
Loss:[0.1538]
Loss:[0.1596]
Loss:[0.1717]
Loss:[0.1564]
Loss:[0.1571]
Loss:[0.1522]
Loss:[0.1536]
Loss:[0.1367]
Loss:[0.1403]
Loss:[0.1401]
Loss:[0.1447]
Loss:[0.1375]
Loss:[0.1360]
Loss:[0.1426]
Loss:[0.1499]
Loss:[0.1280]
Loss:[0.1275]
Loss:[0.1241]
Loss:[0.1250]
Loss:[0.1163]
Loss:[0.1197]
Loss:[0.1169]
Loss:[0.1153]
Loss:[0.1202]
Loss:[0.1125]
Loss:[0.1237]
Loss:[0.0997]
Loss:[0.1132]
Loss:[0.0989]
Loss:[0.1080]
Loss:[0.1155]
Loss:[0.1021]
Loss:[0.1115]
Loss:[0.1129]
Loss:[0.1020]
Loss:[0.0984]
Loss:[0.1147]
Loss:[0.1104]
Loss:[0.0946]
Loss:[0.1098]
Loss:[0.1009]
Loss:[0.0949]
Loss:[0.1041]
Loss:[0.0953]
Loss:[0.0996]
Loss:[0.0837]
Loss:[0.0929]
Loss:[0.0843]
Loss:[0.0982]
Loss:[0.0885]
Loss:[0.0829]
Loss:[0.0854]
Loss:[0.0744]
Loss:[0.0806]
Loss:[0.0766]
Loss:[0.0774]
Loss:[0.0818]
Loss:[0.0662]
Loss:[0.0872]
Loss:[0.0794]
Loss:[0.0765]
Loss:[0.0797]
Loss:[0.0663]
Loss:[0.0771]
Loss:[0.0690]
Loss:[0.0803]
Loss:[0.0830]
Loss:[0.0805]
Loss:[0.0704]
Loss:[0.0797]
Loss:[0.0836]
Loss:[0.0655]
Loss:[0.0684]
Loss:[0.0627]
Loss:[0.0732]
Loss:[0.0683]
Loss:[0.0667]
Loss:[0.0739]
Loss:[0.0663]
Loss:[0.0540]
Loss:[0.0658]
Loss:[0.0626]
Loss:[0.0681]
Loss:[0.0672]
Loss:[0.0717]
Loss:[0.0693]
Loss:[0.0547]
Loss:[0.0530]
Loss:[0.0602]
Loss:[0.0589]
Loss:[0.0572]
Loss:[0.0600]
Loss:[0.0566]
Loss:[0.0720]
Loss:[0.0625]
Loss:[0.0629]
Loss:[0.0611]
Loss:[0.0544]
Loss:[0.0575]
Loss:[0.0545]
Loss:[0.0511]
Loss:[0.0519]
Loss:[0.0511]
Loss:[0.0454]
Loss:[0.0484]
Loss:[0.0548]
Loss:[0.0543]
Loss:[0.0558]
Loss:[0.0528]
Loss:[0.0466]
Loss:[0.0486]
Loss:[0.0492]
Loss:[0.0509]
Loss:[0.0499]
Loss:[0.0447]
Loss:[0.0473]
Loss:[0.0490]
Loss:[0.0444]
Loss:[0.0509]
Loss:[0.0473]
Loss:[0.0422]
Loss:[0.0429]
Loss:[0.0442]
Loss:[0.0446]
Loss:[0.0428]
Loss:[0.0458]
Loss:[0.0471]
Loss:[0.0409]
Loss:[0.0468]
Loss:[0.0370]
Loss:[0.0417]
Loss:[0.0502]
Loss:[0.0512]
Loss:[0.0413]
Loss:[0.0428]
Loss:[0.0444]
Loss:[0.0403]
Loss:[0.0377]
Loss:[0.0426]
Loss:[0.0372]
Loss:[0.0421]
Loss:[0.0346]
Loss:[0.0418]
Loss:[0.0394]
Loss:[0.0391]
Loss:[0.0361]
Loss:[0.0335]
Loss:[0.0378]
Loss:[0.0360]
Loss:[0.0343]
Loss:[0.0350]
Loss:[0.0368]
Loss:[0.0331]
Loss:[0.0369]
Loss:[0.0364]
Loss:[0.0377]
Loss:[0.0333]
Loss:[0.0359]
Loss:[0.0383]
Loss:[0.0371]
Loss:[0.0388]
Loss:[0.0344]
Loss:[0.0343]
Loss:[0.0313]
Loss:[0.0325]
Loss:[0.0352]
Loss:[0.0300]
Loss:[0.0293]
Loss:[0.0254]
Loss:[0.0318]
Loss:[0.0315]
Loss:[0.0323]
Loss:[0.0334]
Loss:[0.0420]
Loss:[0.0322]
Loss:[0.0357]
Loss:[0.0388]
Loss:[0.0302]
Loss:[0.0294]
Loss:[0.0342]
Loss:[0.0296]
Loss:[0.0320]
Loss:[0.0217]
Loss:[0.0363]
Loss:[0.0269]
Loss:[0.0258]
Loss:[0.0319]
Loss:[0.0268]
Loss:[0.0309]
Loss:[0.0264]
Loss:[0.0304]
Loss:[0.0342]
Loss:[0.0323]
Loss:[0.0291]
Loss:[0.0285]
Loss:[0.0307]
Loss:[0.0351]
Loss:[0.0272]
Loss:[0.0271]
Loss:[0.0250]
Loss:[0.0254]
Loss:[0.0251]
Loss:[0.0306]
Early stopping!
Loading 245th epoch
acc:[0.8180]
acc:[0.8230]
acc:[0.8200]
acc:[0.8190]
acc:[0.8210]
acc:[0.8180]
acc:[0.8190]
acc:[0.8160]
acc:[0.8190]
acc:[0.8180]
acc:[0.8190]
acc:[0.8200]
acc:[0.8210]
acc:[0.8200]
acc:[0.8210]
acc:[0.8190]
acc:[0.8200]
acc:[0.8210]
acc:[0.8190]
acc:[0.8210]
acc:[0.8190]
acc:[0.8180]
acc:[0.8210]
acc:[0.8220]
acc:[0.8200]
acc:[0.8180]
acc:[0.8180]
acc:[0.8180]
acc:[0.8190]
acc:[0.8180]
acc:[0.8140]
acc:[0.8190]
acc:[0.8200]
acc:[0.8200]
acc:[0.8190]
acc:[0.8210]
acc:[0.8200]
acc:[0.8200]
acc:[0.8190]
acc:[0.8180]
acc:[0.8220]
acc:[0.8200]
acc:[0.8200]
acc:[0.8170]
acc:[0.8210]
acc:[0.8200]
acc:[0.8200]
acc:[0.8210]
acc:[0.8230]
acc:[0.8220]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8196]
Mean:[81.9580]
Std :[0.1679]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='cora', drop_percent=0.08, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=31)
----------------------------------------------------------------------------------------------------
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6996]
Loss:[0.6861]
Loss:[0.6905]
Loss:[0.6863]
Loss:[0.6782]
Loss:[0.6766]
Loss:[0.6737]
Loss:[0.6636]
Loss:[0.6587]
Loss:[0.6539]
Loss:[0.6406]
Loss:[0.6350]
Loss:[0.6246]
Loss:[0.6099]
Loss:[0.6024]
Loss:[0.5843]
Loss:[0.5788]
Loss:[0.5599]
Loss:[0.5454]
Loss:[0.5318]
Loss:[0.5144]
Loss:[0.5033]
Loss:[0.4803]
Loss:[0.4733]
Loss:[0.4612]
Loss:[0.4360]
Loss:[0.4330]
Loss:[0.4155]
Loss:[0.3938]
Loss:[0.3847]
Loss:[0.3817]
Loss:[0.3637]
Loss:[0.3501]
Loss:[0.3406]
Loss:[0.3207]
Loss:[0.3145]
Loss:[0.3016]
Loss:[0.2942]
Loss:[0.2826]
Loss:[0.2776]
Loss:[0.2657]
Loss:[0.2608]
Loss:[0.2564]
Loss:[0.2476]
Loss:[0.2400]
Loss:[0.2330]
Loss:[0.2336]
Loss:[0.2267]
Loss:[0.2192]
Loss:[0.2099]
Loss:[0.2075]
Loss:[0.2053]
Loss:[0.2096]
Loss:[0.1968]
Loss:[0.1874]
Loss:[0.2011]
Loss:[0.1754]
Loss:[0.1874]
Loss:[0.1808]
Loss:[0.1713]
Loss:[0.1739]
Loss:[0.1654]
Loss:[0.1538]
Loss:[0.1596]
Loss:[0.1717]
Loss:[0.1564]
Loss:[0.1571]
Loss:[0.1522]
Loss:[0.1536]
Loss:[0.1367]
Loss:[0.1403]
Loss:[0.1401]
Loss:[0.1447]
Loss:[0.1375]
Loss:[0.1360]
Loss:[0.1426]
Loss:[0.1499]
Loss:[0.1280]
Loss:[0.1275]
Loss:[0.1241]
Loss:[0.1250]
Loss:[0.1163]
Loss:[0.1197]
Loss:[0.1169]
Loss:[0.1153]
Loss:[0.1202]
Loss:[0.1125]
Loss:[0.1237]
Loss:[0.0997]
Loss:[0.1132]
Loss:[0.0989]
Loss:[0.1080]
Loss:[0.1155]
Loss:[0.1021]
Loss:[0.1115]
Loss:[0.1129]
Loss:[0.1020]
Loss:[0.0984]
Loss:[0.1147]
Loss:[0.1104]
Loss:[0.0946]
Loss:[0.1098]
Loss:[0.1009]
Loss:[0.0949]
Loss:[0.1041]
Loss:[0.0953]
Loss:[0.0996]
Loss:[0.0837]
Loss:[0.0929]
Loss:[0.0843]
Loss:[0.0982]
Loss:[0.0885]
Loss:[0.0829]
Loss:[0.0854]
Loss:[0.0744]
Loss:[0.0806]
Loss:[0.0766]
Loss:[0.0774]
Loss:[0.0818]
Loss:[0.0662]
Loss:[0.0872]
Loss:[0.0794]
Loss:[0.0765]
Loss:[0.0797]
Loss:[0.0663]
Loss:[0.0771]
Loss:[0.0690]
Loss:[0.0803]
Loss:[0.0830]
Loss:[0.0805]
Loss:[0.0704]
Loss:[0.0797]
Loss:[0.0836]
Loss:[0.0655]
Loss:[0.0684]
Loss:[0.0627]
Loss:[0.0732]
Loss:[0.0683]
Loss:[0.0667]
Loss:[0.0739]
Loss:[0.0663]
Loss:[0.0540]
Loss:[0.0658]
Loss:[0.0626]
Loss:[0.0681]
Loss:[0.0672]
Loss:[0.0717]
Loss:[0.0693]
Loss:[0.0547]
Loss:[0.0530]
Loss:[0.0602]
Loss:[0.0589]
Loss:[0.0572]
Loss:[0.0600]
Loss:[0.0566]
Loss:[0.0720]
Loss:[0.0625]
Loss:[0.0629]
Loss:[0.0611]
Loss:[0.0544]
Loss:[0.0575]
Loss:[0.0545]
Loss:[0.0511]
Loss:[0.0519]
Loss:[0.0511]
Loss:[0.0454]
Loss:[0.0484]
Loss:[0.0548]
Loss:[0.0543]
Loss:[0.0558]
Loss:[0.0528]
Loss:[0.0466]
Loss:[0.0486]
Loss:[0.0492]
Loss:[0.0509]
Loss:[0.0499]
Loss:[0.0447]
Loss:[0.0473]
Loss:[0.0490]
Loss:[0.0444]
Loss:[0.0509]
Loss:[0.0473]
Loss:[0.0422]
Loss:[0.0429]
Loss:[0.0442]
Loss:[0.0446]
Loss:[0.0428]
Loss:[0.0458]
Loss:[0.0471]
Loss:[0.0409]
Loss:[0.0468]
Loss:[0.0370]
Loss:[0.0417]
Loss:[0.0502]
Loss:[0.0512]
Loss:[0.0413]
Loss:[0.0428]
Loss:[0.0444]
Loss:[0.0403]
Loss:[0.0377]
Loss:[0.0426]
Loss:[0.0372]
Loss:[0.0421]
Loss:[0.0346]
Loss:[0.0418]
Loss:[0.0394]
Loss:[0.0391]
Loss:[0.0361]
Loss:[0.0335]
Loss:[0.0378]
Loss:[0.0360]
Loss:[0.0343]
Loss:[0.0350]
Loss:[0.0368]
Loss:[0.0331]
Loss:[0.0369]
Loss:[0.0364]
Loss:[0.0377]
Loss:[0.0333]
Loss:[0.0359]
Loss:[0.0383]
Loss:[0.0371]
Loss:[0.0388]
Loss:[0.0344]
Loss:[0.0343]
Loss:[0.0313]
Loss:[0.0325]
Loss:[0.0352]
Loss:[0.0300]
Loss:[0.0293]
Loss:[0.0254]
Loss:[0.0318]
Loss:[0.0315]
Loss:[0.0323]
Loss:[0.0334]
Loss:[0.0420]
Loss:[0.0322]
Loss:[0.0357]
Loss:[0.0388]
Loss:[0.0302]
Loss:[0.0294]
Loss:[0.0342]
Loss:[0.0296]
Loss:[0.0320]
Loss:[0.0217]
Loss:[0.0363]
Loss:[0.0269]
Loss:[0.0258]
Loss:[0.0319]
Loss:[0.0268]
Loss:[0.0309]
Loss:[0.0264]
Loss:[0.0304]
Loss:[0.0342]
Loss:[0.0323]
Loss:[0.0291]
Loss:[0.0285]
Loss:[0.0307]
Loss:[0.0351]
Loss:[0.0272]
Loss:[0.0271]
Loss:[0.0250]
Loss:[0.0254]
Loss:[0.0251]
Loss:[0.0306]
Early stopping!
Loading 245th epoch
acc:[0.8180]
acc:[0.8230]
acc:[0.8200]
acc:[0.8190]
acc:[0.8210]
acc:[0.8180]
acc:[0.8190]
acc:[0.8160]
acc:[0.8190]
acc:[0.8180]
acc:[0.8190]
acc:[0.8200]
acc:[0.8210]
acc:[0.8200]
acc:[0.8210]
acc:[0.8190]
acc:[0.8200]
acc:[0.8210]
acc:[0.8190]
acc:[0.8210]
acc:[0.8190]
acc:[0.8180]
acc:[0.8210]
acc:[0.8220]
acc:[0.8200]
acc:[0.8180]
acc:[0.8180]
acc:[0.8180]
acc:[0.8190]
acc:[0.8180]
acc:[0.8140]
acc:[0.8190]
acc:[0.8200]
acc:[0.8200]
acc:[0.8190]
acc:[0.8210]
acc:[0.8200]
acc:[0.8200]
acc:[0.8190]
acc:[0.8180]
acc:[0.8220]
acc:[0.8200]
acc:[0.8200]
acc:[0.8170]
acc:[0.8210]
acc:[0.8200]
acc:[0.8200]
acc:[0.8210]
acc:[0.8230]
acc:[0.8220]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8196]
Mean:[81.9580]
Std :[0.1679]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='cora', drop_percent=0.1, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=31)
----------------------------------------------------------------------------------------------------
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6996]
Loss:[0.6861]
Loss:[0.6905]
Loss:[0.6863]
Loss:[0.6782]
Loss:[0.6766]
Loss:[0.6737]
Loss:[0.6636]
Loss:[0.6587]
Loss:[0.6539]
Loss:[0.6406]
Loss:[0.6350]
Loss:[0.6246]
Loss:[0.6099]
Loss:[0.6024]
Loss:[0.5843]
Loss:[0.5788]
Loss:[0.5599]
Loss:[0.5454]
Loss:[0.5318]
Loss:[0.5144]
Loss:[0.5033]
Loss:[0.4803]
Loss:[0.4733]
Loss:[0.4612]
Loss:[0.4360]
Loss:[0.4331]
Loss:[0.4155]
Loss:[0.3938]
Loss:[0.3847]
Loss:[0.3817]
Loss:[0.3637]
Loss:[0.3501]
Loss:[0.3406]
Loss:[0.3207]
Loss:[0.3146]
Loss:[0.3017]
Loss:[0.2941]
Loss:[0.2827]
Loss:[0.2777]
Loss:[0.2657]
Loss:[0.2608]
Loss:[0.2564]
Loss:[0.2477]
Loss:[0.2400]
Loss:[0.2330]
Loss:[0.2336]
Loss:[0.2268]
Loss:[0.2193]
Loss:[0.2100]
Loss:[0.2075]
Loss:[0.2053]
Loss:[0.2097]
Loss:[0.1969]
Loss:[0.1874]
Loss:[0.2011]
Loss:[0.1752]
Loss:[0.1874]
Loss:[0.1809]
Loss:[0.1713]
Loss:[0.1738]
Loss:[0.1653]
Loss:[0.1539]
Loss:[0.1597]
Loss:[0.1716]
Loss:[0.1563]
Loss:[0.1572]
Loss:[0.1520]
Loss:[0.1532]
Loss:[0.1369]
Loss:[0.1407]
Loss:[0.1399]
Loss:[0.1445]
Loss:[0.1377]
Loss:[0.1358]
Loss:[0.1422]
Loss:[0.1500]
Loss:[0.1287]
Loss:[0.1278]
Loss:[0.1238]
Loss:[0.1251]
Loss:[0.1162]
Loss:[0.1190]
Loss:[0.1171]
Loss:[0.1162]
Loss:[0.1198]
Loss:[0.1126]
Loss:[0.1254]
Loss:[0.0999]
Loss:[0.1137]
Loss:[0.0997]
Loss:[0.1079]
Loss:[0.1165]
Loss:[0.1030]
Loss:[0.1116]
Loss:[0.1152]
Loss:[0.1038]
Loss:[0.0995]
Loss:[0.1182]
Loss:[0.1100]
Loss:[0.0974]
Loss:[0.1079]
Loss:[0.0962]
Loss:[0.0953]
Loss:[0.0973]
Loss:[0.0968]
Loss:[0.0979]
Loss:[0.0833]
Loss:[0.0936]
Loss:[0.0823]
Loss:[0.0990]
Loss:[0.0857]
Loss:[0.0851]
Loss:[0.0850]
Loss:[0.0738]
Loss:[0.0810]
Loss:[0.0760]
Loss:[0.0778]
Loss:[0.0809]
Loss:[0.0673]
Loss:[0.0869]
Loss:[0.0786]
Loss:[0.0770]
Loss:[0.0787]
Loss:[0.0665]
Loss:[0.0766]
Loss:[0.0693]
Loss:[0.0801]
Loss:[0.0828]
Loss:[0.0807]
Loss:[0.0701]
Loss:[0.0801]
Loss:[0.0832]
Loss:[0.0661]
Loss:[0.0685]
Loss:[0.0630]
Loss:[0.0733]
Loss:[0.0683]
Loss:[0.0668]
Loss:[0.0738]
Loss:[0.0665]
Loss:[0.0540]
Loss:[0.0660]
Loss:[0.0625]
Loss:[0.0684]
Loss:[0.0673]
Loss:[0.0719]
Loss:[0.0696]
Loss:[0.0547]
Loss:[0.0532]
Loss:[0.0602]
Loss:[0.0591]
Loss:[0.0574]
Loss:[0.0603]
Loss:[0.0566]
Loss:[0.0720]
Loss:[0.0627]
Loss:[0.0630]
Loss:[0.0612]
Loss:[0.0545]
Loss:[0.0576]
Loss:[0.0546]
Loss:[0.0512]
Loss:[0.0520]
Loss:[0.0512]
Loss:[0.0454]
Loss:[0.0484]
Loss:[0.0548]
Loss:[0.0543]
Loss:[0.0559]
Loss:[0.0528]
Loss:[0.0466]
Loss:[0.0486]
Loss:[0.0492]
Loss:[0.0509]
Loss:[0.0501]
Loss:[0.0448]
Loss:[0.0474]
Loss:[0.0490]
Loss:[0.0444]
Loss:[0.0510]
Loss:[0.0474]
Loss:[0.0422]
Loss:[0.0429]
Loss:[0.0443]
Loss:[0.0447]
Loss:[0.0429]
Loss:[0.0459]
Loss:[0.0471]
Loss:[0.0409]
Loss:[0.0468]
Loss:[0.0370]
Loss:[0.0417]
Loss:[0.0502]
Loss:[0.0513]
Loss:[0.0413]
Loss:[0.0428]
Loss:[0.0444]
Loss:[0.0404]
Loss:[0.0378]
Loss:[0.0427]
Loss:[0.0372]
Loss:[0.0421]
Loss:[0.0346]
Loss:[0.0418]
Loss:[0.0395]
Loss:[0.0391]
Loss:[0.0362]
Loss:[0.0337]
Loss:[0.0378]
Loss:[0.0360]
Loss:[0.0343]
Loss:[0.0352]
Loss:[0.0369]
Loss:[0.0331]
Loss:[0.0370]
Loss:[0.0366]
Loss:[0.0378]
Loss:[0.0334]
Loss:[0.0361]
Loss:[0.0384]
Loss:[0.0372]
Loss:[0.0390]
Loss:[0.0346]
Loss:[0.0343]
Loss:[0.0313]
Loss:[0.0324]
Loss:[0.0352]
Loss:[0.0301]
Loss:[0.0293]
Loss:[0.0254]
Loss:[0.0320]
Loss:[0.0315]
Loss:[0.0323]
Loss:[0.0335]
Loss:[0.0420]
Loss:[0.0322]
Loss:[0.0356]
Loss:[0.0389]
Loss:[0.0301]
Loss:[0.0294]
Loss:[0.0342]
Loss:[0.0295]
Loss:[0.0320]
Loss:[0.0218]
Loss:[0.0361]
Loss:[0.0269]
Loss:[0.0257]
Loss:[0.0320]
Loss:[0.0267]
Loss:[0.0309]
Loss:[0.0265]
Loss:[0.0304]
Loss:[0.0343]
Loss:[0.0323]
Loss:[0.0292]
Loss:[0.0285]
Loss:[0.0309]
Loss:[0.0349]
Loss:[0.0272]
Loss:[0.0269]
Loss:[0.0250]
Loss:[0.0254]
Loss:[0.0251]
Loss:[0.0307]
Early stopping!
Loading 245th epoch
acc:[0.8190]
acc:[0.8190]
acc:[0.8190]
acc:[0.8200]
acc:[0.8190]
acc:[0.8190]
acc:[0.8200]
acc:[0.8170]
acc:[0.8190]
acc:[0.8180]
acc:[0.8200]
acc:[0.8190]
acc:[0.8180]
acc:[0.8200]
acc:[0.8200]
acc:[0.8170]
acc:[0.8210]
acc:[0.8190]
acc:[0.8200]
acc:[0.8200]
acc:[0.8200]
acc:[0.8180]
acc:[0.8180]
acc:[0.8180]
acc:[0.8150]
acc:[0.8170]
acc:[0.8170]
acc:[0.8190]
acc:[0.8160]
acc:[0.8190]
acc:[0.8180]
acc:[0.8180]
acc:[0.8210]
acc:[0.8180]
acc:[0.8180]
acc:[0.8200]
acc:[0.8170]
acc:[0.8200]
acc:[0.8200]
acc:[0.8170]
acc:[0.8200]
acc:[0.8190]
acc:[0.8170]
acc:[0.8170]
acc:[0.8200]
acc:[0.8190]
acc:[0.8160]
acc:[0.8220]
acc:[0.8200]
acc:[0.8200]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8187]
Mean:[81.8740]
Std :[0.1454]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='cora', drop_percent=0.2, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=31)
----------------------------------------------------------------------------------------------------
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Traceback (most recent call last):
  File "execute.py", line 206, in <module>
    loss.backward()
  File "/data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/tensor.py", line 185, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/autograd/__init__.py", line 127, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 2.64 GiB (GPU 0; 10.92 GiB total capacity; 125.72 MiB already allocated; 2.27 GiB free; 2.79 GiB reserved in total by PyTorch)
Exception raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7f985d6bf1e2 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1e64b (0x7f985d91564b in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x1f464 (0x7f985d916464 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x1faa1 (0x7f985d916aa1 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x11e (0x7f986063baee in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xf4f8e9 (0x7f985ea778e9 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xf69717 (0x7f985ea91717 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0x10e9c7d (0x7f9892080c7d in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #8: <unknown function> + 0x10e9f97 (0x7f9892080f97 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #9: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0xfa (0x7f989218ba1a in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #10: at::TensorIterator::allocate_outputs() + 0x378 (0x7f9891e1b4f8 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #11: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x1e6 (0x7f9891e1d166 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #12: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7f9891e1d65d in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #13: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&, bool) + 0x14a (0x7f9891e1d80a in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #14: at::native::mul(at::Tensor const&, at::Tensor const&) + 0x47 (0x7f9891b5aeb7 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #15: <unknown function> + 0xf3eae0 (0x7f985ea66ae0 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #16: <unknown function> + 0xa56530 (0x7f98919ed530 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #17: at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, at::Tensor const&, at::Tensor const&) const + 0xbc (0x7f98921d581c in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #18: at::mul(at::Tensor const&, at::Tensor const&) + 0x4b (0x7f989212682b in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xcc6952 (0x7f9891c5d952 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #20: at::native::_trilinear(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long) + 0xdba (0x7f9891c5ef4a in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #21: <unknown function> + 0x129b8c0 (0x7f98922328c0 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x12c508b (0x7f989225c08b in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #23: at::_trilinear(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long) + 0x1d7 (0x7f9892183337 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #24: <unknown function> + 0x2dfa205 (0x7f9893d91205 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #25: <unknown function> + 0x12c508b (0x7f989225c08b in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #26: at::_trilinear(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long) + 0x1d7 (0x7f9892183337 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #27: torch::autograd::generated::TrilinearBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x6a8 (0x7f9893cf0f78 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #28: <unknown function> + 0x3375bb7 (0x7f989430cbb7 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #29: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7f9894308400 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #30: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7f9894308fa1 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #31: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7f9894301119 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #32: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7f98a1aa134a in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
frame #33: <unknown function> + 0xc819d (0x7f98a398619d in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/scipy/sparse/../../../../libstdc++.so.6)
frame #34: <unknown function> + 0x76ba (0x7f98b4e016ba in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #35: clone + 0x6d (0x7f98b4b374dd in /lib/x86_64-linux-gnu/libc.so.6)

----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='cora', drop_percent=0.3, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=31)
----------------------------------------------------------------------------------------------------
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6996]
Loss:[0.6861]
Loss:[0.6905]
Loss:[0.6863]
Loss:[0.6782]
Loss:[0.6766]
Loss:[0.6737]
Loss:[0.6636]
Loss:[0.6587]
Loss:[0.6539]
Loss:[0.6406]
Loss:[0.6350]
Loss:[0.6246]
Loss:[0.6099]
Loss:[0.6024]
Loss:[0.5843]
Loss:[0.5788]
Loss:[0.5599]
Loss:[0.5454]
Loss:[0.5317]
Loss:[0.5144]
Loss:[0.5033]
Loss:[0.4803]
Loss:[0.4733]
Loss:[0.4612]
Loss:[0.4360]
Loss:[0.4330]
Loss:[0.4155]
Loss:[0.3938]
Loss:[0.3846]
Loss:[0.3816]
Loss:[0.3637]
Loss:[0.3500]
Loss:[0.3406]
Loss:[0.3207]
Loss:[0.3145]
Loss:[0.3015]
Loss:[0.2942]
Loss:[0.2826]
Loss:[0.2775]
Loss:[0.2656]
Loss:[0.2609]
Loss:[0.2564]
Loss:[0.2475]
Loss:[0.2400]
Loss:[0.2331]
Loss:[0.2337]
Loss:[0.2266]
Loss:[0.2191]
Loss:[0.2099]
Loss:[0.2075]
Loss:[0.2054]
Loss:[0.2096]
Loss:[0.1967]
Loss:[0.1874]
Loss:[0.2011]
Loss:[0.1753]
Loss:[0.1874]
Loss:[0.1808]
Loss:[0.1713]
Loss:[0.1739]
Loss:[0.1654]
Loss:[0.1538]
Loss:[0.1597]
Loss:[0.1718]
Loss:[0.1564]
Loss:[0.1571]
Loss:[0.1523]
Loss:[0.1538]
Loss:[0.1367]
Loss:[0.1401]
Loss:[0.1402]
Loss:[0.1447]
Loss:[0.1374]
Loss:[0.1361]
Loss:[0.1429]
Loss:[0.1500]
Loss:[0.1278]
Loss:[0.1274]
Loss:[0.1242]
Loss:[0.1250]
Loss:[0.1164]
Loss:[0.1201]
Loss:[0.1169]
Loss:[0.1150]
Loss:[0.1203]
Loss:[0.1124]
Loss:[0.1227]
Loss:[0.0996]
Loss:[0.1131]
Loss:[0.0986]
Loss:[0.1081]
Loss:[0.1151]
Loss:[0.1016]
Loss:[0.1115]
Loss:[0.1122]
Loss:[0.1011]
Loss:[0.0982]
Loss:[0.1131]
Loss:[0.1096]
Loss:[0.0935]
Loss:[0.1091]
Loss:[0.1024]
Loss:[0.0940]
Loss:[0.1069]
Loss:[0.0959]
Loss:[0.1001]
Loss:[0.0852]
Loss:[0.0916]
Loss:[0.0853]
Loss:[0.0972]
Loss:[0.0898]
Loss:[0.0826]
Loss:[0.0852]
Loss:[0.0756]
Loss:[0.0800]
Loss:[0.0772]
Loss:[0.0771]
Loss:[0.0822]
Loss:[0.0660]
Loss:[0.0872]
Loss:[0.0804]
Loss:[0.0758]
Loss:[0.0802]
Loss:[0.0661]
Loss:[0.0772]
Loss:[0.0689]
Loss:[0.0801]
Loss:[0.0833]
Loss:[0.0802]
Loss:[0.0704]
Loss:[0.0792]
Loss:[0.0837]
Loss:[0.0650]
Loss:[0.0680]
Loss:[0.0624]
Loss:[0.0730]
Loss:[0.0682]
Loss:[0.0663]
Loss:[0.0738]
Loss:[0.0661]
Loss:[0.0539]
Loss:[0.0655]
Loss:[0.0624]
Loss:[0.0679]
Loss:[0.0669]
Loss:[0.0716]
Loss:[0.0690]
Loss:[0.0548]
Loss:[0.0527]
Loss:[0.0603]
Loss:[0.0586]
Loss:[0.0572]
Loss:[0.0597]
Loss:[0.0567]
Loss:[0.0716]
Loss:[0.0624]
Loss:[0.0626]
Loss:[0.0610]
Loss:[0.0542]
Loss:[0.0573]
Loss:[0.0544]
Loss:[0.0509]
Loss:[0.0520]
Loss:[0.0509]
Loss:[0.0456]
Loss:[0.0481]
Loss:[0.0550]
Loss:[0.0542]
Loss:[0.0559]
Loss:[0.0529]
Loss:[0.0465]
Loss:[0.0487]
Loss:[0.0492]
Loss:[0.0508]
Loss:[0.0498]
Loss:[0.0448]
Loss:[0.0473]
Loss:[0.0490]
Loss:[0.0445]
Loss:[0.0507]
Loss:[0.0473]
Loss:[0.0421]
Loss:[0.0430]
Loss:[0.0442]
Loss:[0.0445]
Loss:[0.0428]
Loss:[0.0459]
Loss:[0.0471]
Loss:[0.0409]
Loss:[0.0468]
Loss:[0.0370]
Loss:[0.0416]
Loss:[0.0502]
Loss:[0.0513]
Loss:[0.0413]
Loss:[0.0428]
Loss:[0.0444]
Loss:[0.0404]
Loss:[0.0378]
Loss:[0.0427]
Loss:[0.0372]
Loss:[0.0420]
Loss:[0.0346]
Loss:[0.0416]
Loss:[0.0394]
Loss:[0.0390]
Loss:[0.0361]
Loss:[0.0335]
Loss:[0.0377]
Loss:[0.0359]
Loss:[0.0343]
Loss:[0.0350]
Loss:[0.0368]
Loss:[0.0329]
Loss:[0.0370]
Loss:[0.0364]
Loss:[0.0378]
Loss:[0.0332]
Loss:[0.0360]
Loss:[0.0383]
Loss:[0.0372]
Loss:[0.0390]
Loss:[0.0345]
Loss:[0.0343]
Loss:[0.0312]
Loss:[0.0326]
Loss:[0.0351]
Loss:[0.0301]
Loss:[0.0292]
Loss:[0.0255]
Loss:[0.0318]
Loss:[0.0316]
Loss:[0.0324]
Loss:[0.0334]
Loss:[0.0419]
Loss:[0.0323]
Loss:[0.0356]
Loss:[0.0388]
Loss:[0.0302]
Loss:[0.0293]
Loss:[0.0342]
Loss:[0.0296]
Loss:[0.0319]
Loss:[0.0217]
Loss:[0.0364]
Loss:[0.0269]
Loss:[0.0259]
Loss:[0.0320]
Loss:[0.0269]
Loss:[0.0310]
Loss:[0.0264]
Loss:[0.0306]
Loss:[0.0342]
Loss:[0.0324]
Loss:[0.0290]
Loss:[0.0287]
Loss:[0.0307]
Loss:[0.0350]
Loss:[0.0271]
Loss:[0.0270]
Loss:[0.0249]
Loss:[0.0254]
Loss:[0.0250]
Loss:[0.0307]
Early stopping!
Loading 245th epoch
acc:[0.8160]
acc:[0.8170]
acc:[0.8180]
acc:[0.8170]
acc:[0.8160]
acc:[0.8150]
acc:[0.8170]
acc:[0.8150]
acc:[0.8180]
acc:[0.8170]
acc:[0.8170]
acc:[0.8160]
acc:[0.8200]
acc:[0.8190]
acc:[0.8160]
acc:[0.8170]
acc:[0.8190]
acc:[0.8170]
acc:[0.8160]
acc:[0.8170]
acc:[0.8180]
acc:[0.8150]
acc:[0.8210]
acc:[0.8170]
acc:[0.8170]
acc:[0.8160]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
acc:[0.8170]
acc:[0.8150]
acc:[0.8170]
acc:[0.8180]
acc:[0.8200]
acc:[0.8160]
acc:[0.8170]
acc:[0.8190]
acc:[0.8210]
acc:[0.8160]
acc:[0.8160]
acc:[0.8190]
acc:[0.8180]
acc:[0.8160]
acc:[0.8160]
acc:[0.8200]
acc:[0.8150]
acc:[0.8180]
acc:[0.8190]
acc:[0.8210]
acc:[0.8220]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8174]
Mean:[81.7380]
Std :[0.1760]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='cora', drop_percent=0.4, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=31)
----------------------------------------------------------------------------------------------------
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6996]
Traceback (most recent call last):
  File "execute.py", line 206, in <module>
    loss.backward()
  File "/data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/tensor.py", line 185, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/autograd/__init__.py", line 127, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 2.64 GiB (GPU 0; 10.92 GiB total capacity; 126.58 MiB already allocated; 2.26 GiB free; 2.79 GiB reserved in total by PyTorch)
Exception raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7f8b9da8c1e2 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1e64b (0x7f8b9dce264b in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x1f464 (0x7f8b9dce3464 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x1faa1 (0x7f8b9dce3aa1 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)
frame #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x11e (0x7f8ba0a08aee in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xf4f8e9 (0x7f8b9ee448e9 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xf69717 (0x7f8b9ee5e717 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0x10e9c7d (0x7f8bd244dc7d in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #8: <unknown function> + 0x10e9f97 (0x7f8bd244df97 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #9: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0xfa (0x7f8bd2558a1a in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #10: at::TensorIterator::allocate_outputs() + 0x378 (0x7f8bd21e84f8 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #11: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x1e6 (0x7f8bd21ea166 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #12: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7f8bd21ea65d in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #13: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&, bool) + 0x14a (0x7f8bd21ea80a in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #14: at::native::mul(at::Tensor const&, at::Tensor const&) + 0x47 (0x7f8bd1f27eb7 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #15: <unknown function> + 0xf3eae0 (0x7f8b9ee33ae0 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)
frame #16: <unknown function> + 0xa56530 (0x7f8bd1dba530 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #17: at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, at::Tensor const&, at::Tensor const&) const + 0xbc (0x7f8bd25a281c in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #18: at::mul(at::Tensor const&, at::Tensor const&) + 0x4b (0x7f8bd24f382b in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xcc6952 (0x7f8bd202a952 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #20: at::native::_trilinear(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long) + 0xdba (0x7f8bd202bf4a in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #21: <unknown function> + 0x129b8c0 (0x7f8bd25ff8c0 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #22: <unknown function> + 0x12c508b (0x7f8bd262908b in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #23: at::_trilinear(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long) + 0x1d7 (0x7f8bd2550337 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #24: <unknown function> + 0x2dfa205 (0x7f8bd415e205 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #25: <unknown function> + 0x12c508b (0x7f8bd262908b in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #26: at::_trilinear(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long) + 0x1d7 (0x7f8bd2550337 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #27: torch::autograd::generated::TrilinearBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x6a8 (0x7f8bd40bdf78 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #28: <unknown function> + 0x3375bb7 (0x7f8bd46d9bb7 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #29: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7f8bd46d5400 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #30: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7f8bd46d5fa1 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #31: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7f8bd46ce119 in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #32: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7f8be1e6e34a in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
frame #33: <unknown function> + 0xc819d (0x7f8be3d5319d in /data3/Syd/.conda/envs/syd/lib/python3.6/site-packages/scipy/sparse/../../../../libstdc++.so.6)
frame #34: <unknown function> + 0x76ba (0x7f8bf51ce6ba in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #35: clone + 0x6d (0x7f8bf4f044dd in /lib/x86_64-linux-gnu/libc.so.6)

----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='cora', drop_percent=0.5, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=31)
----------------------------------------------------------------------------------------------------
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6996]
Loss:[0.6861]
Loss:[0.6905]
Loss:[0.6863]
Loss:[0.6782]
Loss:[0.6766]
Loss:[0.6737]
Loss:[0.6636]
Loss:[0.6587]
Loss:[0.6539]
Loss:[0.6406]
Loss:[0.6350]
Loss:[0.6246]
Loss:[0.6099]
Loss:[0.6024]
Loss:[0.5843]
Loss:[0.5788]
Loss:[0.5599]
Loss:[0.5454]
Loss:[0.5318]
Loss:[0.5144]
Loss:[0.5033]
Loss:[0.4803]
Loss:[0.4732]
Loss:[0.4612]
Loss:[0.4360]
Loss:[0.4329]
Loss:[0.4155]
Loss:[0.3938]
Loss:[0.3845]
Loss:[0.3816]
Loss:[0.3637]
Loss:[0.3498]
Loss:[0.3406]
Loss:[0.3208]
Loss:[0.3143]
Loss:[0.3014]
Loss:[0.2943]
Loss:[0.2826]
Loss:[0.2773]
Loss:[0.2655]
Loss:[0.2609]
Loss:[0.2563]
Loss:[0.2473]
Loss:[0.2399]
Loss:[0.2331]
Loss:[0.2337]
Loss:[0.2265]
Loss:[0.2190]
Loss:[0.2099]
Loss:[0.2077]
Loss:[0.2054]
Loss:[0.2095]
Loss:[0.1968]
Loss:[0.1874]
Loss:[0.2012]
Loss:[0.1753]
Loss:[0.1873]
Loss:[0.1808]
Loss:[0.1714]
Loss:[0.1739]
Loss:[0.1653]
Loss:[0.1537]
Loss:[0.1597]
Loss:[0.1719]
Loss:[0.1564]
Loss:[0.1571]
Loss:[0.1525]
Loss:[0.1541]
Loss:[0.1365]
Loss:[0.1399]
Loss:[0.1404]
Loss:[0.1446]
Loss:[0.1374]
Loss:[0.1364]
Loss:[0.1431]
Loss:[0.1498]
Loss:[0.1275]
Loss:[0.1273]
Loss:[0.1244]
Loss:[0.1249]
Loss:[0.1166]
Loss:[0.1205]
Loss:[0.1168]
Loss:[0.1148]
Loss:[0.1206]
Loss:[0.1122]
Loss:[0.1218]
Loss:[0.0996]
Loss:[0.1129]
Loss:[0.0983]
Loss:[0.1082]
Loss:[0.1146]
Loss:[0.1012]
Loss:[0.1116]
Loss:[0.1110]
Loss:[0.1001]
Loss:[0.0979]
Loss:[0.1111]
Loss:[0.1087]
Loss:[0.0922]
Loss:[0.1075]
Loss:[0.1045]
Loss:[0.0926]
Loss:[0.1104]
Loss:[0.0987]
Loss:[0.1009]
Loss:[0.0893]
Loss:[0.0897]
Loss:[0.0869]
Loss:[0.0959]
Loss:[0.0914]
Loss:[0.0836]
Loss:[0.0852]
Loss:[0.0782]
Loss:[0.0798]
Loss:[0.0784]
Loss:[0.0770]
Loss:[0.0830]
Loss:[0.0664]
Loss:[0.0876]
Loss:[0.0820]
Loss:[0.0754]
Loss:[0.0811]
Loss:[0.0661]
Loss:[0.0777]
Loss:[0.0692]
Loss:[0.0804]
Loss:[0.0835]
Loss:[0.0804]
Loss:[0.0706]
Loss:[0.0794]
Loss:[0.0839]
Loss:[0.0652]
Loss:[0.0682]
Loss:[0.0626]
Loss:[0.0730]
Loss:[0.0681]
Loss:[0.0665]
Loss:[0.0737]
Loss:[0.0667]
Loss:[0.0539]
Loss:[0.0663]
Loss:[0.0622]
Loss:[0.0690]
Loss:[0.0670]
Loss:[0.0724]
Loss:[0.0687]
Loss:[0.0544]
Loss:[0.0529]
Loss:[0.0605]
Loss:[0.0592]
Loss:[0.0565]
Loss:[0.0600]
Loss:[0.0558]
Loss:[0.0731]
Loss:[0.0622]
Loss:[0.0637]
Loss:[0.0605]
Loss:[0.0552]
Loss:[0.0576]
Loss:[0.0548]
Loss:[0.0513]
Loss:[0.0515]
Loss:[0.0513]
Loss:[0.0447]
Loss:[0.0492]
Loss:[0.0542]
Loss:[0.0546]
Loss:[0.0550]
Loss:[0.0527]
Loss:[0.0464]
Loss:[0.0488]
Loss:[0.0492]
Loss:[0.0515]
Loss:[0.0500]
Loss:[0.0446]
Loss:[0.0473]
Loss:[0.0491]
Loss:[0.0444]
Loss:[0.0515]
Loss:[0.0476]
Loss:[0.0424]
Loss:[0.0432]
Loss:[0.0445]
Loss:[0.0448]
Loss:[0.0431]
Loss:[0.0460]
Loss:[0.0474]
Loss:[0.0413]
Loss:[0.0468]
Loss:[0.0370]
Loss:[0.0417]
Loss:[0.0501]
Loss:[0.0514]
Loss:[0.0414]
Loss:[0.0430]
Loss:[0.0445]
Loss:[0.0404]
Loss:[0.0379]
Loss:[0.0429]
Loss:[0.0371]
Loss:[0.0421]
Loss:[0.0347]
Loss:[0.0419]
Loss:[0.0395]
Loss:[0.0391]
Loss:[0.0361]
Loss:[0.0337]
Loss:[0.0378]
Loss:[0.0360]
Loss:[0.0345]
Loss:[0.0351]
Loss:[0.0370]
Loss:[0.0331]
Loss:[0.0371]
Loss:[0.0367]
Loss:[0.0377]
Loss:[0.0335]
Loss:[0.0358]
Loss:[0.0385]
Loss:[0.0372]
Loss:[0.0387]
Loss:[0.0346]
Loss:[0.0342]
Loss:[0.0315]
Loss:[0.0325]
Loss:[0.0353]
Loss:[0.0301]
Loss:[0.0296]
Loss:[0.0254]
Loss:[0.0321]
Loss:[0.0314]
Loss:[0.0324]
Loss:[0.0334]
Loss:[0.0421]
Loss:[0.0321]
Loss:[0.0354]
Loss:[0.0389]
Loss:[0.0299]
Loss:[0.0296]
Loss:[0.0341]
Loss:[0.0295]
Loss:[0.0320]
Loss:[0.0218]
Loss:[0.0359]
Loss:[0.0268]
Loss:[0.0256]
Loss:[0.0316]
Loss:[0.0268]
Loss:[0.0308]
Loss:[0.0266]
Loss:[0.0301]
Loss:[0.0343]
Loss:[0.0322]
Loss:[0.0293]
Loss:[0.0283]
Loss:[0.0309]
Loss:[0.0348]
Loss:[0.0273]
Loss:[0.0269]
Loss:[0.0251]
Loss:[0.0252]
Loss:[0.0254]
Loss:[0.0306]
Early stopping!
Loading 245th epoch
acc:[0.8140]
acc:[0.8190]
acc:[0.8170]
acc:[0.8170]
acc:[0.8180]
acc:[0.8170]
acc:[0.8190]
acc:[0.8180]
acc:[0.8190]
acc:[0.8170]
acc:[0.8190]
acc:[0.8180]
acc:[0.8210]
acc:[0.8170]
acc:[0.8190]
acc:[0.8170]
acc:[0.8180]
acc:[0.8190]
acc:[0.8180]
acc:[0.8190]
acc:[0.8200]
acc:[0.8190]
acc:[0.8190]
acc:[0.8170]
acc:[0.8190]
acc:[0.8160]
acc:[0.8180]
acc:[0.8190]
acc:[0.8140]
acc:[0.8160]
acc:[0.8120]
acc:[0.8170]
acc:[0.8180]
acc:[0.8180]
acc:[0.8190]
acc:[0.8180]
acc:[0.8170]
acc:[0.8180]
acc:[0.8190]
acc:[0.8170]
acc:[0.8200]
acc:[0.8180]
acc:[0.8200]
acc:[0.8170]
acc:[0.8160]
acc:[0.8190]
acc:[0.8160]
acc:[0.8170]
acc:[0.8210]
acc:[0.8180]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8178]
Mean:[81.7840]
Std :[0.1683]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='cora', drop_percent=0.6, gpu=6, save_name='cora_best_dgi.pkl.pkl', seed=31)
----------------------------------------------------------------------------------------------------
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6996]
Loss:[0.6861]
Loss:[0.6905]
Loss:[0.6863]
Loss:[0.6782]
Loss:[0.6766]
Loss:[0.6737]
Loss:[0.6636]
Loss:[0.6587]
Loss:[0.6539]
Loss:[0.6406]
Loss:[0.6350]
Loss:[0.6246]
Loss:[0.6099]
Loss:[0.6024]
Loss:[0.5843]
Loss:[0.5788]
Loss:[0.5599]
Loss:[0.5454]
Loss:[0.5317]
Loss:[0.5144]
Loss:[0.5033]
Loss:[0.4803]
Loss:[0.4733]
Loss:[0.4612]
Loss:[0.4360]
Loss:[0.4330]
Loss:[0.4155]
Loss:[0.3938]
Loss:[0.3845]
Loss:[0.3816]
Loss:[0.3637]
Loss:[0.3499]
Loss:[0.3406]
Loss:[0.3208]
Loss:[0.3144]
Loss:[0.3015]
Loss:[0.2943]
Loss:[0.2827]
Loss:[0.2774]
Loss:[0.2656]
Loss:[0.2609]
Loss:[0.2564]
Loss:[0.2474]
Loss:[0.2400]
Loss:[0.2331]
Loss:[0.2337]
Loss:[0.2266]
Loss:[0.2191]
Loss:[0.2100]
Loss:[0.2076]
Loss:[0.2054]
Loss:[0.2096]
Loss:[0.1968]
Loss:[0.1874]
Loss:[0.2012]
Loss:[0.1753]
Loss:[0.1874]
Loss:[0.1808]
Loss:[0.1713]
Loss:[0.1738]
Loss:[0.1653]
Loss:[0.1537]
Loss:[0.1597]
Loss:[0.1718]
Loss:[0.1564]
Loss:[0.1571]
Loss:[0.1523]
Loss:[0.1538]
Loss:[0.1367]
Loss:[0.1401]
Loss:[0.1402]
Loss:[0.1447]
Loss:[0.1375]
Loss:[0.1361]
Loss:[0.1428]
Loss:[0.1500]
Loss:[0.1279]
Loss:[0.1275]
Loss:[0.1242]
Loss:[0.1250]
Loss:[0.1164]
Loss:[0.1199]
Loss:[0.1170]
Loss:[0.1152]
Loss:[0.1202]
Loss:[0.1124]
Loss:[0.1232]
Loss:[0.0997]
Loss:[0.1132]
Loss:[0.0989]
Loss:[0.1081]
Loss:[0.1154]
Loss:[0.1019]
Loss:[0.1116]
Loss:[0.1126]
Loss:[0.1016]
Loss:[0.0985]
Loss:[0.1142]
Loss:[0.1100]
Loss:[0.0942]
Loss:[0.1095]
Loss:[0.1014]
Loss:[0.0946]
Loss:[0.1050]
Loss:[0.0954]
Loss:[0.0996]
Loss:[0.0841]
Loss:[0.0924]
Loss:[0.0846]
Loss:[0.0978]
Loss:[0.0890]
Loss:[0.0827]
Loss:[0.0852]
Loss:[0.0748]
Loss:[0.0802]
Loss:[0.0768]
Loss:[0.0772]
Loss:[0.0820]
Loss:[0.0662]
Loss:[0.0872]
Loss:[0.0799]
Loss:[0.0762]
Loss:[0.0800]
Loss:[0.0662]
Loss:[0.0771]
Loss:[0.0690]
Loss:[0.0802]
Loss:[0.0831]
Loss:[0.0803]
Loss:[0.0704]
Loss:[0.0793]
Loss:[0.0836]
Loss:[0.0651]
Loss:[0.0680]
Loss:[0.0624]
Loss:[0.0729]
Loss:[0.0683]
Loss:[0.0663]
Loss:[0.0740]
Loss:[0.0661]
Loss:[0.0540]
Loss:[0.0654]
Loss:[0.0625]
Loss:[0.0678]
Loss:[0.0670]
Loss:[0.0713]
Loss:[0.0690]
Loss:[0.0549]
Loss:[0.0527]
Loss:[0.0602]
Loss:[0.0583]
Loss:[0.0573]
Loss:[0.0595]
Loss:[0.0568]
Loss:[0.0713]
Loss:[0.0624]
Loss:[0.0623]
Loss:[0.0610]
Loss:[0.0541]
Loss:[0.0572]
Loss:[0.0545]
Loss:[0.0507]
Loss:[0.0521]
Loss:[0.0507]
Loss:[0.0458]
Loss:[0.0479]
Loss:[0.0550]
Loss:[0.0542]
Loss:[0.0559]
Loss:[0.0531]
Loss:[0.0465]
Loss:[0.0487]
Loss:[0.0491]
Loss:[0.0508]
Loss:[0.0497]
Loss:[0.0449]
Loss:[0.0474]
Loss:[0.0491]
Loss:[0.0446]
Loss:[0.0508]
Loss:[0.0472]
Loss:[0.0421]
Loss:[0.0429]
Loss:[0.0442]
Loss:[0.0446]
Loss:[0.0428]
Loss:[0.0459]
Loss:[0.0471]
Loss:[0.0408]
Loss:[0.0468]
Loss:[0.0370]
Loss:[0.0417]
Loss:[0.0502]
Loss:[0.0514]
Loss:[0.0413]
Loss:[0.0429]
Loss:[0.0444]
Loss:[0.0404]
Loss:[0.0378]
Loss:[0.0427]
Loss:[0.0373]
Loss:[0.0421]
Loss:[0.0346]
Loss:[0.0418]
Loss:[0.0394]
Loss:[0.0391]
Loss:[0.0362]
Loss:[0.0335]
Loss:[0.0377]
Loss:[0.0359]
Loss:[0.0343]
Loss:[0.0350]
Loss:[0.0368]
Loss:[0.0330]
Loss:[0.0369]
Loss:[0.0363]
Loss:[0.0376]
Loss:[0.0332]
Loss:[0.0359]
Loss:[0.0382]
Loss:[0.0370]
Loss:[0.0392]
Loss:[0.0345]
Loss:[0.0343]
Loss:[0.0311]
Loss:[0.0326]
Loss:[0.0351]
Loss:[0.0301]
Loss:[0.0292]
Loss:[0.0254]
Loss:[0.0317]
Loss:[0.0316]
Loss:[0.0323]
Loss:[0.0334]
Loss:[0.0419]
Loss:[0.0322]
Loss:[0.0358]
Loss:[0.0387]
Loss:[0.0303]
Loss:[0.0295]
Loss:[0.0342]
Loss:[0.0296]
Loss:[0.0320]
Loss:[0.0217]
Loss:[0.0362]
Loss:[0.0270]
Loss:[0.0258]
Loss:[0.0322]
Loss:[0.0267]
Loss:[0.0311]
Loss:[0.0263]
Loss:[0.0306]
Loss:[0.0340]
Loss:[0.0325]
Loss:[0.0289]
Loss:[0.0287]
Loss:[0.0307]
Loss:[0.0352]
Loss:[0.0271]
Loss:[0.0271]
Loss:[0.0248]
Loss:[0.0255]
Loss:[0.0250]
Loss:[0.0307]
Early stopping!
Loading 245th epoch
acc:[0.8150]
acc:[0.8190]
acc:[0.8150]
acc:[0.8160]
acc:[0.8180]
acc:[0.8180]
acc:[0.8150]
acc:[0.8140]
acc:[0.8130]
acc:[0.8140]
acc:[0.8170]
acc:[0.8180]
acc:[0.8180]
acc:[0.8160]
acc:[0.8160]
acc:[0.8160]
acc:[0.8180]
acc:[0.8160]
acc:[0.8180]
acc:[0.8180]
acc:[0.8150]
acc:[0.8160]
acc:[0.8160]
acc:[0.8180]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
acc:[0.8170]
acc:[0.8150]
acc:[0.8160]
acc:[0.8120]
acc:[0.8180]
acc:[0.8170]
acc:[0.8170]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
acc:[0.8160]
acc:[0.8160]
acc:[0.8170]
acc:[0.8180]
acc:[0.8170]
acc:[0.8150]
acc:[0.8160]
acc:[0.8170]
acc:[0.8160]
acc:[0.8160]
acc:[0.8180]
acc:[0.8160]
acc:[0.8200]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.8164]
Mean:[81.6420]
Std :[0.1486]
----------------------------------------------------------------------------------------------------
