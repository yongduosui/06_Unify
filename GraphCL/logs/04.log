----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.2, gpu=0, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6940]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6793]
Loss:[0.6811]
Loss:[0.6692]
Loss:[0.6649]
Loss:[0.6572]
Loss:[0.6437]
Loss:[0.6386]
Loss:[0.6210]
Loss:[0.6135]
Loss:[0.5932]
Loss:[0.5843]
Loss:[0.5650]
Loss:[0.5459]
Loss:[0.5347]
Loss:[0.5091]
Loss:[0.4974]
Loss:[0.4810]
Loss:[0.4545]
Loss:[0.4400]
Loss:[0.4283]
Loss:[0.4092]
Loss:[0.3837]
Loss:[0.3718]
Loss:[0.3632]
Loss:[0.3429]
Loss:[0.3208]
Loss:[0.3132]
Loss:[0.3023]
Loss:[0.2856]
Loss:[0.2767]
Loss:[0.2617]
Loss:[0.2535]
Loss:[0.2453]
Loss:[0.2386]
Loss:[0.2302]
Loss:[0.2218]
Loss:[0.2162]
Loss:[0.2103]
Loss:[0.2103]
Loss:[0.1940]
Loss:[0.1885]
Loss:[0.1827]
Loss:[0.1834]
Loss:[0.1748]
Loss:[0.1695]
Loss:[0.1615]
Loss:[0.1637]
Loss:[0.1582]
Loss:[0.1619]
Loss:[0.1529]
Loss:[0.1550]
Loss:[0.1484]
Loss:[0.1382]
Loss:[0.1470]
Loss:[0.1416]
Loss:[0.1375]
Loss:[0.1297]
Loss:[0.1412]
Loss:[0.1434]
Loss:[0.1328]
Loss:[0.1220]
Loss:[0.1365]
Loss:[0.1272]
Loss:[0.1190]
Loss:[0.1236]
Loss:[0.1287]
Loss:[0.1300]
Loss:[0.1240]
Loss:[0.1211]
Loss:[0.1296]
Loss:[0.1187]
Loss:[0.1217]
Loss:[0.1248]
Loss:[0.1179]
Loss:[0.1079]
Loss:[0.1215]
Loss:[0.1195]
Loss:[0.1186]
Loss:[0.1139]
Loss:[0.1138]
Loss:[0.1070]
Loss:[0.1126]
Loss:[0.1094]
Loss:[0.1079]
Loss:[0.1024]
Loss:[0.1030]
Loss:[0.1046]
Loss:[0.1118]
Loss:[0.1069]
Loss:[0.1058]
Loss:[0.1060]
Loss:[0.1030]
Loss:[0.1006]
Loss:[0.1162]
Loss:[0.1030]
Loss:[0.1032]
Loss:[0.0964]
Loss:[0.0999]
Loss:[0.1007]
Loss:[0.1014]
Loss:[0.0923]
Loss:[0.1039]
Loss:[0.1020]
Loss:[0.0998]
Loss:[0.1048]
Loss:[0.0942]
Loss:[0.0947]
Loss:[0.0965]
Loss:[0.1015]
Loss:[0.0948]
Loss:[0.0913]
Loss:[0.0985]
Loss:[0.0976]
Loss:[0.0932]
Loss:[0.1052]
Loss:[0.1012]
Loss:[0.1041]
Loss:[0.0993]
Loss:[0.0948]
Loss:[0.0965]
Loss:[0.0984]
Loss:[0.1045]
Loss:[0.0886]
Loss:[0.0973]
Loss:[0.0977]
Loss:[0.0980]
Loss:[0.0924]
Loss:[0.1032]
Loss:[0.0943]
Loss:[0.0956]
Loss:[0.0957]
Loss:[0.0927]
Loss:[0.0954]
Loss:[0.1010]
Loss:[0.0952]
Loss:[0.1115]
Loss:[0.0975]
Loss:[0.1051]
Loss:[0.1061]
Loss:[0.0906]
Loss:[0.1084]
Loss:[0.1223]
Loss:[0.0877]
Loss:[0.1313]
Loss:[0.1683]
Loss:[0.1172]
Loss:[0.1889]
Loss:[0.1462]
Loss:[0.1777]
Loss:[0.0908]
Loss:[0.2137]
Loss:[0.1894]
Loss:[0.2833]
Loss:[0.0934]
Loss:[0.4339]
Loss:[0.1950]
Loss:[0.5875]
Loss:[0.1663]
Loss:[0.1723]
Loss:[0.3593]
Loss:[0.1221]
Loss:[0.4052]
Loss:[0.3172]
Early stopping!
Loading 146th epoch
acc:[0.7270]
acc:[0.7250]
acc:[0.7220]
acc:[0.7260]
acc:[0.7250]
acc:[0.7230]
acc:[0.7240]
acc:[0.7230]
acc:[0.7260]
acc:[0.7290]
acc:[0.7270]
acc:[0.7270]
acc:[0.7220]
acc:[0.7270]
acc:[0.7240]
acc:[0.7230]
acc:[0.7240]
acc:[0.7260]
acc:[0.7260]
acc:[0.7250]
acc:[0.7230]
acc:[0.7240]
acc:[0.7240]
acc:[0.7230]
acc:[0.7260]
acc:[0.7260]
acc:[0.7240]
acc:[0.7250]
acc:[0.7250]
acc:[0.7250]
acc:[0.7260]
acc:[0.7270]
acc:[0.7260]
acc:[0.7220]
acc:[0.7230]
acc:[0.7240]
acc:[0.7210]
acc:[0.7260]
acc:[0.7250]
acc:[0.7270]
acc:[0.7260]
acc:[0.7240]
acc:[0.7270]
acc:[0.7250]
acc:[0.7240]
acc:[0.7220]
acc:[0.7250]
acc:[0.7220]
acc:[0.7250]
acc:[0.7200]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7247]
Mean:[72.4660]
Std :[0.1847]
----------------------------------------------------------------------------------------------------
