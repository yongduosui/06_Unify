----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.02, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6941]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6794]
Loss:[0.6811]
Loss:[0.6694]
Loss:[0.6649]
Loss:[0.6573]
Loss:[0.6438]
Loss:[0.6386]
Loss:[0.6211]
Loss:[0.6134]
Loss:[0.5933]
Loss:[0.5839]
Loss:[0.5651]
Loss:[0.5454]
Loss:[0.5348]
Loss:[0.5093]
Loss:[0.4965]
Loss:[0.4812]
Loss:[0.4547]
Loss:[0.4391]
Loss:[0.4281]
Loss:[0.4100]
Loss:[0.3837]
Loss:[0.3709]
Loss:[0.3627]
Loss:[0.3432]
Loss:[0.3211]
Loss:[0.3128]
Loss:[0.3017]
Loss:[0.2856]
Loss:[0.2772]
Loss:[0.2619]
Loss:[0.2531]
Loss:[0.2447]
Loss:[0.2384]
Loss:[0.2308]
Loss:[0.2221]
Loss:[0.2154]
Loss:[0.2087]
Loss:[0.2098]
Loss:[0.1946]
Loss:[0.1887]
Loss:[0.1814]
Loss:[0.1811]
Loss:[0.1745]
Loss:[0.1722]
Loss:[0.1632]
Loss:[0.1626]
Loss:[0.1565]
Loss:[0.1621]
Loss:[0.1523]
Loss:[0.1518]
Loss:[0.1465]
Loss:[0.1383]
Loss:[0.1457]
Loss:[0.1377]
Loss:[0.1355]
Loss:[0.1310]
Loss:[0.1367]
Loss:[0.1345]
Loss:[0.1336]
Loss:[0.1257]
Loss:[0.1312]
Loss:[0.1239]
Loss:[0.1278]
Loss:[0.1271]
Loss:[0.1247]
Loss:[0.1306]
Loss:[0.1295]
Loss:[0.1201]
Loss:[0.1253]
Loss:[0.1251]
Loss:[0.1292]
Loss:[0.1213]
Loss:[0.1254]
Loss:[0.1200]
Loss:[0.1168]
Loss:[0.1380]
Loss:[0.1451]
Loss:[0.1124]
Loss:[0.1507]
Loss:[0.1318]
Loss:[0.1332]
Loss:[0.1389]
Loss:[0.1071]
Loss:[0.1165]
Loss:[0.1040]
Loss:[0.1218]
Loss:[0.1109]
Loss:[0.1188]
Loss:[0.1041]
Loss:[0.1151]
Loss:[0.1063]
Loss:[0.1058]
Loss:[0.1128]
Loss:[0.1115]
Loss:[0.1076]
Loss:[0.1007]
Loss:[0.0996]
Loss:[0.1039]
Loss:[0.1013]
Loss:[0.0956]
Loss:[0.1038]
Loss:[0.1067]
Loss:[0.1008]
Loss:[0.1072]
Loss:[0.0948]
Loss:[0.0966]
Loss:[0.0971]
Loss:[0.1043]
Loss:[0.0957]
Loss:[0.0927]
Loss:[0.0995]
Loss:[0.0992]
Loss:[0.0952]
Loss:[0.1023]
Loss:[0.1024]
Loss:[0.1028]
Loss:[0.0999]
Loss:[0.0943]
Loss:[0.0978]
Loss:[0.0985]
Loss:[0.1060]
Loss:[0.0879]
Loss:[0.0986]
Loss:[0.0969]
Loss:[0.0986]
Loss:[0.0928]
Loss:[0.1020]
Loss:[0.0920]
Loss:[0.0931]
Loss:[0.0950]
Loss:[0.0912]
Loss:[0.0907]
Loss:[0.0991]
Loss:[0.0897]
Loss:[0.0969]
Loss:[0.0961]
Loss:[0.0978]
Loss:[0.0922]
Loss:[0.0902]
Loss:[0.0915]
Loss:[0.0945]
Loss:[0.0869]
Loss:[0.0855]
Loss:[0.0949]
Loss:[0.0916]
Loss:[0.0940]
Loss:[0.0901]
Loss:[0.0950]
Loss:[0.0872]
Loss:[0.0910]
Loss:[0.0914]
Loss:[0.0896]
Loss:[0.0934]
Loss:[0.0877]
Loss:[0.0934]
Loss:[0.0918]
Loss:[0.0819]
Loss:[0.0878]
Loss:[0.0847]
Loss:[0.0889]
Loss:[0.0857]
Loss:[0.0937]
Loss:[0.0850]
Loss:[0.0944]
Loss:[0.0887]
Loss:[0.0923]
Loss:[0.0952]
Loss:[0.0847]
Loss:[0.0892]
Loss:[0.0859]
Loss:[0.0856]
Loss:[0.0947]
Loss:[0.0870]
Loss:[0.0917]
Loss:[0.0845]
Loss:[0.0827]
Loss:[0.0832]
Early stopping!
Loading 161th epoch
acc:[0.7250]
acc:[0.7250]
acc:[0.7280]
acc:[0.7250]
acc:[0.7270]
acc:[0.7250]
acc:[0.7250]
acc:[0.7270]
acc:[0.7250]
acc:[0.7270]
acc:[0.7230]
acc:[0.7300]
acc:[0.7230]
acc:[0.7290]
acc:[0.7230]
acc:[0.7260]
acc:[0.7230]
acc:[0.7270]
acc:[0.7250]
acc:[0.7270]
acc:[0.7250]
acc:[0.7260]
acc:[0.7220]
acc:[0.7270]
acc:[0.7230]
acc:[0.7240]
acc:[0.7240]
acc:[0.7250]
acc:[0.7240]
acc:[0.7220]
acc:[0.7240]
acc:[0.7240]
acc:[0.7270]
acc:[0.7270]
acc:[0.7240]
acc:[0.7260]
acc:[0.7220]
acc:[0.7250]
acc:[0.7210]
acc:[0.7260]
acc:[0.7250]
acc:[0.7250]
acc:[0.7250]
acc:[0.7220]
acc:[0.7230]
acc:[0.7250]
acc:[0.7240]
acc:[0.7260]
acc:[0.7240]
acc:[0.7230]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7249]
Mean:[72.4900]
Std :[0.1887]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.05, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6941]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6794]
Loss:[0.6811]
Loss:[0.6694]
Loss:[0.6649]
Loss:[0.6573]
Loss:[0.6438]
Loss:[0.6386]
Loss:[0.6211]
Loss:[0.6133]
Loss:[0.5933]
Loss:[0.5839]
Loss:[0.5651]
Loss:[0.5454]
Loss:[0.5348]
Loss:[0.5093]
Loss:[0.4965]
Loss:[0.4813]
Loss:[0.4547]
Loss:[0.4390]
Loss:[0.4281]
Loss:[0.4100]
Loss:[0.3837]
Loss:[0.3710]
Loss:[0.3628]
Loss:[0.3432]
Loss:[0.3211]
Loss:[0.3128]
Loss:[0.3018]
Loss:[0.2856]
Loss:[0.2771]
Loss:[0.2618]
Loss:[0.2532]
Loss:[0.2448]
Loss:[0.2384]
Loss:[0.2307]
Loss:[0.2221]
Loss:[0.2156]
Loss:[0.2090]
Loss:[0.2099]
Loss:[0.1945]
Loss:[0.1887]
Loss:[0.1816]
Loss:[0.1814]
Loss:[0.1746]
Loss:[0.1718]
Loss:[0.1629]
Loss:[0.1627]
Loss:[0.1567]
Loss:[0.1621]
Loss:[0.1523]
Loss:[0.1520]
Loss:[0.1468]
Loss:[0.1384]
Loss:[0.1457]
Loss:[0.1381]
Loss:[0.1360]
Loss:[0.1310]
Loss:[0.1368]
Loss:[0.1353]
Loss:[0.1340]
Loss:[0.1253]
Loss:[0.1310]
Loss:[0.1250]
Loss:[0.1289]
Loss:[0.1262]
Loss:[0.1252]
Loss:[0.1330]
Loss:[0.1286]
Loss:[0.1185]
Loss:[0.1272]
Loss:[0.1242]
Loss:[0.1244]
Loss:[0.1216]
Loss:[0.1234]
Loss:[0.1140]
Loss:[0.1170]
Loss:[0.1308]
Loss:[0.1340]
Loss:[0.1111]
Loss:[0.1326]
Loss:[0.1241]
Loss:[0.1174]
Loss:[0.1406]
Loss:[0.1192]
Loss:[0.1137]
Loss:[0.1112]
Loss:[0.1052]
Loss:[0.1202]
Loss:[0.1077]
Loss:[0.1167]
Loss:[0.1048]
Loss:[0.1096]
Loss:[0.0999]
Loss:[0.1147]
Loss:[0.1031]
Loss:[0.1045]
Loss:[0.0982]
Loss:[0.0985]
Loss:[0.0989]
Loss:[0.0984]
Loss:[0.0939]
Loss:[0.1046]
Loss:[0.1013]
Loss:[0.0992]
Loss:[0.1032]
Loss:[0.0943]
Loss:[0.0932]
Loss:[0.0951]
Loss:[0.1015]
Loss:[0.0939]
Loss:[0.0902]
Loss:[0.0970]
Loss:[0.0963]
Loss:[0.0922]
Loss:[0.1025]
Loss:[0.1004]
Loss:[0.1029]
Loss:[0.0980]
Loss:[0.0945]
Loss:[0.0953]
Loss:[0.0985]
Loss:[0.1038]
Loss:[0.0880]
Loss:[0.0973]
Loss:[0.0975]
Loss:[0.0979]
Loss:[0.0928]
Loss:[0.1011]
Loss:[0.0926]
Loss:[0.0933]
Loss:[0.0948]
Loss:[0.0921]
Loss:[0.0912]
Loss:[0.1007]
Loss:[0.0919]
Loss:[0.0976]
Loss:[0.0966]
Loss:[0.0987]
Loss:[0.0928]
Loss:[0.0911]
Loss:[0.0918]
Loss:[0.0959]
Loss:[0.0870]
Loss:[0.0861]
Loss:[0.0953]
Loss:[0.0919]
Loss:[0.0948]
Loss:[0.0906]
Loss:[0.0957]
Loss:[0.0874]
Loss:[0.0919]
Loss:[0.0920]
Loss:[0.0907]
Loss:[0.0947]
Loss:[0.0884]
Loss:[0.0951]
Loss:[0.0911]
Loss:[0.0827]
Loss:[0.0870]
Loss:[0.0860]
Loss:[0.0886]
Loss:[0.0869]
Loss:[0.0943]
Loss:[0.0848]
Loss:[0.0973]
Loss:[0.0902]
Loss:[0.0929]
Loss:[0.0964]
Loss:[0.0847]
Loss:[0.0894]
Loss:[0.0864]
Loss:[0.0855]
Loss:[0.0952]
Loss:[0.0871]
Loss:[0.0929]
Loss:[0.0850]
Loss:[0.0834]
Loss:[0.0839]
Early stopping!
Loading 161th epoch
acc:[0.7250]
acc:[0.7250]
acc:[0.7260]
acc:[0.7270]
acc:[0.7260]
acc:[0.7270]
acc:[0.7250]
acc:[0.7260]
acc:[0.7270]
acc:[0.7300]
acc:[0.7240]
acc:[0.7260]
acc:[0.7250]
acc:[0.7250]
acc:[0.7250]
acc:[0.7240]
acc:[0.7250]
acc:[0.7260]
acc:[0.7250]
acc:[0.7260]
acc:[0.7260]
acc:[0.7250]
acc:[0.7230]
acc:[0.7280]
acc:[0.7270]
acc:[0.7220]
acc:[0.7250]
acc:[0.7240]
acc:[0.7230]
acc:[0.7240]
acc:[0.7260]
acc:[0.7250]
acc:[0.7270]
acc:[0.7250]
acc:[0.7250]
acc:[0.7250]
acc:[0.7250]
acc:[0.7240]
acc:[0.7250]
acc:[0.7260]
acc:[0.7270]
acc:[0.7250]
acc:[0.7240]
acc:[0.7260]
acc:[0.7240]
acc:[0.7230]
acc:[0.7240]
acc:[0.7270]
acc:[0.7270]
acc:[0.7260]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7254]
Mean:[72.5360]
Std :[0.1425]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.08, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6941]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6794]
Loss:[0.6811]
Loss:[0.6694]
Loss:[0.6649]
Loss:[0.6573]
Loss:[0.6438]
Loss:[0.6386]
Loss:[0.6211]
Loss:[0.6134]
Loss:[0.5933]
Loss:[0.5839]
Loss:[0.5651]
Loss:[0.5454]
Loss:[0.5348]
Loss:[0.5093]
Loss:[0.4965]
Loss:[0.4812]
Loss:[0.4547]
Loss:[0.4390]
Loss:[0.4280]
Loss:[0.4101]
Loss:[0.3837]
Loss:[0.3709]
Loss:[0.3627]
Loss:[0.3432]
Loss:[0.3211]
Loss:[0.3128]
Loss:[0.3017]
Loss:[0.2856]
Loss:[0.2771]
Loss:[0.2618]
Loss:[0.2532]
Loss:[0.2448]
Loss:[0.2384]
Loss:[0.2307]
Loss:[0.2221]
Loss:[0.2155]
Loss:[0.2090]
Loss:[0.2100]
Loss:[0.1945]
Loss:[0.1887]
Loss:[0.1815]
Loss:[0.1814]
Loss:[0.1746]
Loss:[0.1719]
Loss:[0.1629]
Loss:[0.1626]
Loss:[0.1567]
Loss:[0.1622]
Loss:[0.1523]
Loss:[0.1520]
Loss:[0.1467]
Loss:[0.1384]
Loss:[0.1457]
Loss:[0.1380]
Loss:[0.1360]
Loss:[0.1310]
Loss:[0.1368]
Loss:[0.1352]
Loss:[0.1340]
Loss:[0.1253]
Loss:[0.1310]
Loss:[0.1250]
Loss:[0.1289]
Loss:[0.1262]
Loss:[0.1252]
Loss:[0.1330]
Loss:[0.1286]
Loss:[0.1185]
Loss:[0.1271]
Loss:[0.1242]
Loss:[0.1244]
Loss:[0.1215]
Loss:[0.1234]
Loss:[0.1140]
Loss:[0.1170]
Loss:[0.1308]
Loss:[0.1342]
Loss:[0.1111]
Loss:[0.1328]
Loss:[0.1243]
Loss:[0.1174]
Loss:[0.1409]
Loss:[0.1192]
Loss:[0.1139]
Loss:[0.1112]
Loss:[0.1053]
Loss:[0.1202]
Loss:[0.1078]
Loss:[0.1168]
Loss:[0.1047]
Loss:[0.1097]
Loss:[0.0998]
Loss:[0.1147]
Loss:[0.1030]
Loss:[0.1046]
Loss:[0.0981]
Loss:[0.0985]
Loss:[0.0989]
Loss:[0.0985]
Loss:[0.0940]
Loss:[0.1045]
Loss:[0.1013]
Loss:[0.0992]
Loss:[0.1032]
Loss:[0.0943]
Loss:[0.0931]
Loss:[0.0951]
Loss:[0.1015]
Loss:[0.0939]
Loss:[0.0901]
Loss:[0.0970]
Loss:[0.0964]
Loss:[0.0921]
Loss:[0.1024]
Loss:[0.1004]
Loss:[0.1030]
Loss:[0.0980]
Loss:[0.0945]
Loss:[0.0953]
Loss:[0.0985]
Loss:[0.1038]
Loss:[0.0881]
Loss:[0.0973]
Loss:[0.0975]
Loss:[0.0979]
Loss:[0.0927]
Loss:[0.1011]
Loss:[0.0926]
Loss:[0.0933]
Loss:[0.0948]
Loss:[0.0921]
Loss:[0.0912]
Loss:[0.1007]
Loss:[0.0919]
Loss:[0.0976]
Loss:[0.0966]
Loss:[0.0988]
Loss:[0.0928]
Loss:[0.0911]
Loss:[0.0919]
Loss:[0.0958]
Loss:[0.0869]
Loss:[0.0861]
Loss:[0.0953]
Loss:[0.0919]
Loss:[0.0948]
Loss:[0.0905]
Loss:[0.0957]
Loss:[0.0874]
Loss:[0.0919]
Loss:[0.0920]
Loss:[0.0907]
Loss:[0.0947]
Loss:[0.0885]
Loss:[0.0951]
Loss:[0.0910]
Loss:[0.0828]
Loss:[0.0871]
Loss:[0.0860]
Loss:[0.0887]
Loss:[0.0869]
Loss:[0.0944]
Loss:[0.0849]
Loss:[0.0974]
Loss:[0.0902]
Loss:[0.0929]
Loss:[0.0965]
Loss:[0.0848]
Loss:[0.0895]
Loss:[0.0864]
Loss:[0.0854]
Loss:[0.0952]
Loss:[0.0872]
Loss:[0.0928]
Loss:[0.0850]
Loss:[0.0834]
Loss:[0.0839]
Early stopping!
Loading 161th epoch
acc:[0.7250]
acc:[0.7250]
acc:[0.7260]
acc:[0.7230]
acc:[0.7250]
acc:[0.7260]
acc:[0.7260]
acc:[0.7260]
acc:[0.7260]
acc:[0.7270]
acc:[0.7240]
acc:[0.7260]
acc:[0.7240]
acc:[0.7280]
acc:[0.7270]
acc:[0.7250]
acc:[0.7240]
acc:[0.7280]
acc:[0.7240]
acc:[0.7250]
acc:[0.7290]
acc:[0.7240]
acc:[0.7240]
acc:[0.7280]
acc:[0.7250]
acc:[0.7240]
acc:[0.7250]
acc:[0.7240]
acc:[0.7270]
acc:[0.7260]
acc:[0.7260]
acc:[0.7250]
acc:[0.7280]
acc:[0.7240]
acc:[0.7260]
acc:[0.7250]
acc:[0.7230]
acc:[0.7250]
acc:[0.7260]
acc:[0.7250]
acc:[0.7260]
acc:[0.7240]
acc:[0.7290]
acc:[0.7260]
acc:[0.7260]
acc:[0.7230]
acc:[0.7250]
acc:[0.7260]
acc:[0.7250]
acc:[0.7280]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7255]
Mean:[72.5540]
Std :[0.1501]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.1, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6941]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6794]
Loss:[0.6811]
Loss:[0.6694]
Loss:[0.6649]
Loss:[0.6573]
Loss:[0.6438]
Loss:[0.6386]
Loss:[0.6211]
Loss:[0.6134]
Loss:[0.5933]
Loss:[0.5839]
Loss:[0.5651]
Loss:[0.5454]
Loss:[0.5348]
Loss:[0.5093]
Loss:[0.4965]
Loss:[0.4813]
Loss:[0.4547]
Loss:[0.4391]
Loss:[0.4281]
Loss:[0.4100]
Loss:[0.3837]
Loss:[0.3710]
Loss:[0.3628]
Loss:[0.3432]
Loss:[0.3211]
Loss:[0.3128]
Loss:[0.3018]
Loss:[0.2856]
Loss:[0.2771]
Loss:[0.2618]
Loss:[0.2532]
Loss:[0.2448]
Loss:[0.2384]
Loss:[0.2308]
Loss:[0.2221]
Loss:[0.2155]
Loss:[0.2089]
Loss:[0.2099]
Loss:[0.1946]
Loss:[0.1887]
Loss:[0.1814]
Loss:[0.1812]
Loss:[0.1746]
Loss:[0.1721]
Loss:[0.1631]
Loss:[0.1626]
Loss:[0.1566]
Loss:[0.1622]
Loss:[0.1523]
Loss:[0.1518]
Loss:[0.1466]
Loss:[0.1383]
Loss:[0.1457]
Loss:[0.1378]
Loss:[0.1358]
Loss:[0.1310]
Loss:[0.1367]
Loss:[0.1349]
Loss:[0.1338]
Loss:[0.1256]
Loss:[0.1311]
Loss:[0.1244]
Loss:[0.1284]
Loss:[0.1266]
Loss:[0.1249]
Loss:[0.1318]
Loss:[0.1290]
Loss:[0.1191]
Loss:[0.1261]
Loss:[0.1247]
Loss:[0.1267]
Loss:[0.1214]
Loss:[0.1244]
Loss:[0.1170]
Loss:[0.1168]
Loss:[0.1342]
Loss:[0.1397]
Loss:[0.1115]
Loss:[0.1417]
Loss:[0.1294]
Loss:[0.1245]
Loss:[0.1437]
Loss:[0.1114]
Loss:[0.1170]
Loss:[0.1036]
Loss:[0.1180]
Loss:[0.1192]
Loss:[0.1168]
Loss:[0.1117]
Loss:[0.1067]
Loss:[0.1103]
Loss:[0.1005]
Loss:[0.1167]
Loss:[0.1030]
Loss:[0.1082]
Loss:[0.0955]
Loss:[0.1017]
Loss:[0.0974]
Loss:[0.1015]
Loss:[0.0913]
Loss:[0.1050]
Loss:[0.1011]
Loss:[0.1000]
Loss:[0.1031]
Loss:[0.0956]
Loss:[0.0932]
Loss:[0.0971]
Loss:[0.1013]
Loss:[0.0964]
Loss:[0.0901]
Loss:[0.0993]
Loss:[0.0960]
Loss:[0.0955]
Loss:[0.1034]
Loss:[0.1022]
Loss:[0.1036]
Loss:[0.0986]
Loss:[0.0959]
Loss:[0.0967]
Loss:[0.1000]
Loss:[0.1038]
Loss:[0.0896]
Loss:[0.0972]
Loss:[0.0990]
Loss:[0.0975]
Loss:[0.0932]
Loss:[0.1007]
Loss:[0.0939]
Loss:[0.0933]
Loss:[0.0958]
Loss:[0.0929]
Loss:[0.0909]
Loss:[0.1014]
Loss:[0.0898]
Loss:[0.0968]
Loss:[0.0960]
Loss:[0.0974]
Loss:[0.0927]
Loss:[0.0903]
Loss:[0.0919]
Loss:[0.0950]
Loss:[0.0878]
Loss:[0.0855]
Loss:[0.0958]
Loss:[0.0915]
Loss:[0.0949]
Loss:[0.0907]
Loss:[0.0955]
Loss:[0.0879]
Loss:[0.0913]
Loss:[0.0920]
Loss:[0.0901]
Loss:[0.0938]
Loss:[0.0881]
Loss:[0.0937]
Loss:[0.0918]
Loss:[0.0819]
Loss:[0.0880]
Loss:[0.0848]
Loss:[0.0887]
Loss:[0.0865]
Loss:[0.0933]
Loss:[0.0858]
Loss:[0.0950]
Loss:[0.0891]
Loss:[0.0924]
Loss:[0.0953]
Loss:[0.0844]
Loss:[0.0892]
Loss:[0.0858]
Loss:[0.0856]
Loss:[0.0950]
Loss:[0.0869]
Loss:[0.0920]
Loss:[0.0844]
Loss:[0.0828]
Loss:[0.0829]
Early stopping!
Loading 161th epoch
acc:[0.7260]
acc:[0.7260]
acc:[0.7270]
acc:[0.7270]
acc:[0.7270]
acc:[0.7250]
acc:[0.7250]
acc:[0.7260]
acc:[0.7280]
acc:[0.7290]
acc:[0.7250]
acc:[0.7270]
acc:[0.7250]
acc:[0.7260]
acc:[0.7240]
acc:[0.7250]
acc:[0.7240]
acc:[0.7270]
acc:[0.7250]
acc:[0.7280]
acc:[0.7260]
acc:[0.7270]
acc:[0.7260]
acc:[0.7250]
acc:[0.7260]
acc:[0.7260]
acc:[0.7250]
acc:[0.7250]
acc:[0.7260]
acc:[0.7240]
acc:[0.7260]
acc:[0.7260]
acc:[0.7290]
acc:[0.7260]
acc:[0.7250]
acc:[0.7240]
acc:[0.7260]
acc:[0.7280]
acc:[0.7260]
acc:[0.7270]
acc:[0.7260]
acc:[0.7250]
acc:[0.7260]
acc:[0.7270]
acc:[0.7260]
acc:[0.7240]
acc:[0.7260]
acc:[0.7270]
acc:[0.7290]
acc:[0.7270]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7261]
Mean:[72.6080]
Std :[0.1275]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.2, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6941]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6794]
Loss:[0.6811]
Loss:[0.6694]
Loss:[0.6649]
Loss:[0.6573]
Loss:[0.6438]
Loss:[0.6386]
Loss:[0.6211]
Loss:[0.6133]
Loss:[0.5933]
Loss:[0.5839]
Loss:[0.5651]
Loss:[0.5454]
Loss:[0.5347]
Loss:[0.5093]
Loss:[0.4965]
Loss:[0.4813]
Loss:[0.4547]
Loss:[0.4390]
Loss:[0.4280]
Loss:[0.4101]
Loss:[0.3837]
Loss:[0.3709]
Loss:[0.3627]
Loss:[0.3433]
Loss:[0.3212]
Loss:[0.3127]
Loss:[0.3017]
Loss:[0.2856]
Loss:[0.2772]
Loss:[0.2619]
Loss:[0.2532]
Loss:[0.2447]
Loss:[0.2384]
Loss:[0.2308]
Loss:[0.2221]
Loss:[0.2155]
Loss:[0.2088]
Loss:[0.2098]
Loss:[0.1946]
Loss:[0.1887]
Loss:[0.1814]
Loss:[0.1812]
Loss:[0.1746]
Loss:[0.1721]
Loss:[0.1631]
Loss:[0.1626]
Loss:[0.1565]
Loss:[0.1622]
Loss:[0.1523]
Loss:[0.1518]
Loss:[0.1465]
Loss:[0.1384]
Loss:[0.1457]
Loss:[0.1377]
Loss:[0.1356]
Loss:[0.1310]
Loss:[0.1367]
Loss:[0.1347]
Loss:[0.1337]
Loss:[0.1257]
Loss:[0.1312]
Loss:[0.1241]
Loss:[0.1280]
Loss:[0.1270]
Loss:[0.1247]
Loss:[0.1310]
Loss:[0.1294]
Loss:[0.1197]
Loss:[0.1256]
Loss:[0.1251]
Loss:[0.1284]
Loss:[0.1213]
Loss:[0.1251]
Loss:[0.1191]
Loss:[0.1168]
Loss:[0.1369]
Loss:[0.1434]
Loss:[0.1121]
Loss:[0.1481]
Loss:[0.1312]
Loss:[0.1306]
Loss:[0.1406]
Loss:[0.1074]
Loss:[0.1166]
Loss:[0.1029]
Loss:[0.1218]
Loss:[0.1133]
Loss:[0.1188]
Loss:[0.1057]
Loss:[0.1133]
Loss:[0.1081]
Loss:[0.1050]
Loss:[0.1139]
Loss:[0.1094]
Loss:[0.1085]
Loss:[0.0997]
Loss:[0.1005]
Loss:[0.1020]
Loss:[0.1019]
Loss:[0.0944]
Loss:[0.1044]
Loss:[0.1048]
Loss:[0.1009]
Loss:[0.1058]
Loss:[0.0955]
Loss:[0.0952]
Loss:[0.0974]
Loss:[0.1031]
Loss:[0.0964]
Loss:[0.0912]
Loss:[0.0999]
Loss:[0.0976]
Loss:[0.0962]
Loss:[0.1016]
Loss:[0.1028]
Loss:[0.1020]
Loss:[0.1006]
Loss:[0.0941]
Loss:[0.0983]
Loss:[0.0981]
Loss:[0.1064]
Loss:[0.0880]
Loss:[0.0987]
Loss:[0.0971]
Loss:[0.0987]
Loss:[0.0927]
Loss:[0.1020]
Loss:[0.0927]
Loss:[0.0930]
Loss:[0.0955]
Loss:[0.0912]
Loss:[0.0912]
Loss:[0.0994]
Loss:[0.0895]
Loss:[0.0966]
Loss:[0.0961]
Loss:[0.0974]
Loss:[0.0924]
Loss:[0.0901]
Loss:[0.0914]
Loss:[0.0945]
Loss:[0.0869]
Loss:[0.0853]
Loss:[0.0950]
Loss:[0.0915]
Loss:[0.0941]
Loss:[0.0900]
Loss:[0.0952]
Loss:[0.0870]
Loss:[0.0910]
Loss:[0.0913]
Loss:[0.0897]
Loss:[0.0935]
Loss:[0.0878]
Loss:[0.0935]
Loss:[0.0916]
Loss:[0.0820]
Loss:[0.0878]
Loss:[0.0848]
Loss:[0.0887]
Loss:[0.0860]
Loss:[0.0933]
Loss:[0.0852]
Loss:[0.0946]
Loss:[0.0887]
Loss:[0.0923]
Loss:[0.0953]
Loss:[0.0847]
Loss:[0.0891]
Loss:[0.0859]
Loss:[0.0855]
Loss:[0.0947]
Loss:[0.0871]
Loss:[0.0917]
Loss:[0.0845]
Loss:[0.0827]
Loss:[0.0831]
Early stopping!
Loading 161th epoch
acc:[0.7260]
acc:[0.7250]
acc:[0.7270]
acc:[0.7260]
acc:[0.7280]
acc:[0.7260]
acc:[0.7270]
acc:[0.7280]
acc:[0.7290]
acc:[0.7290]
acc:[0.7280]
acc:[0.7260]
acc:[0.7260]
acc:[0.7260]
acc:[0.7250]
acc:[0.7250]
acc:[0.7250]
acc:[0.7290]
acc:[0.7250]
acc:[0.7270]
acc:[0.7280]
acc:[0.7260]
acc:[0.7250]
acc:[0.7290]
acc:[0.7250]
acc:[0.7260]
acc:[0.7270]
acc:[0.7250]
acc:[0.7260]
acc:[0.7230]
acc:[0.7270]
acc:[0.7260]
acc:[0.7290]
acc:[0.7250]
acc:[0.7250]
acc:[0.7240]
acc:[0.7270]
acc:[0.7260]
acc:[0.7270]
acc:[0.7260]
acc:[0.7270]
acc:[0.7260]
acc:[0.7260]
acc:[0.7250]
acc:[0.7270]
acc:[0.7230]
acc:[0.7250]
acc:[0.7260]
acc:[0.7270]
acc:[0.7270]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7263]
Mean:[72.6280]
Std :[0.1443]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.3, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6941]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6794]
Loss:[0.6811]
Loss:[0.6694]
Loss:[0.6649]
Loss:[0.6573]
Loss:[0.6438]
Loss:[0.6386]
Loss:[0.6211]
Loss:[0.6134]
Loss:[0.5933]
Loss:[0.5840]
Loss:[0.5651]
Loss:[0.5454]
Loss:[0.5348]
Loss:[0.5093]
Loss:[0.4966]
Loss:[0.4812]
Loss:[0.4547]
Loss:[0.4392]
Loss:[0.4281]
Loss:[0.4100]
Loss:[0.3837]
Loss:[0.3710]
Loss:[0.3628]
Loss:[0.3432]
Loss:[0.3211]
Loss:[0.3128]
Loss:[0.3018]
Loss:[0.2856]
Loss:[0.2771]
Loss:[0.2618]
Loss:[0.2533]
Loss:[0.2449]
Loss:[0.2385]
Loss:[0.2307]
Loss:[0.2221]
Loss:[0.2156]
Loss:[0.2091]
Loss:[0.2100]
Loss:[0.1945]
Loss:[0.1887]
Loss:[0.1817]
Loss:[0.1816]
Loss:[0.1747]
Loss:[0.1717]
Loss:[0.1627]
Loss:[0.1628]
Loss:[0.1569]
Loss:[0.1623]
Loss:[0.1523]
Loss:[0.1523]
Loss:[0.1470]
Loss:[0.1385]
Loss:[0.1458]
Loss:[0.1385]
Loss:[0.1364]
Loss:[0.1308]
Loss:[0.1371]
Loss:[0.1362]
Loss:[0.1344]
Loss:[0.1246]
Loss:[0.1310]
Loss:[0.1263]
Loss:[0.1298]
Loss:[0.1252]
Loss:[0.1265]
Loss:[0.1359]
Loss:[0.1274]
Loss:[0.1186]
Loss:[0.1306]
Loss:[0.1223]
Loss:[0.1207]
Loss:[0.1233]
Loss:[0.1205]
Loss:[0.1078]
Loss:[0.1184]
Loss:[0.1235]
Loss:[0.1211]
Loss:[0.1114]
Loss:[0.1173]
Loss:[0.1095]
Loss:[0.1106]
Loss:[0.1161]
Loss:[0.1132]
Loss:[0.1018]
Loss:[0.1073]
Loss:[0.1046]
Loss:[0.1103]
Loss:[0.1109]
Loss:[0.1057]
Loss:[0.1048]
Loss:[0.1064]
Loss:[0.0997]
Loss:[0.1150]
Loss:[0.1076]
Loss:[0.1046]
Loss:[0.0958]
Loss:[0.1009]
Loss:[0.0983]
Loss:[0.1003]
Loss:[0.0933]
Loss:[0.1033]
Loss:[0.1016]
Loss:[0.0992]
Loss:[0.1044]
Loss:[0.0942]
Loss:[0.0944]
Loss:[0.0963]
Loss:[0.1015]
Loss:[0.0947]
Loss:[0.0909]
Loss:[0.0979]
Loss:[0.0972]
Loss:[0.0924]
Loss:[0.1044]
Loss:[0.1008]
Loss:[0.1036]
Loss:[0.0990]
Loss:[0.0945]
Loss:[0.0961]
Loss:[0.0983]
Loss:[0.1041]
Loss:[0.0884]
Loss:[0.0973]
Loss:[0.0974]
Loss:[0.0982]
Loss:[0.0923]
Loss:[0.1030]
Loss:[0.0946]
Loss:[0.0944]
Loss:[0.0953]
Loss:[0.0938]
Loss:[0.0942]
Loss:[0.1007]
Loss:[0.0975]
Loss:[0.1086]
Loss:[0.0968]
Loss:[0.1113]
Loss:[0.1084]
Loss:[0.0933]
Loss:[0.1157]
Loss:[0.1173]
Loss:[0.0937]
Early stopping!
Loading 126th epoch
acc:[0.7280]
acc:[0.7290]
acc:[0.7280]
acc:[0.7270]
acc:[0.7280]
acc:[0.7280]
acc:[0.7290]
acc:[0.7280]
acc:[0.7290]
acc:[0.7320]
acc:[0.7270]
acc:[0.7300]
acc:[0.7250]
acc:[0.7300]
acc:[0.7260]
acc:[0.7280]
acc:[0.7300]
acc:[0.7250]
acc:[0.7280]
acc:[0.7280]
acc:[0.7290]
acc:[0.7280]
acc:[0.7270]
acc:[0.7250]
acc:[0.7260]
acc:[0.7260]
acc:[0.7270]
acc:[0.7260]
acc:[0.7290]
acc:[0.7260]
acc:[0.7260]
acc:[0.7250]
acc:[0.7300]
acc:[0.7260]
acc:[0.7270]
acc:[0.7310]
acc:[0.7280]
acc:[0.7260]
acc:[0.7300]
acc:[0.7270]
acc:[0.7280]
acc:[0.7280]
acc:[0.7280]
acc:[0.7260]
acc:[0.7270]
acc:[0.7260]
acc:[0.7280]
acc:[0.7270]
acc:[0.7310]
acc:[0.7270]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7277]
Mean:[72.7680]
Std :[0.1684]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.4, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6941]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6794]
Loss:[0.6811]
Loss:[0.6694]
Loss:[0.6649]
Loss:[0.6573]
Loss:[0.6438]
Loss:[0.6386]
Loss:[0.6211]
Loss:[0.6134]
Loss:[0.5933]
Loss:[0.5839]
Loss:[0.5651]
Loss:[0.5454]
Loss:[0.5348]
Loss:[0.5093]
Loss:[0.4965]
Loss:[0.4813]
Loss:[0.4547]
Loss:[0.4391]
Loss:[0.4281]
Loss:[0.4100]
Loss:[0.3837]
Loss:[0.3710]
Loss:[0.3628]
Loss:[0.3433]
Loss:[0.3211]
Loss:[0.3129]
Loss:[0.3019]
Loss:[0.2856]
Loss:[0.2770]
Loss:[0.2618]
Loss:[0.2533]
Loss:[0.2449]
Loss:[0.2384]
Loss:[0.2307]
Loss:[0.2221]
Loss:[0.2157]
Loss:[0.2092]
Loss:[0.2100]
Loss:[0.1945]
Loss:[0.1887]
Loss:[0.1817]
Loss:[0.1817]
Loss:[0.1748]
Loss:[0.1717]
Loss:[0.1627]
Loss:[0.1628]
Loss:[0.1570]
Loss:[0.1623]
Loss:[0.1523]
Loss:[0.1523]
Loss:[0.1471]
Loss:[0.1385]
Loss:[0.1458]
Loss:[0.1386]
Loss:[0.1366]
Loss:[0.1309]
Loss:[0.1372]
Loss:[0.1364]
Loss:[0.1343]
Loss:[0.1245]
Loss:[0.1310]
Loss:[0.1264]
Loss:[0.1296]
Loss:[0.1250]
Loss:[0.1268]
Loss:[0.1362]
Loss:[0.1272]
Loss:[0.1187]
Loss:[0.1310]
Loss:[0.1220]
Loss:[0.1206]
Loss:[0.1237]
Loss:[0.1202]
Loss:[0.1074]
Loss:[0.1188]
Loss:[0.1228]
Loss:[0.1200]
Loss:[0.1116]
Loss:[0.1165]
Loss:[0.1085]
Loss:[0.1107]
Loss:[0.1144]
Loss:[0.1115]
Loss:[0.1017]
Loss:[0.1056]
Loss:[0.1037]
Loss:[0.1101]
Loss:[0.1091]
Loss:[0.1052]
Loss:[0.1044]
Loss:[0.1050]
Loss:[0.0996]
Loss:[0.1140]
Loss:[0.1067]
Loss:[0.1053]
Loss:[0.0954]
Loss:[0.1011]
Loss:[0.0987]
Loss:[0.0996]
Loss:[0.0944]
Loss:[0.1047]
Loss:[0.1016]
Loss:[0.1000]
Loss:[0.1049]
Loss:[0.0942]
Loss:[0.0946]
Loss:[0.0966]
Loss:[0.1017]
Loss:[0.0957]
Loss:[0.0913]
Loss:[0.0983]
Loss:[0.0983]
Loss:[0.0921]
Loss:[0.1049]
Loss:[0.1024]
Loss:[0.1036]
Loss:[0.0992]
Loss:[0.0952]
Loss:[0.0961]
Loss:[0.0983]
Loss:[0.1048]
Loss:[0.0880]
Loss:[0.0972]
Loss:[0.0978]
Loss:[0.0979]
Loss:[0.0923]
Loss:[0.1026]
Loss:[0.0939]
Loss:[0.0947]
Loss:[0.0952]
Loss:[0.0926]
Loss:[0.0941]
Loss:[0.1006]
Loss:[0.0959]
Loss:[0.1082]
Loss:[0.0968]
Loss:[0.1081]
Loss:[0.1062]
Loss:[0.0917]
Loss:[0.1112]
Loss:[0.1173]
Loss:[0.0903]
Early stopping!
Loading 126th epoch
acc:[0.7280]
acc:[0.7290]
acc:[0.7260]
acc:[0.7250]
acc:[0.7270]
acc:[0.7230]
acc:[0.7270]
acc:[0.7260]
acc:[0.7270]
acc:[0.7290]
acc:[0.7250]
acc:[0.7270]
acc:[0.7260]
acc:[0.7280]
acc:[0.7240]
acc:[0.7240]
acc:[0.7280]
acc:[0.7250]
acc:[0.7270]
acc:[0.7250]
acc:[0.7240]
acc:[0.7240]
acc:[0.7250]
acc:[0.7250]
acc:[0.7270]
acc:[0.7270]
acc:[0.7260]
acc:[0.7250]
acc:[0.7270]
acc:[0.7250]
acc:[0.7280]
acc:[0.7250]
acc:[0.7270]
acc:[0.7260]
acc:[0.7250]
acc:[0.7270]
acc:[0.7220]
acc:[0.7260]
acc:[0.7280]
acc:[0.7270]
acc:[0.7220]
acc:[0.7240]
acc:[0.7260]
acc:[0.7260]
acc:[0.7280]
acc:[0.7240]
acc:[0.7250]
acc:[0.7230]
acc:[0.7270]
acc:[0.7260]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7259]
Mean:[72.5860]
Std :[0.1678]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.5, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6941]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6794]
Loss:[0.6811]
Loss:[0.6694]
Loss:[0.6649]
Loss:[0.6573]
Loss:[0.6438]
Loss:[0.6386]
Loss:[0.6211]
Loss:[0.6134]
Loss:[0.5933]
Loss:[0.5839]
Loss:[0.5651]
Loss:[0.5454]
Loss:[0.5347]
Loss:[0.5093]
Loss:[0.4965]
Loss:[0.4812]
Loss:[0.4547]
Loss:[0.4390]
Loss:[0.4280]
Loss:[0.4101]
Loss:[0.3837]
Loss:[0.3708]
Loss:[0.3627]
Loss:[0.3434]
Loss:[0.3212]
Loss:[0.3127]
Loss:[0.3016]
Loss:[0.2856]
Loss:[0.2772]
Loss:[0.2619]
Loss:[0.2532]
Loss:[0.2447]
Loss:[0.2384]
Loss:[0.2308]
Loss:[0.2221]
Loss:[0.2155]
Loss:[0.2089]
Loss:[0.2099]
Loss:[0.1945]
Loss:[0.1887]
Loss:[0.1815]
Loss:[0.1814]
Loss:[0.1746]
Loss:[0.1720]
Loss:[0.1630]
Loss:[0.1626]
Loss:[0.1567]
Loss:[0.1622]
Loss:[0.1523]
Loss:[0.1519]
Loss:[0.1467]
Loss:[0.1384]
Loss:[0.1458]
Loss:[0.1380]
Loss:[0.1360]
Loss:[0.1309]
Loss:[0.1368]
Loss:[0.1352]
Loss:[0.1340]
Loss:[0.1254]
Loss:[0.1310]
Loss:[0.1249]
Loss:[0.1289]
Loss:[0.1263]
Loss:[0.1252]
Loss:[0.1328]
Loss:[0.1287]
Loss:[0.1185]
Loss:[0.1269]
Loss:[0.1243]
Loss:[0.1249]
Loss:[0.1215]
Loss:[0.1236]
Loss:[0.1146]
Loss:[0.1169]
Loss:[0.1315]
Loss:[0.1350]
Loss:[0.1112]
Loss:[0.1346]
Loss:[0.1253]
Loss:[0.1186]
Loss:[0.1419]
Loss:[0.1177]
Loss:[0.1148]
Loss:[0.1093]
Loss:[0.1077]
Loss:[0.1206]
Loss:[0.1094]
Loss:[0.1168]
Loss:[0.1037]
Loss:[0.1100]
Loss:[0.0984]
Loss:[0.1161]
Loss:[0.1022]
Loss:[0.1057]
Loss:[0.0967]
Loss:[0.0991]
Loss:[0.0985]
Loss:[0.0990]
Loss:[0.0938]
Loss:[0.1038]
Loss:[0.1014]
Loss:[0.0987]
Loss:[0.1035]
Loss:[0.0940]
Loss:[0.0935]
Loss:[0.0951]
Loss:[0.1017]
Loss:[0.0939]
Loss:[0.0901]
Loss:[0.0971]
Loss:[0.0963]
Loss:[0.0920]
Loss:[0.1027]
Loss:[0.1003]
Loss:[0.1031]
Loss:[0.0978]
Loss:[0.0947]
Loss:[0.0953]
Loss:[0.0985]
Loss:[0.1037]
Loss:[0.0881]
Loss:[0.0974]
Loss:[0.0974]
Loss:[0.0979]
Loss:[0.0928]
Loss:[0.1010]
Loss:[0.0925]
Loss:[0.0933]
Loss:[0.0949]
Loss:[0.0921]
Loss:[0.0912]
Loss:[0.1007]
Loss:[0.0919]
Loss:[0.0975]
Loss:[0.0967]
Loss:[0.0987]
Loss:[0.0929]
Loss:[0.0909]
Loss:[0.0918]
Loss:[0.0958]
Loss:[0.0870]
Loss:[0.0861]
Loss:[0.0953]
Loss:[0.0920]
Loss:[0.0946]
Loss:[0.0907]
Loss:[0.0957]
Loss:[0.0875]
Loss:[0.0920]
Loss:[0.0921]
Loss:[0.0910]
Loss:[0.0947]
Loss:[0.0885]
Loss:[0.0950]
Loss:[0.0911]
Loss:[0.0829]
Loss:[0.0871]
Loss:[0.0862]
Loss:[0.0886]
Loss:[0.0870]
Loss:[0.0942]
Loss:[0.0850]
Loss:[0.0976]
Loss:[0.0899]
Loss:[0.0932]
Loss:[0.0964]
Loss:[0.0846]
Loss:[0.0895]
Loss:[0.0862]
Loss:[0.0855]
Loss:[0.0951]
Loss:[0.0873]
Loss:[0.0929]
Loss:[0.0847]
Loss:[0.0836]
Loss:[0.0837]
Early stopping!
Loading 161th epoch
acc:[0.7280]
acc:[0.7260]
acc:[0.7280]
acc:[0.7270]
acc:[0.7270]
acc:[0.7270]
acc:[0.7280]
acc:[0.7270]
acc:[0.7300]
acc:[0.7300]
acc:[0.7300]
acc:[0.7270]
acc:[0.7260]
acc:[0.7280]
acc:[0.7250]
acc:[0.7300]
acc:[0.7270]
acc:[0.7290]
acc:[0.7250]
acc:[0.7280]
acc:[0.7280]
acc:[0.7260]
acc:[0.7250]
acc:[0.7290]
acc:[0.7280]
acc:[0.7260]
acc:[0.7260]
acc:[0.7270]
acc:[0.7260]
acc:[0.7260]
acc:[0.7250]
acc:[0.7250]
acc:[0.7310]
acc:[0.7260]
acc:[0.7250]
acc:[0.7250]
acc:[0.7260]
acc:[0.7260]
acc:[0.7270]
acc:[0.7290]
acc:[0.7290]
acc:[0.7270]
acc:[0.7280]
acc:[0.7280]
acc:[0.7250]
acc:[0.7240]
acc:[0.7260]
acc:[0.7260]
acc:[0.7280]
acc:[0.7290]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7271]
Mean:[72.7100]
Std :[0.1657]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.6, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6941]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6794]
Loss:[0.6811]
Loss:[0.6694]
Loss:[0.6649]
Loss:[0.6573]
Loss:[0.6438]
Loss:[0.6386]
Loss:[0.6211]
Loss:[0.6134]
Loss:[0.5933]
Loss:[0.5840]
Loss:[0.5651]
Loss:[0.5454]
Loss:[0.5348]
Loss:[0.5092]
Loss:[0.4966]
Loss:[0.4812]
Loss:[0.4547]
Loss:[0.4392]
Loss:[0.4281]
Loss:[0.4100]
Loss:[0.3837]
Loss:[0.3710]
Loss:[0.3628]
Loss:[0.3433]
Loss:[0.3211]
Loss:[0.3128]
Loss:[0.3018]
Loss:[0.2856]
Loss:[0.2770]
Loss:[0.2618]
Loss:[0.2533]
Loss:[0.2449]
Loss:[0.2384]
Loss:[0.2307]
Loss:[0.2221]
Loss:[0.2156]
Loss:[0.2091]
Loss:[0.2100]
Loss:[0.1944]
Loss:[0.1886]
Loss:[0.1817]
Loss:[0.1817]
Loss:[0.1747]
Loss:[0.1717]
Loss:[0.1627]
Loss:[0.1628]
Loss:[0.1570]
Loss:[0.1622]
Loss:[0.1523]
Loss:[0.1523]
Loss:[0.1470]
Loss:[0.1384]
Loss:[0.1458]
Loss:[0.1385]
Loss:[0.1365]
Loss:[0.1309]
Loss:[0.1371]
Loss:[0.1363]
Loss:[0.1343]
Loss:[0.1246]
Loss:[0.1310]
Loss:[0.1263]
Loss:[0.1297]
Loss:[0.1251]
Loss:[0.1265]
Loss:[0.1359]
Loss:[0.1274]
Loss:[0.1186]
Loss:[0.1306]
Loss:[0.1222]
Loss:[0.1207]
Loss:[0.1234]
Loss:[0.1204]
Loss:[0.1077]
Loss:[0.1184]
Loss:[0.1233]
Loss:[0.1208]
Loss:[0.1115]
Loss:[0.1172]
Loss:[0.1093]
Loss:[0.1107]
Loss:[0.1158]
Loss:[0.1130]
Loss:[0.1018]
Loss:[0.1070]
Loss:[0.1044]
Loss:[0.1103]
Loss:[0.1105]
Loss:[0.1057]
Loss:[0.1047]
Loss:[0.1062]
Loss:[0.0997]
Loss:[0.1148]
Loss:[0.1077]
Loss:[0.1049]
Loss:[0.0957]
Loss:[0.1012]
Loss:[0.0983]
Loss:[0.1002]
Loss:[0.0937]
Loss:[0.1035]
Loss:[0.1015]
Loss:[0.0994]
Loss:[0.1044]
Loss:[0.0942]
Loss:[0.0943]
Loss:[0.0963]
Loss:[0.1015]
Loss:[0.0948]
Loss:[0.0909]
Loss:[0.0980]
Loss:[0.0972]
Loss:[0.0923]
Loss:[0.1046]
Loss:[0.1010]
Loss:[0.1036]
Loss:[0.0992]
Loss:[0.0945]
Loss:[0.0961]
Loss:[0.0985]
Loss:[0.1041]
Loss:[0.0885]
Loss:[0.0973]
Loss:[0.0974]
Loss:[0.0982]
Loss:[0.0923]
Loss:[0.1030]
Loss:[0.0947]
Loss:[0.0947]
Loss:[0.0953]
Loss:[0.0940]
Loss:[0.0945]
Loss:[0.1007]
Loss:[0.0982]
Loss:[0.1095]
Loss:[0.0969]
Loss:[0.1129]
Loss:[0.1105]
Loss:[0.0940]
Loss:[0.1187]
Loss:[0.1195]
Loss:[0.0958]
Early stopping!
Loading 126th epoch
acc:[0.7300]
acc:[0.7290]
acc:[0.7280]
acc:[0.7270]
acc:[0.7270]
acc:[0.7280]
acc:[0.7290]
acc:[0.7290]
acc:[0.7280]
acc:[0.7300]
acc:[0.7280]
acc:[0.7290]
acc:[0.7270]
acc:[0.7290]
acc:[0.7260]
acc:[0.7270]
acc:[0.7300]
acc:[0.7280]
acc:[0.7270]
acc:[0.7280]
acc:[0.7280]
acc:[0.7290]
acc:[0.7280]
acc:[0.7260]
acc:[0.7270]
acc:[0.7280]
acc:[0.7270]
acc:[0.7260]
acc:[0.7280]
acc:[0.7260]
acc:[0.7270]
acc:[0.7250]
acc:[0.7280]
acc:[0.7280]
acc:[0.7290]
acc:[0.7280]
acc:[0.7260]
acc:[0.7280]
acc:[0.7310]
acc:[0.7280]
acc:[0.7280]
acc:[0.7290]
acc:[0.7270]
acc:[0.7260]
acc:[0.7270]
acc:[0.7270]
acc:[0.7290]
acc:[0.7250]
acc:[0.7270]
acc:[0.7260]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7277]
Mean:[72.7720]
Std :[0.1310]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.02, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6940]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6793]
Loss:[0.6811]
Loss:[0.6692]
Loss:[0.6649]
Loss:[0.6572]
Loss:[0.6437]
Loss:[0.6386]
Loss:[0.6210]
Loss:[0.6135]
Loss:[0.5932]
Loss:[0.5843]
Loss:[0.5650]
Loss:[0.5459]
Loss:[0.5347]
Loss:[0.5091]
Loss:[0.4974]
Loss:[0.4810]
Loss:[0.4545]
Loss:[0.4400]
Loss:[0.4283]
Loss:[0.4092]
Loss:[0.3837]
Loss:[0.3718]
Loss:[0.3632]
Loss:[0.3429]
Loss:[0.3208]
Loss:[0.3132]
Loss:[0.3023]
Loss:[0.2856]
Loss:[0.2767]
Loss:[0.2617]
Loss:[0.2535]
Loss:[0.2453]
Loss:[0.2386]
Loss:[0.2302]
Loss:[0.2218]
Loss:[0.2162]
Loss:[0.2103]
Loss:[0.2103]
Loss:[0.1940]
Loss:[0.1885]
Loss:[0.1827]
Loss:[0.1834]
Loss:[0.1748]
Loss:[0.1695]
Loss:[0.1616]
Loss:[0.1637]
Loss:[0.1582]
Loss:[0.1619]
Loss:[0.1529]
Loss:[0.1550]
Loss:[0.1484]
Loss:[0.1382]
Loss:[0.1469]
Loss:[0.1416]
Loss:[0.1375]
Loss:[0.1297]
Loss:[0.1411]
Loss:[0.1433]
Loss:[0.1329]
Loss:[0.1219]
Loss:[0.1364]
Loss:[0.1273]
Loss:[0.1192]
Loss:[0.1236]
Loss:[0.1288]
Loss:[0.1303]
Loss:[0.1240]
Loss:[0.1212]
Loss:[0.1299]
Loss:[0.1187]
Loss:[0.1218]
Loss:[0.1250]
Loss:[0.1178]
Loss:[0.1080]
Loss:[0.1217]
Loss:[0.1194]
Loss:[0.1187]
Loss:[0.1141]
Loss:[0.1137]
Loss:[0.1072]
Loss:[0.1129]
Loss:[0.1092]
Loss:[0.1080]
Loss:[0.1026]
Loss:[0.1031]
Loss:[0.1052]
Loss:[0.1123]
Loss:[0.1068]
Loss:[0.1064]
Loss:[0.1066]
Loss:[0.1029]
Loss:[0.1014]
Loss:[0.1171]
Loss:[0.1026]
Loss:[0.1039]
Loss:[0.0971]
Loss:[0.1000]
Loss:[0.1025]
Loss:[0.1028]
Loss:[0.0921]
Loss:[0.1053]
Loss:[0.1026]
Loss:[0.0999]
Loss:[0.1063]
Loss:[0.0944]
Loss:[0.0950]
Loss:[0.0975]
Loss:[0.1015]
Loss:[0.0949]
Loss:[0.0918]
Loss:[0.0987]
Loss:[0.0976]
Loss:[0.0946]
Loss:[0.1059]
Loss:[0.1006]
Loss:[0.1057]
Loss:[0.1005]
Loss:[0.0945]
Loss:[0.0982]
Loss:[0.0997]
Loss:[0.1039]
Loss:[0.0909]
Loss:[0.0990]
Loss:[0.0973]
Loss:[0.1001]
Loss:[0.0935]
Loss:[0.1034]
Loss:[0.0987]
Loss:[0.0992]
Loss:[0.0958]
Loss:[0.1032]
Loss:[0.1067]
Loss:[0.1029]
Loss:[0.1247]
Loss:[0.1460]
Loss:[0.1106]
Loss:[0.1674]
Loss:[0.1390]
Loss:[0.1505]
Loss:[0.1014]
Loss:[0.1537]
Loss:[0.1498]
Early stopping!
Loading 126th epoch
acc:[0.7220]
acc:[0.7240]
acc:[0.7210]
acc:[0.7240]
acc:[0.7220]
acc:[0.7200]
acc:[0.7210]
acc:[0.7210]
acc:[0.7250]
acc:[0.7240]
acc:[0.7200]
acc:[0.7230]
acc:[0.7190]
acc:[0.7250]
acc:[0.7230]
acc:[0.7220]
acc:[0.7220]
acc:[0.7210]
acc:[0.7210]
acc:[0.7210]
acc:[0.7200]
acc:[0.7190]
acc:[0.7200]
acc:[0.7220]
acc:[0.7190]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7220]
acc:[0.7230]
acc:[0.7190]
acc:[0.7210]
acc:[0.7240]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7190]
acc:[0.7230]
acc:[0.7210]
acc:[0.7250]
acc:[0.7200]
acc:[0.7230]
acc:[0.7240]
acc:[0.7220]
acc:[0.7230]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7230]
acc:[0.7230]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7218]
Mean:[72.1820]
Std :[0.1625]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.05, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6940]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6793]
Loss:[0.6811]
Loss:[0.6692]
Loss:[0.6649]
Loss:[0.6572]
Loss:[0.6437]
Loss:[0.6386]
Loss:[0.6210]
Loss:[0.6135]
Loss:[0.5932]
Loss:[0.5843]
Loss:[0.5650]
Loss:[0.5459]
Loss:[0.5347]
Loss:[0.5091]
Loss:[0.4974]
Loss:[0.4810]
Loss:[0.4545]
Loss:[0.4400]
Loss:[0.4283]
Loss:[0.4092]
Loss:[0.3837]
Loss:[0.3718]
Loss:[0.3632]
Loss:[0.3429]
Loss:[0.3208]
Loss:[0.3132]
Loss:[0.3023]
Loss:[0.2856]
Loss:[0.2767]
Loss:[0.2617]
Loss:[0.2535]
Loss:[0.2453]
Loss:[0.2386]
Loss:[0.2302]
Loss:[0.2218]
Loss:[0.2162]
Loss:[0.2103]
Loss:[0.2103]
Loss:[0.1940]
Loss:[0.1885]
Loss:[0.1827]
Loss:[0.1834]
Loss:[0.1748]
Loss:[0.1695]
Loss:[0.1616]
Loss:[0.1637]
Loss:[0.1582]
Loss:[0.1619]
Loss:[0.1529]
Loss:[0.1550]
Loss:[0.1484]
Loss:[0.1382]
Loss:[0.1469]
Loss:[0.1416]
Loss:[0.1375]
Loss:[0.1297]
Loss:[0.1411]
Loss:[0.1433]
Loss:[0.1329]
Loss:[0.1219]
Loss:[0.1364]
Loss:[0.1273]
Loss:[0.1192]
Loss:[0.1236]
Loss:[0.1288]
Loss:[0.1303]
Loss:[0.1240]
Loss:[0.1212]
Loss:[0.1299]
Loss:[0.1187]
Loss:[0.1218]
Loss:[0.1250]
Loss:[0.1178]
Loss:[0.1080]
Loss:[0.1217]
Loss:[0.1194]
Loss:[0.1187]
Loss:[0.1141]
Loss:[0.1137]
Loss:[0.1072]
Loss:[0.1129]
Loss:[0.1092]
Loss:[0.1080]
Loss:[0.1026]
Loss:[0.1031]
Loss:[0.1052]
Loss:[0.1123]
Loss:[0.1068]
Loss:[0.1064]
Loss:[0.1066]
Loss:[0.1029]
Loss:[0.1014]
Loss:[0.1171]
Loss:[0.1026]
Loss:[0.1039]
Loss:[0.0971]
Loss:[0.1000]
Loss:[0.1025]
Loss:[0.1028]
Loss:[0.0921]
Loss:[0.1053]
Loss:[0.1026]
Loss:[0.0999]
Loss:[0.1063]
Loss:[0.0944]
Loss:[0.0950]
Loss:[0.0975]
Loss:[0.1015]
Loss:[0.0949]
Loss:[0.0918]
Loss:[0.0987]
Loss:[0.0976]
Loss:[0.0946]
Loss:[0.1059]
Loss:[0.1006]
Loss:[0.1057]
Loss:[0.1005]
Loss:[0.0945]
Loss:[0.0982]
Loss:[0.0997]
Loss:[0.1039]
Loss:[0.0909]
Loss:[0.0990]
Loss:[0.0973]
Loss:[0.1001]
Loss:[0.0935]
Loss:[0.1034]
Loss:[0.0987]
Loss:[0.0992]
Loss:[0.0958]
Loss:[0.1032]
Loss:[0.1067]
Loss:[0.1029]
Loss:[0.1247]
Loss:[0.1460]
Loss:[0.1106]
Loss:[0.1674]
Loss:[0.1390]
Loss:[0.1505]
Loss:[0.1014]
Loss:[0.1537]
Loss:[0.1498]
Early stopping!
Loading 126th epoch
acc:[0.7220]
acc:[0.7240]
acc:[0.7210]
acc:[0.7240]
acc:[0.7220]
acc:[0.7200]
acc:[0.7210]
acc:[0.7210]
acc:[0.7250]
acc:[0.7240]
acc:[0.7200]
acc:[0.7230]
acc:[0.7190]
acc:[0.7250]
acc:[0.7230]
acc:[0.7220]
acc:[0.7220]
acc:[0.7210]
acc:[0.7210]
acc:[0.7210]
acc:[0.7200]
acc:[0.7190]
acc:[0.7200]
acc:[0.7220]
acc:[0.7190]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7220]
acc:[0.7230]
acc:[0.7190]
acc:[0.7210]
acc:[0.7240]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7190]
acc:[0.7230]
acc:[0.7210]
acc:[0.7250]
acc:[0.7200]
acc:[0.7230]
acc:[0.7240]
acc:[0.7220]
acc:[0.7230]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7230]
acc:[0.7230]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7218]
Mean:[72.1820]
Std :[0.1625]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.08, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6940]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6793]
Loss:[0.6811]
Loss:[0.6692]
Loss:[0.6649]
Loss:[0.6572]
Loss:[0.6437]
Loss:[0.6386]
Loss:[0.6210]
Loss:[0.6135]
Loss:[0.5932]
Loss:[0.5843]
Loss:[0.5650]
Loss:[0.5459]
Loss:[0.5347]
Loss:[0.5091]
Loss:[0.4974]
Loss:[0.4810]
Loss:[0.4545]
Loss:[0.4400]
Loss:[0.4283]
Loss:[0.4092]
Loss:[0.3837]
Loss:[0.3718]
Loss:[0.3632]
Loss:[0.3429]
Loss:[0.3208]
Loss:[0.3132]
Loss:[0.3023]
Loss:[0.2856]
Loss:[0.2767]
Loss:[0.2617]
Loss:[0.2535]
Loss:[0.2453]
Loss:[0.2386]
Loss:[0.2302]
Loss:[0.2218]
Loss:[0.2162]
Loss:[0.2103]
Loss:[0.2103]
Loss:[0.1940]
Loss:[0.1885]
Loss:[0.1827]
Loss:[0.1834]
Loss:[0.1748]
Loss:[0.1695]
Loss:[0.1616]
Loss:[0.1637]
Loss:[0.1582]
Loss:[0.1619]
Loss:[0.1529]
Loss:[0.1550]
Loss:[0.1484]
Loss:[0.1382]
Loss:[0.1469]
Loss:[0.1416]
Loss:[0.1375]
Loss:[0.1297]
Loss:[0.1411]
Loss:[0.1433]
Loss:[0.1329]
Loss:[0.1219]
Loss:[0.1364]
Loss:[0.1273]
Loss:[0.1192]
Loss:[0.1236]
Loss:[0.1288]
Loss:[0.1303]
Loss:[0.1240]
Loss:[0.1212]
Loss:[0.1299]
Loss:[0.1187]
Loss:[0.1218]
Loss:[0.1250]
Loss:[0.1178]
Loss:[0.1080]
Loss:[0.1217]
Loss:[0.1194]
Loss:[0.1187]
Loss:[0.1141]
Loss:[0.1137]
Loss:[0.1072]
Loss:[0.1129]
Loss:[0.1092]
Loss:[0.1080]
Loss:[0.1026]
Loss:[0.1031]
Loss:[0.1052]
Loss:[0.1123]
Loss:[0.1068]
Loss:[0.1064]
Loss:[0.1066]
Loss:[0.1029]
Loss:[0.1014]
Loss:[0.1171]
Loss:[0.1026]
Loss:[0.1039]
Loss:[0.0971]
Loss:[0.1000]
Loss:[0.1025]
Loss:[0.1028]
Loss:[0.0921]
Loss:[0.1053]
Loss:[0.1026]
Loss:[0.0999]
Loss:[0.1063]
Loss:[0.0944]
Loss:[0.0950]
Loss:[0.0975]
Loss:[0.1015]
Loss:[0.0949]
Loss:[0.0918]
Loss:[0.0987]
Loss:[0.0976]
Loss:[0.0946]
Loss:[0.1059]
Loss:[0.1006]
Loss:[0.1057]
Loss:[0.1005]
Loss:[0.0945]
Loss:[0.0982]
Loss:[0.0997]
Loss:[0.1039]
Loss:[0.0909]
Loss:[0.0990]
Loss:[0.0973]
Loss:[0.1001]
Loss:[0.0935]
Loss:[0.1034]
Loss:[0.0987]
Loss:[0.0992]
Loss:[0.0958]
Loss:[0.1032]
Loss:[0.1067]
Loss:[0.1029]
Loss:[0.1247]
Loss:[0.1460]
Loss:[0.1106]
Loss:[0.1674]
Loss:[0.1390]
Loss:[0.1505]
Loss:[0.1014]
Loss:[0.1537]
Loss:[0.1498]
Early stopping!
Loading 126th epoch
acc:[0.7220]
acc:[0.7240]
acc:[0.7210]
acc:[0.7240]
acc:[0.7220]
acc:[0.7200]
acc:[0.7210]
acc:[0.7210]
acc:[0.7250]
acc:[0.7240]
acc:[0.7200]
acc:[0.7230]
acc:[0.7190]
acc:[0.7250]
acc:[0.7230]
acc:[0.7220]
acc:[0.7220]
acc:[0.7210]
acc:[0.7210]
acc:[0.7210]
acc:[0.7200]
acc:[0.7190]
acc:[0.7200]
acc:[0.7220]
acc:[0.7190]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7220]
acc:[0.7230]
acc:[0.7190]
acc:[0.7210]
acc:[0.7240]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7190]
acc:[0.7230]
acc:[0.7210]
acc:[0.7250]
acc:[0.7200]
acc:[0.7230]
acc:[0.7240]
acc:[0.7220]
acc:[0.7230]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7230]
acc:[0.7230]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7218]
Mean:[72.1820]
Std :[0.1625]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.1, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6940]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6793]
Loss:[0.6811]
Loss:[0.6692]
Loss:[0.6649]
Loss:[0.6572]
Loss:[0.6437]
Loss:[0.6386]
Loss:[0.6210]
Loss:[0.6135]
Loss:[0.5932]
Loss:[0.5843]
Loss:[0.5650]
Loss:[0.5459]
Loss:[0.5347]
Loss:[0.5091]
Loss:[0.4974]
Loss:[0.4810]
Loss:[0.4545]
Loss:[0.4400]
Loss:[0.4283]
Loss:[0.4092]
Loss:[0.3837]
Loss:[0.3718]
Loss:[0.3632]
Loss:[0.3429]
Loss:[0.3208]
Loss:[0.3132]
Loss:[0.3023]
Loss:[0.2856]
Loss:[0.2767]
Loss:[0.2617]
Loss:[0.2535]
Loss:[0.2453]
Loss:[0.2386]
Loss:[0.2302]
Loss:[0.2218]
Loss:[0.2162]
Loss:[0.2103]
Loss:[0.2103]
Loss:[0.1940]
Loss:[0.1885]
Loss:[0.1827]
Loss:[0.1834]
Loss:[0.1748]
Loss:[0.1695]
Loss:[0.1616]
Loss:[0.1637]
Loss:[0.1582]
Loss:[0.1619]
Loss:[0.1529]
Loss:[0.1550]
Loss:[0.1484]
Loss:[0.1382]
Loss:[0.1469]
Loss:[0.1416]
Loss:[0.1375]
Loss:[0.1297]
Loss:[0.1411]
Loss:[0.1433]
Loss:[0.1329]
Loss:[0.1219]
Loss:[0.1364]
Loss:[0.1273]
Loss:[0.1192]
Loss:[0.1236]
Loss:[0.1288]
Loss:[0.1303]
Loss:[0.1240]
Loss:[0.1212]
Loss:[0.1299]
Loss:[0.1187]
Loss:[0.1218]
Loss:[0.1250]
Loss:[0.1178]
Loss:[0.1080]
Loss:[0.1217]
Loss:[0.1194]
Loss:[0.1187]
Loss:[0.1141]
Loss:[0.1137]
Loss:[0.1072]
Loss:[0.1129]
Loss:[0.1092]
Loss:[0.1080]
Loss:[0.1026]
Loss:[0.1031]
Loss:[0.1052]
Loss:[0.1123]
Loss:[0.1068]
Loss:[0.1064]
Loss:[0.1066]
Loss:[0.1029]
Loss:[0.1014]
Loss:[0.1171]
Loss:[0.1026]
Loss:[0.1039]
Loss:[0.0971]
Loss:[0.1000]
Loss:[0.1025]
Loss:[0.1028]
Loss:[0.0921]
Loss:[0.1053]
Loss:[0.1026]
Loss:[0.0999]
Loss:[0.1063]
Loss:[0.0944]
Loss:[0.0950]
Loss:[0.0975]
Loss:[0.1015]
Loss:[0.0949]
Loss:[0.0918]
Loss:[0.0987]
Loss:[0.0976]
Loss:[0.0946]
Loss:[0.1059]
Loss:[0.1006]
Loss:[0.1057]
Loss:[0.1005]
Loss:[0.0945]
Loss:[0.0982]
Loss:[0.0997]
Loss:[0.1039]
Loss:[0.0909]
Loss:[0.0990]
Loss:[0.0973]
Loss:[0.1001]
Loss:[0.0935]
Loss:[0.1034]
Loss:[0.0987]
Loss:[0.0992]
Loss:[0.0958]
Loss:[0.1032]
Loss:[0.1067]
Loss:[0.1029]
Loss:[0.1247]
Loss:[0.1460]
Loss:[0.1106]
Loss:[0.1674]
Loss:[0.1390]
Loss:[0.1505]
Loss:[0.1014]
Loss:[0.1537]
Loss:[0.1498]
Early stopping!
Loading 126th epoch
acc:[0.7220]
acc:[0.7240]
acc:[0.7210]
acc:[0.7240]
acc:[0.7220]
acc:[0.7200]
acc:[0.7210]
acc:[0.7210]
acc:[0.7250]
acc:[0.7240]
acc:[0.7200]
acc:[0.7230]
acc:[0.7190]
acc:[0.7250]
acc:[0.7230]
acc:[0.7220]
acc:[0.7220]
acc:[0.7210]
acc:[0.7210]
acc:[0.7210]
acc:[0.7200]
acc:[0.7190]
acc:[0.7200]
acc:[0.7220]
acc:[0.7190]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7220]
acc:[0.7230]
acc:[0.7190]
acc:[0.7210]
acc:[0.7240]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7190]
acc:[0.7230]
acc:[0.7210]
acc:[0.7250]
acc:[0.7200]
acc:[0.7230]
acc:[0.7240]
acc:[0.7220]
acc:[0.7230]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7230]
acc:[0.7230]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7218]
Mean:[72.1820]
Std :[0.1625]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.2, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6940]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6793]
Loss:[0.6811]
Loss:[0.6692]
Loss:[0.6649]
Loss:[0.6572]
Loss:[0.6437]
Loss:[0.6386]
Loss:[0.6210]
Loss:[0.6135]
Loss:[0.5932]
Loss:[0.5843]
Loss:[0.5650]
Loss:[0.5459]
Loss:[0.5347]
Loss:[0.5091]
Loss:[0.4974]
Loss:[0.4810]
Loss:[0.4545]
Loss:[0.4400]
Loss:[0.4283]
Loss:[0.4092]
Loss:[0.3837]
Loss:[0.3718]
Loss:[0.3632]
Loss:[0.3429]
Loss:[0.3208]
Loss:[0.3132]
Loss:[0.3023]
Loss:[0.2856]
Loss:[0.2767]
Loss:[0.2617]
Loss:[0.2535]
Loss:[0.2453]
Loss:[0.2386]
Loss:[0.2302]
Loss:[0.2218]
Loss:[0.2162]
Loss:[0.2103]
Loss:[0.2103]
Loss:[0.1940]
Loss:[0.1885]
Loss:[0.1827]
Loss:[0.1834]
Loss:[0.1748]
Loss:[0.1695]
Loss:[0.1616]
Loss:[0.1637]
Loss:[0.1582]
Loss:[0.1619]
Loss:[0.1529]
Loss:[0.1550]
Loss:[0.1484]
Loss:[0.1382]
Loss:[0.1469]
Loss:[0.1416]
Loss:[0.1375]
Loss:[0.1297]
Loss:[0.1411]
Loss:[0.1433]
Loss:[0.1329]
Loss:[0.1219]
Loss:[0.1364]
Loss:[0.1273]
Loss:[0.1192]
Loss:[0.1236]
Loss:[0.1288]
Loss:[0.1303]
Loss:[0.1240]
Loss:[0.1212]
Loss:[0.1299]
Loss:[0.1187]
Loss:[0.1218]
Loss:[0.1250]
Loss:[0.1178]
Loss:[0.1080]
Loss:[0.1217]
Loss:[0.1194]
Loss:[0.1187]
Loss:[0.1141]
Loss:[0.1137]
Loss:[0.1072]
Loss:[0.1129]
Loss:[0.1092]
Loss:[0.1080]
Loss:[0.1026]
Loss:[0.1031]
Loss:[0.1052]
Loss:[0.1123]
Loss:[0.1068]
Loss:[0.1064]
Loss:[0.1066]
Loss:[0.1029]
Loss:[0.1014]
Loss:[0.1171]
Loss:[0.1026]
Loss:[0.1039]
Loss:[0.0971]
Loss:[0.1000]
Loss:[0.1025]
Loss:[0.1028]
Loss:[0.0921]
Loss:[0.1053]
Loss:[0.1026]
Loss:[0.0999]
Loss:[0.1063]
Loss:[0.0944]
Loss:[0.0950]
Loss:[0.0975]
Loss:[0.1015]
Loss:[0.0949]
Loss:[0.0918]
Loss:[0.0987]
Loss:[0.0976]
Loss:[0.0946]
Loss:[0.1059]
Loss:[0.1006]
Loss:[0.1057]
Loss:[0.1005]
Loss:[0.0945]
Loss:[0.0982]
Loss:[0.0997]
Loss:[0.1039]
Loss:[0.0909]
Loss:[0.0990]
Loss:[0.0973]
Loss:[0.1001]
Loss:[0.0935]
Loss:[0.1034]
Loss:[0.0987]
Loss:[0.0992]
Loss:[0.0958]
Loss:[0.1032]
Loss:[0.1067]
Loss:[0.1029]
Loss:[0.1247]
Loss:[0.1460]
Loss:[0.1106]
Loss:[0.1674]
Loss:[0.1390]
Loss:[0.1505]
Loss:[0.1014]
Loss:[0.1537]
Loss:[0.1498]
Early stopping!
Loading 126th epoch
acc:[0.7220]
acc:[0.7240]
acc:[0.7210]
acc:[0.7240]
acc:[0.7220]
acc:[0.7200]
acc:[0.7210]
acc:[0.7210]
acc:[0.7250]
acc:[0.7240]
acc:[0.7200]
acc:[0.7230]
acc:[0.7190]
acc:[0.7250]
acc:[0.7230]
acc:[0.7220]
acc:[0.7220]
acc:[0.7210]
acc:[0.7210]
acc:[0.7210]
acc:[0.7200]
acc:[0.7190]
acc:[0.7200]
acc:[0.7220]
acc:[0.7190]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7220]
acc:[0.7230]
acc:[0.7190]
acc:[0.7210]
acc:[0.7240]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7190]
acc:[0.7230]
acc:[0.7210]
acc:[0.7250]
acc:[0.7200]
acc:[0.7230]
acc:[0.7240]
acc:[0.7220]
acc:[0.7230]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7230]
acc:[0.7230]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7218]
Mean:[72.1820]
Std :[0.1625]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.3, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6940]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6793]
Loss:[0.6811]
Loss:[0.6692]
Loss:[0.6649]
Loss:[0.6572]
Loss:[0.6437]
Loss:[0.6386]
Loss:[0.6210]
Loss:[0.6135]
Loss:[0.5932]
Loss:[0.5843]
Loss:[0.5650]
Loss:[0.5459]
Loss:[0.5347]
Loss:[0.5091]
Loss:[0.4974]
Loss:[0.4810]
Loss:[0.4545]
Loss:[0.4400]
Loss:[0.4283]
Loss:[0.4092]
Loss:[0.3837]
Loss:[0.3718]
Loss:[0.3632]
Loss:[0.3429]
Loss:[0.3208]
Loss:[0.3132]
Loss:[0.3023]
Loss:[0.2856]
Loss:[0.2767]
Loss:[0.2617]
Loss:[0.2535]
Loss:[0.2453]
Loss:[0.2386]
Loss:[0.2302]
Loss:[0.2218]
Loss:[0.2162]
Loss:[0.2103]
Loss:[0.2103]
Loss:[0.1940]
Loss:[0.1885]
Loss:[0.1827]
Loss:[0.1834]
Loss:[0.1748]
Loss:[0.1695]
Loss:[0.1616]
Loss:[0.1637]
Loss:[0.1582]
Loss:[0.1619]
Loss:[0.1529]
Loss:[0.1550]
Loss:[0.1484]
Loss:[0.1382]
Loss:[0.1469]
Loss:[0.1416]
Loss:[0.1375]
Loss:[0.1297]
Loss:[0.1411]
Loss:[0.1433]
Loss:[0.1329]
Loss:[0.1219]
Loss:[0.1364]
Loss:[0.1273]
Loss:[0.1192]
Loss:[0.1236]
Loss:[0.1288]
Loss:[0.1303]
Loss:[0.1240]
Loss:[0.1212]
Loss:[0.1299]
Loss:[0.1187]
Loss:[0.1218]
Loss:[0.1250]
Loss:[0.1178]
Loss:[0.1080]
Loss:[0.1217]
Loss:[0.1194]
Loss:[0.1187]
Loss:[0.1141]
Loss:[0.1137]
Loss:[0.1072]
Loss:[0.1129]
Loss:[0.1092]
Loss:[0.1080]
Loss:[0.1026]
Loss:[0.1031]
Loss:[0.1052]
Loss:[0.1123]
Loss:[0.1068]
Loss:[0.1064]
Loss:[0.1066]
Loss:[0.1029]
Loss:[0.1014]
Loss:[0.1171]
Loss:[0.1026]
Loss:[0.1039]
Loss:[0.0971]
Loss:[0.1000]
Loss:[0.1025]
Loss:[0.1028]
Loss:[0.0921]
Loss:[0.1053]
Loss:[0.1026]
Loss:[0.0999]
Loss:[0.1063]
Loss:[0.0944]
Loss:[0.0950]
Loss:[0.0975]
Loss:[0.1015]
Loss:[0.0949]
Loss:[0.0918]
Loss:[0.0987]
Loss:[0.0976]
Loss:[0.0946]
Loss:[0.1059]
Loss:[0.1006]
Loss:[0.1057]
Loss:[0.1005]
Loss:[0.0945]
Loss:[0.0982]
Loss:[0.0997]
Loss:[0.1039]
Loss:[0.0909]
Loss:[0.0990]
Loss:[0.0973]
Loss:[0.1001]
Loss:[0.0935]
Loss:[0.1034]
Loss:[0.0987]
Loss:[0.0992]
Loss:[0.0958]
Loss:[0.1032]
Loss:[0.1067]
Loss:[0.1029]
Loss:[0.1247]
Loss:[0.1460]
Loss:[0.1106]
Loss:[0.1674]
Loss:[0.1390]
Loss:[0.1505]
Loss:[0.1014]
Loss:[0.1537]
Loss:[0.1498]
Early stopping!
Loading 126th epoch
acc:[0.7220]
acc:[0.7240]
acc:[0.7210]
acc:[0.7240]
acc:[0.7220]
acc:[0.7200]
acc:[0.7210]
acc:[0.7210]
acc:[0.7250]
acc:[0.7240]
acc:[0.7200]
acc:[0.7230]
acc:[0.7190]
acc:[0.7250]
acc:[0.7230]
acc:[0.7220]
acc:[0.7220]
acc:[0.7210]
acc:[0.7210]
acc:[0.7210]
acc:[0.7200]
acc:[0.7190]
acc:[0.7200]
acc:[0.7220]
acc:[0.7190]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7220]
acc:[0.7230]
acc:[0.7190]
acc:[0.7210]
acc:[0.7240]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7190]
acc:[0.7230]
acc:[0.7210]
acc:[0.7250]
acc:[0.7200]
acc:[0.7230]
acc:[0.7240]
acc:[0.7220]
acc:[0.7230]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7230]
acc:[0.7230]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7218]
Mean:[72.1820]
Std :[0.1625]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.4, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6941]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6794]
Loss:[0.6811]
Loss:[0.6694]
Loss:[0.6649]
Loss:[0.6573]
Loss:[0.6438]
Loss:[0.6386]
Loss:[0.6211]
Loss:[0.6134]
Loss:[0.5933]
Loss:[0.5840]
Loss:[0.5651]
Loss:[0.5455]
Loss:[0.5348]
Loss:[0.5093]
Loss:[0.4966]
Loss:[0.4813]
Loss:[0.4547]
Loss:[0.4392]
Loss:[0.4281]
Loss:[0.4100]
Loss:[0.3837]
Loss:[0.3710]
Loss:[0.3628]
Loss:[0.3433]
Loss:[0.3211]
Loss:[0.3129]
Loss:[0.3018]
Loss:[0.2856]
Loss:[0.2771]
Loss:[0.2619]
Loss:[0.2533]
Loss:[0.2449]
Loss:[0.2385]
Loss:[0.2307]
Loss:[0.2221]
Loss:[0.2156]
Loss:[0.2091]
Loss:[0.2099]
Loss:[0.1945]
Loss:[0.1887]
Loss:[0.1817]
Loss:[0.1816]
Loss:[0.1748]
Loss:[0.1718]
Loss:[0.1628]
Loss:[0.1628]
Loss:[0.1570]
Loss:[0.1623]
Loss:[0.1523]
Loss:[0.1523]
Loss:[0.1470]
Loss:[0.1385]
Loss:[0.1458]
Loss:[0.1385]
Loss:[0.1365]
Loss:[0.1309]
Loss:[0.1371]
Loss:[0.1361]
Loss:[0.1343]
Loss:[0.1247]
Loss:[0.1309]
Loss:[0.1261]
Loss:[0.1296]
Loss:[0.1252]
Loss:[0.1263]
Loss:[0.1356]
Loss:[0.1275]
Loss:[0.1185]
Loss:[0.1301]
Loss:[0.1225]
Loss:[0.1210]
Loss:[0.1229]
Loss:[0.1208]
Loss:[0.1082]
Loss:[0.1182]
Loss:[0.1241]
Loss:[0.1223]
Loss:[0.1113]
Loss:[0.1184]
Loss:[0.1108]
Loss:[0.1108]
Loss:[0.1184]
Loss:[0.1151]
Loss:[0.1021]
Loss:[0.1097]
Loss:[0.1055]
Loss:[0.1110]
Loss:[0.1128]
Loss:[0.1059]
Loss:[0.1056]
Loss:[0.1074]
Loss:[0.0993]
Loss:[0.1164]
Loss:[0.1065]
Loss:[0.1029]
Loss:[0.0963]
Loss:[0.0993]
Loss:[0.0992]
Loss:[0.1006]
Loss:[0.0919]
Loss:[0.1036]
Loss:[0.1015]
Loss:[0.0995]
Loss:[0.1046]
Loss:[0.0942]
Loss:[0.0949]
Loss:[0.0962]
Loss:[0.1019]
Loss:[0.0947]
Loss:[0.0907]
Loss:[0.0974]
Loss:[0.0972]
Loss:[0.0920]
Loss:[0.1033]
Loss:[0.1007]
Loss:[0.1033]
Loss:[0.0983]
Loss:[0.0945]
Loss:[0.0958]
Loss:[0.0982]
Loss:[0.1040]
Loss:[0.0881]
Loss:[0.0971]
Loss:[0.0975]
Loss:[0.0978]
Loss:[0.0925]
Loss:[0.1028]
Loss:[0.0937]
Loss:[0.0938]
Loss:[0.0952]
Loss:[0.0928]
Loss:[0.0928]
Loss:[0.1008]
Loss:[0.0947]
Loss:[0.1038]
Loss:[0.0968]
Loss:[0.1045]
Loss:[0.0987]
Loss:[0.0913]
Loss:[0.0999]
Loss:[0.1012]
Loss:[0.0874]
Loss:[0.0961]
Loss:[0.1030]
Loss:[0.0941]
Loss:[0.1111]
Loss:[0.1011]
Loss:[0.0995]
Loss:[0.1020]
Loss:[0.0951]
Loss:[0.0988]
Loss:[0.0980]
Loss:[0.0938]
Loss:[0.0935]
Loss:[0.0941]
Loss:[0.0961]
Loss:[0.0854]
Loss:[0.0897]
Loss:[0.0917]
Loss:[0.0881]
Loss:[0.0897]
Loss:[0.0941]
Loss:[0.0889]
Loss:[0.0997]
Loss:[0.0897]
Loss:[0.0963]
Loss:[0.0954]
Loss:[0.0851]
Loss:[0.0892]
Loss:[0.0864]
Loss:[0.0855]
Loss:[0.0957]
Loss:[0.0884]
Loss:[0.0920]
Loss:[0.0846]
Loss:[0.0832]
Loss:[0.0821]
Loss:[0.0864]
Loss:[0.0836]
Loss:[0.0851]
Loss:[0.0810]
Loss:[0.0828]
Loss:[0.0845]
Loss:[0.0905]
Loss:[0.0818]
Loss:[0.0869]
Loss:[0.0859]
Loss:[0.0847]
Loss:[0.0955]
Loss:[0.0952]
Loss:[0.0810]
Loss:[0.0923]
Loss:[0.0856]
Loss:[0.0863]
Loss:[0.0866]
Loss:[0.0878]
Loss:[0.0893]
Loss:[0.0870]
Loss:[0.0814]
Loss:[0.0920]
Loss:[0.0923]
Loss:[0.0852]
Loss:[0.0916]
Loss:[0.0851]
Loss:[0.0832]
Loss:[0.0813]
Loss:[0.0803]
Loss:[0.0875]
Loss:[0.0915]
Loss:[0.0845]
Loss:[0.0878]
Loss:[0.0866]
Loss:[0.0812]
Loss:[0.0849]
Loss:[0.0873]
Loss:[0.0891]
Loss:[0.0870]
Loss:[0.0835]
Loss:[0.0880]
Loss:[0.0821]
Loss:[0.0908]
Loss:[0.0841]
Loss:[0.0820]
Loss:[0.0893]
Loss:[0.0879]
Loss:[0.0854]
Loss:[0.0838]
Early stopping!
Loading 211th epoch
acc:[0.7220]
acc:[0.7210]
acc:[0.7220]
acc:[0.7230]
acc:[0.7200]
acc:[0.7210]
acc:[0.7210]
acc:[0.7210]
acc:[0.7210]
acc:[0.7220]
acc:[0.7210]
acc:[0.7210]
acc:[0.7220]
acc:[0.7210]
acc:[0.7230]
acc:[0.7200]
acc:[0.7210]
acc:[0.7230]
acc:[0.7210]
acc:[0.7220]
acc:[0.7210]
acc:[0.7230]
acc:[0.7200]
acc:[0.7240]
acc:[0.7190]
acc:[0.7220]
acc:[0.7210]
acc:[0.7220]
acc:[0.7220]
acc:[0.7220]
acc:[0.7220]
acc:[0.7200]
acc:[0.7220]
acc:[0.7200]
acc:[0.7220]
acc:[0.7210]
acc:[0.7200]
acc:[0.7210]
acc:[0.7230]
acc:[0.7240]
acc:[0.7210]
acc:[0.7200]
acc:[0.7210]
acc:[0.7210]
acc:[0.7220]
acc:[0.7210]
acc:[0.7220]
acc:[0.7210]
acc:[0.7200]
acc:[0.7210]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7214]
Mean:[72.1400]
Std :[0.1069]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.5, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6941]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6794]
Loss:[0.6811]
Loss:[0.6694]
Loss:[0.6649]
Loss:[0.6573]
Loss:[0.6438]
Loss:[0.6386]
Loss:[0.6211]
Loss:[0.6134]
Loss:[0.5933]
Loss:[0.5841]
Loss:[0.5651]
Loss:[0.5456]
Loss:[0.5348]
Loss:[0.5093]
Loss:[0.4968]
Loss:[0.4812]
Loss:[0.4547]
Loss:[0.4393]
Loss:[0.4281]
Loss:[0.4099]
Loss:[0.3837]
Loss:[0.3711]
Loss:[0.3628]
Loss:[0.3433]
Loss:[0.3211]
Loss:[0.3128]
Loss:[0.3018]
Loss:[0.2857]
Loss:[0.2772]
Loss:[0.2619]
Loss:[0.2532]
Loss:[0.2448]
Loss:[0.2384]
Loss:[0.2308]
Loss:[0.2222]
Loss:[0.2155]
Loss:[0.2089]
Loss:[0.2100]
Loss:[0.1947]
Loss:[0.1887]
Loss:[0.1814]
Loss:[0.1813]
Loss:[0.1746]
Loss:[0.1721]
Loss:[0.1631]
Loss:[0.1627]
Loss:[0.1567]
Loss:[0.1622]
Loss:[0.1523]
Loss:[0.1519]
Loss:[0.1466]
Loss:[0.1384]
Loss:[0.1457]
Loss:[0.1380]
Loss:[0.1358]
Loss:[0.1310]
Loss:[0.1368]
Loss:[0.1351]
Loss:[0.1340]
Loss:[0.1254]
Loss:[0.1311]
Loss:[0.1247]
Loss:[0.1287]
Loss:[0.1264]
Loss:[0.1251]
Loss:[0.1325]
Loss:[0.1289]
Loss:[0.1188]
Loss:[0.1265]
Loss:[0.1245]
Loss:[0.1257]
Loss:[0.1213]
Loss:[0.1240]
Loss:[0.1157]
Loss:[0.1169]
Loss:[0.1329]
Loss:[0.1372]
Loss:[0.1113]
Loss:[0.1378]
Loss:[0.1273]
Loss:[0.1214]
Loss:[0.1430]
Loss:[0.1143]
Loss:[0.1161]
Loss:[0.1060]
Loss:[0.1129]
Loss:[0.1205]
Loss:[0.1129]
Loss:[0.1149]
Loss:[0.1035]
Loss:[0.1101]
Loss:[0.0980]
Loss:[0.1173]
Loss:[0.1014]
Loss:[0.1072]
Loss:[0.0949]
Loss:[0.1009]
Loss:[0.0973]
Loss:[0.1005]
Loss:[0.0921]
Loss:[0.1040]
Loss:[0.1011]
Loss:[0.0989]
Loss:[0.1034]
Loss:[0.0943]
Loss:[0.0936]
Loss:[0.0958]
Loss:[0.1018]
Loss:[0.0944]
Loss:[0.0903]
Loss:[0.0976]
Loss:[0.0964]
Loss:[0.0926]
Loss:[0.1035]
Loss:[0.1005]
Loss:[0.1041]
Loss:[0.0976]
Loss:[0.0953]
Loss:[0.0954]
Loss:[0.0993]
Loss:[0.1035]
Loss:[0.0886]
Loss:[0.0974]
Loss:[0.0977]
Loss:[0.0981]
Loss:[0.0925]
Loss:[0.1010]
Loss:[0.0927]
Loss:[0.0938]
Loss:[0.0948]
Loss:[0.0928]
Loss:[0.0911]
Loss:[0.1012]
Loss:[0.0919]
Loss:[0.0968]
Loss:[0.0968]
Loss:[0.0980]
Loss:[0.0932]
Loss:[0.0906]
Loss:[0.0921]
Loss:[0.0956]
Loss:[0.0871]
Loss:[0.0863]
Loss:[0.0952]
Loss:[0.0923]
Loss:[0.0944]
Loss:[0.0910]
Loss:[0.0956]
Loss:[0.0880]
Loss:[0.0920]
Loss:[0.0919]
Loss:[0.0915]
Loss:[0.0940]
Loss:[0.0888]
Loss:[0.0943]
Loss:[0.0917]
Loss:[0.0829]
Loss:[0.0876]
Loss:[0.0867]
Loss:[0.0881]
Loss:[0.0876]
Loss:[0.0935]
Loss:[0.0862]
Loss:[0.0970]
Loss:[0.0893]
Loss:[0.0939]
Loss:[0.0957]
Loss:[0.0847]
Loss:[0.0892]
Loss:[0.0859]
Loss:[0.0854]
Loss:[0.0950]
Loss:[0.0877]
Loss:[0.0923]
Loss:[0.0847]
Loss:[0.0833]
Loss:[0.0827]
Loss:[0.0868]
Loss:[0.0843]
Loss:[0.0852]
Loss:[0.0817]
Loss:[0.0834]
Loss:[0.0851]
Loss:[0.0917]
Loss:[0.0822]
Loss:[0.0876]
Loss:[0.0865]
Loss:[0.0855]
Loss:[0.0964]
Loss:[0.0954]
Loss:[0.0817]
Loss:[0.0926]
Loss:[0.0866]
Loss:[0.0870]
Loss:[0.0874]
Loss:[0.0887]
Loss:[0.0893]
Loss:[0.0880]
Loss:[0.0815]
Loss:[0.0927]
Loss:[0.0930]
Loss:[0.0856]
Loss:[0.0921]
Loss:[0.0852]
Loss:[0.0839]
Loss:[0.0814]
Loss:[0.0807]
Loss:[0.0881]
Loss:[0.0918]
Loss:[0.0856]
Loss:[0.0883]
Loss:[0.0872]
Loss:[0.0818]
Loss:[0.0854]
Loss:[0.0875]
Loss:[0.0893]
Loss:[0.0878]
Loss:[0.0835]
Loss:[0.0887]
Loss:[0.0824]
Loss:[0.0908]
Loss:[0.0851]
Loss:[0.0825]
Loss:[0.0900]
Loss:[0.0877]
Loss:[0.0856]
Loss:[0.0843]
Early stopping!
Loading 211th epoch
acc:[0.7230]
acc:[0.7230]
acc:[0.7240]
acc:[0.7240]
acc:[0.7210]
acc:[0.7230]
acc:[0.7240]
acc:[0.7240]
acc:[0.7230]
acc:[0.7240]
acc:[0.7220]
acc:[0.7230]
acc:[0.7220]
acc:[0.7230]
acc:[0.7230]
acc:[0.7230]
acc:[0.7240]
acc:[0.7230]
acc:[0.7230]
acc:[0.7240]
acc:[0.7230]
acc:[0.7230]
acc:[0.7200]
acc:[0.7240]
acc:[0.7210]
acc:[0.7210]
acc:[0.7230]
acc:[0.7220]
acc:[0.7220]
acc:[0.7200]
acc:[0.7220]
acc:[0.7220]
acc:[0.7240]
acc:[0.7220]
acc:[0.7210]
acc:[0.7230]
acc:[0.7220]
acc:[0.7230]
acc:[0.7210]
acc:[0.7240]
acc:[0.7250]
acc:[0.7200]
acc:[0.7210]
acc:[0.7220]
acc:[0.7210]
acc:[0.7230]
acc:[0.7220]
acc:[0.7230]
acc:[0.7250]
acc:[0.7230]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7226]
Mean:[72.2620]
Std :[0.1244]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.6, gpu=5, save_name='cite_best_dgi.pkl', seed=39)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6940]
Loss:[0.6891]
Loss:[0.6878]
Loss:[0.6793]
Loss:[0.6811]
Loss:[0.6693]
Loss:[0.6649]
Loss:[0.6573]
Loss:[0.6437]
Loss:[0.6387]
Loss:[0.6210]
Loss:[0.6136]
Loss:[0.5932]
Loss:[0.5844]
Loss:[0.5650]
Loss:[0.5460]
Loss:[0.5348]
Loss:[0.5092]
Loss:[0.4975]
Loss:[0.4810]
Loss:[0.4545]
Loss:[0.4402]
Loss:[0.4284]
Loss:[0.4093]
Loss:[0.3838]
Loss:[0.3719]
Loss:[0.3633]
Loss:[0.3430]
Loss:[0.3209]
Loss:[0.3133]
Loss:[0.3024]
Loss:[0.2856]
Loss:[0.2767]
Loss:[0.2618]
Loss:[0.2536]
Loss:[0.2453]
Loss:[0.2385]
Loss:[0.2303]
Loss:[0.2219]
Loss:[0.2162]
Loss:[0.2102]
Loss:[0.2103]
Loss:[0.1941]
Loss:[0.1885]
Loss:[0.1825]
Loss:[0.1830]
Loss:[0.1749]
Loss:[0.1699]
Loss:[0.1617]
Loss:[0.1635]
Loss:[0.1581]
Loss:[0.1620]
Loss:[0.1526]
Loss:[0.1544]
Loss:[0.1482]
Loss:[0.1383]
Loss:[0.1466]
Loss:[0.1411]
Loss:[0.1378]
Loss:[0.1298]
Loss:[0.1402]
Loss:[0.1423]
Loss:[0.1335]
Loss:[0.1217]
Loss:[0.1350]
Loss:[0.1280]
Loss:[0.1213]
Loss:[0.1232]
Loss:[0.1297]
Loss:[0.1329]
Loss:[0.1242]
Loss:[0.1223]
Loss:[0.1330]
Loss:[0.1187]
Loss:[0.1230]
Loss:[0.1278]
Loss:[0.1173]
Loss:[0.1103]
Loss:[0.1255]
Loss:[0.1181]
Loss:[0.1232]
Loss:[0.1199]
Loss:[0.1128]
Loss:[0.1153]
Loss:[0.1193]
Loss:[0.1081]
Loss:[0.1205]
Loss:[0.1088]
Loss:[0.1097]
Loss:[0.1209]
Loss:[0.1136]
Loss:[0.1130]
Loss:[0.1106]
Loss:[0.1035]
Loss:[0.1074]
Loss:[0.0989]
Loss:[0.1150]
Loss:[0.1028]
Loss:[0.1045]
Loss:[0.0978]
Loss:[0.0997]
Loss:[0.0991]
Loss:[0.0990]
Loss:[0.0923]
Loss:[0.1060]
Loss:[0.1018]
Loss:[0.1006]
Loss:[0.1038]
Loss:[0.0942]
Loss:[0.0943]
Loss:[0.0955]
Loss:[0.1021]
Loss:[0.0950]
Loss:[0.0907]
Loss:[0.0992]
Loss:[0.0964]
Loss:[0.0943]
Loss:[0.1047]
Loss:[0.1007]
Loss:[0.1061]
Loss:[0.0986]
Loss:[0.0956]
Loss:[0.0968]
Loss:[0.0983]
Loss:[0.1048]
Loss:[0.0885]
Loss:[0.0979]
Loss:[0.0977]
Loss:[0.0980]
Loss:[0.0942]
Loss:[0.1021]
Loss:[0.0930]
Loss:[0.0934]
Loss:[0.0950]
Loss:[0.0908]
Loss:[0.0913]
Loss:[0.1004]
Loss:[0.0912]
Loss:[0.0988]
Loss:[0.0965]
Loss:[0.0994]
Loss:[0.0934]
Loss:[0.0908]
Loss:[0.0925]
Loss:[0.0956]
Loss:[0.0872]
Loss:[0.0864]
Loss:[0.0953]
Loss:[0.0917]
Loss:[0.0955]
Loss:[0.0907]
Loss:[0.0957]
Loss:[0.0876]
Loss:[0.0917]
Loss:[0.0923]
Loss:[0.0903]
Loss:[0.0946]
Loss:[0.0886]
Loss:[0.0950]
Loss:[0.0911]
Loss:[0.0826]
Loss:[0.0870]
Loss:[0.0856]
Loss:[0.0888]
Loss:[0.0867]
Loss:[0.0949]
Loss:[0.0847]
Loss:[0.0967]
Loss:[0.0922]
Loss:[0.0928]
Loss:[0.0963]
Loss:[0.0868]
Loss:[0.0896]
Loss:[0.0868]
Loss:[0.0875]
Loss:[0.0953]
Loss:[0.0875]
Loss:[0.0927]
Loss:[0.0853]
Loss:[0.0828]
Loss:[0.0836]
Early stopping!
Loading 161th epoch
acc:[0.7300]
acc:[0.7290]
acc:[0.7270]
acc:[0.7280]
acc:[0.7270]
acc:[0.7270]
acc:[0.7280]
acc:[0.7270]
acc:[0.7290]
acc:[0.7280]
acc:[0.7260]
acc:[0.7290]
acc:[0.7260]
acc:[0.7280]
acc:[0.7260]
acc:[0.7260]
acc:[0.7290]
acc:[0.7290]
acc:[0.7270]
acc:[0.7260]
acc:[0.7270]
acc:[0.7270]
acc:[0.7290]
acc:[0.7250]
acc:[0.7260]
acc:[0.7270]
acc:[0.7270]
acc:[0.7270]
acc:[0.7290]
acc:[0.7270]
acc:[0.7280]
acc:[0.7290]
acc:[0.7280]
acc:[0.7270]
acc:[0.7250]
acc:[0.7270]
acc:[0.7230]
acc:[0.7270]
acc:[0.7290]
acc:[0.7300]
acc:[0.7270]
acc:[0.7260]
acc:[0.7290]
acc:[0.7280]
acc:[0.7260]
acc:[0.7260]
acc:[0.7270]
acc:[0.7250]
acc:[0.7300]
acc:[0.7270]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7273]
Mean:[72.7340]
Std :[0.1465]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.02, gpu=5, save_name='cite_best_dgi.pkl', seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6887]
Loss:[0.6862]
Loss:[0.6772]
Loss:[0.6792]
Loss:[0.6652]
Loss:[0.6640]
Loss:[0.6513]
Loss:[0.6417]
Loss:[0.6306]
Loss:[0.6175]
Loss:[0.6045]
Loss:[0.5883]
Loss:[0.5714]
Loss:[0.5605]
Loss:[0.5373]
Loss:[0.5234]
Loss:[0.5061]
Loss:[0.4814]
Loss:[0.4667]
Loss:[0.4555]
Loss:[0.4253]
Loss:[0.4096]
Loss:[0.4010]
Loss:[0.3828]
Loss:[0.3599]
Loss:[0.3489]
Loss:[0.3422]
Loss:[0.3215]
Loss:[0.3031]
Loss:[0.3031]
Loss:[0.2902]
Loss:[0.2698]
Loss:[0.2597]
Loss:[0.2556]
Loss:[0.2491]
Loss:[0.2357]
Loss:[0.2292]
Loss:[0.2189]
Loss:[0.2080]
Loss:[0.2070]
Loss:[0.2056]
Loss:[0.1929]
Loss:[0.1901]
Loss:[0.1799]
Loss:[0.1761]
Loss:[0.1714]
Loss:[0.1696]
Loss:[0.1661]
Loss:[0.1652]
Loss:[0.1641]
Loss:[0.1536]
Loss:[0.1534]
Loss:[0.1506]
Loss:[0.1416]
Loss:[0.1380]
Loss:[0.1515]
Loss:[0.1517]
Loss:[0.1381]
Loss:[0.1336]
Loss:[0.1365]
Loss:[0.1449]
Loss:[0.1391]
Loss:[0.1311]
Loss:[0.1339]
Loss:[0.1311]
Loss:[0.1360]
Loss:[0.1278]
Loss:[0.1299]
Loss:[0.1281]
Loss:[0.1266]
Loss:[0.1180]
Loss:[0.1193]
Loss:[0.1278]
Loss:[0.1185]
Loss:[0.1176]
Loss:[0.1196]
Loss:[0.1225]
Loss:[0.1099]
Loss:[0.1104]
Loss:[0.1141]
Loss:[0.1152]
Loss:[0.1089]
Loss:[0.1097]
Loss:[0.1122]
Loss:[0.1162]
Loss:[0.1059]
Loss:[0.1085]
Loss:[0.1072]
Loss:[0.1019]
Loss:[0.1091]
Loss:[0.1041]
Loss:[0.1085]
Loss:[0.1083]
Loss:[0.1125]
Loss:[0.1065]
Loss:[0.1067]
Loss:[0.1068]
Loss:[0.1074]
Loss:[0.1023]
Loss:[0.0979]
Loss:[0.1045]
Loss:[0.1055]
Loss:[0.1034]
Loss:[0.1022]
Loss:[0.1013]
Loss:[0.0979]
Loss:[0.1024]
Loss:[0.1042]
Loss:[0.1039]
Loss:[0.1067]
Loss:[0.0932]
Loss:[0.0946]
Loss:[0.1118]
Loss:[0.1046]
Loss:[0.1027]
Loss:[0.0990]
Loss:[0.0956]
Loss:[0.1016]
Loss:[0.0932]
Loss:[0.0970]
Loss:[0.0910]
Loss:[0.0970]
Loss:[0.0909]
Loss:[0.0891]
Loss:[0.0904]
Loss:[0.0988]
Loss:[0.0943]
Loss:[0.0990]
Loss:[0.0971]
Loss:[0.1041]
Loss:[0.0922]
Loss:[0.0963]
Loss:[0.0894]
Loss:[0.0929]
Loss:[0.0910]
Loss:[0.0903]
Loss:[0.0955]
Loss:[0.1020]
Loss:[0.0960]
Loss:[0.0925]
Loss:[0.0917]
Loss:[0.1039]
Loss:[0.0940]
Loss:[0.0999]
Early stopping!
Loading 125th epoch
acc:[0.7350]
acc:[0.7330]
acc:[0.7360]
acc:[0.7350]
acc:[0.7390]
acc:[0.7330]
acc:[0.7340]
acc:[0.7340]
acc:[0.7350]
acc:[0.7360]
acc:[0.7370]
acc:[0.7310]
acc:[0.7360]
acc:[0.7350]
acc:[0.7370]
acc:[0.7310]
acc:[0.7370]
acc:[0.7370]
acc:[0.7350]
acc:[0.7330]
acc:[0.7360]
acc:[0.7330]
acc:[0.7390]
acc:[0.7360]
acc:[0.7350]
acc:[0.7350]
acc:[0.7330]
acc:[0.7320]
acc:[0.7390]
acc:[0.7350]
acc:[0.7350]
acc:[0.7330]
acc:[0.7360]
acc:[0.7370]
acc:[0.7360]
acc:[0.7370]
acc:[0.7320]
acc:[0.7360]
acc:[0.7370]
acc:[0.7380]
acc:[0.7370]
acc:[0.7360]
acc:[0.7410]
acc:[0.7370]
acc:[0.7330]
acc:[0.7350]
acc:[0.7330]
acc:[0.7350]
acc:[0.7340]
acc:[0.7330]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7353]
Mean:[73.5260]
Std :[0.2136]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.05, gpu=5, save_name='cite_best_dgi.pkl', seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6887]
Loss:[0.6862]
Loss:[0.6772]
Loss:[0.6792]
Loss:[0.6652]
Loss:[0.6640]
Loss:[0.6513]
Loss:[0.6417]
Loss:[0.6306]
Loss:[0.6175]
Loss:[0.6045]
Loss:[0.5883]
Loss:[0.5714]
Loss:[0.5606]
Loss:[0.5373]
Loss:[0.5235]
Loss:[0.5061]
Loss:[0.4814]
Loss:[0.4667]
Loss:[0.4555]
Loss:[0.4253]
Loss:[0.4097]
Loss:[0.4010]
Loss:[0.3828]
Loss:[0.3599]
Loss:[0.3490]
Loss:[0.3422]
Loss:[0.3214]
Loss:[0.3033]
Loss:[0.3033]
Loss:[0.2901]
Loss:[0.2698]
Loss:[0.2600]
Loss:[0.2557]
Loss:[0.2490]
Loss:[0.2359]
Loss:[0.2294]
Loss:[0.2189]
Loss:[0.2079]
Loss:[0.2071]
Loss:[0.2059]
Loss:[0.1928]
Loss:[0.1898]
Loss:[0.1798]
Loss:[0.1763]
Loss:[0.1715]
Loss:[0.1695]
Loss:[0.1658]
Loss:[0.1651]
Loss:[0.1641]
Loss:[0.1537]
Loss:[0.1535]
Loss:[0.1506]
Loss:[0.1417]
Loss:[0.1381]
Loss:[0.1516]
Loss:[0.1516]
Loss:[0.1379]
Loss:[0.1336]
Loss:[0.1365]
Loss:[0.1448]
Loss:[0.1391]
Loss:[0.1313]
Loss:[0.1338]
Loss:[0.1310]
Loss:[0.1361]
Loss:[0.1279]
Loss:[0.1297]
Loss:[0.1278]
Loss:[0.1268]
Loss:[0.1180]
Loss:[0.1189]
Loss:[0.1280]
Loss:[0.1185]
Loss:[0.1172]
Loss:[0.1197]
Loss:[0.1231]
Loss:[0.1104]
Loss:[0.1105]
Loss:[0.1141]
Loss:[0.1156]
Loss:[0.1092]
Loss:[0.1097]
Loss:[0.1123]
Loss:[0.1164]
Loss:[0.1060]
Loss:[0.1085]
Loss:[0.1069]
Loss:[0.1017]
Loss:[0.1092]
Loss:[0.1039]
Loss:[0.1080]
Loss:[0.1082]
Loss:[0.1124]
Loss:[0.1060]
Loss:[0.1066]
Loss:[0.1069]
Loss:[0.1076]
Loss:[0.1023]
Loss:[0.0979]
Loss:[0.1044]
Loss:[0.1054]
Loss:[0.1034]
Loss:[0.1022]
Loss:[0.1013]
Loss:[0.0981]
Loss:[0.1024]
Loss:[0.1041]
Loss:[0.1040]
Loss:[0.1072]
Loss:[0.0933]
Loss:[0.0947]
Loss:[0.1122]
Loss:[0.1047]
Loss:[0.1028]
Loss:[0.0991]
Loss:[0.0956]
Loss:[0.1015]
Loss:[0.0932]
Loss:[0.0969]
Loss:[0.0910]
Loss:[0.0970]
Loss:[0.0910]
Loss:[0.0891]
Loss:[0.0904]
Loss:[0.0989]
Loss:[0.0944]
Loss:[0.0989]
Loss:[0.0972]
Loss:[0.1043]
Loss:[0.0922]
Loss:[0.0964]
Loss:[0.0896]
Loss:[0.0929]
Loss:[0.0912]
Loss:[0.0905]
Loss:[0.0955]
Loss:[0.1023]
Loss:[0.0966]
Loss:[0.0925]
Loss:[0.0926]
Loss:[0.1057]
Loss:[0.0938]
Loss:[0.1035]
Early stopping!
Loading 125th epoch
acc:[0.7360]
acc:[0.7350]
acc:[0.7350]
acc:[0.7340]
acc:[0.7360]
acc:[0.7340]
acc:[0.7350]
acc:[0.7340]
acc:[0.7350]
acc:[0.7350]
acc:[0.7370]
acc:[0.7330]
acc:[0.7350]
acc:[0.7350]
acc:[0.7370]
acc:[0.7350]
acc:[0.7360]
acc:[0.7360]
acc:[0.7330]
acc:[0.7310]
acc:[0.7320]
acc:[0.7330]
acc:[0.7380]
acc:[0.7340]
acc:[0.7340]
acc:[0.7370]
acc:[0.7350]
acc:[0.7320]
acc:[0.7370]
acc:[0.7340]
acc:[0.7350]
acc:[0.7330]
acc:[0.7350]
acc:[0.7350]
acc:[0.7340]
acc:[0.7340]
acc:[0.7330]
acc:[0.7360]
acc:[0.7340]
acc:[0.7370]
acc:[0.7340]
acc:[0.7370]
acc:[0.7400]
acc:[0.7370]
acc:[0.7310]
acc:[0.7340]
acc:[0.7330]
acc:[0.7360]
acc:[0.7340]
acc:[0.7340]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7348]
Mean:[73.4780]
Std :[0.1765]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.08, gpu=5, save_name='cite_best_dgi.pkl', seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6887]
Loss:[0.6862]
Loss:[0.6772]
Loss:[0.6792]
Loss:[0.6652]
Loss:[0.6640]
Loss:[0.6513]
Loss:[0.6417]
Loss:[0.6306]
Loss:[0.6175]
Loss:[0.6045]
Loss:[0.5883]
Loss:[0.5715]
Loss:[0.5606]
Loss:[0.5373]
Loss:[0.5235]
Loss:[0.5061]
Loss:[0.4814]
Loss:[0.4667]
Loss:[0.4555]
Loss:[0.4253]
Loss:[0.4097]
Loss:[0.4010]
Loss:[0.3828]
Loss:[0.3599]
Loss:[0.3490]
Loss:[0.3422]
Loss:[0.3214]
Loss:[0.3032]
Loss:[0.3031]
Loss:[0.2902]
Loss:[0.2698]
Loss:[0.2597]
Loss:[0.2557]
Loss:[0.2491]
Loss:[0.2358]
Loss:[0.2293]
Loss:[0.2189]
Loss:[0.2080]
Loss:[0.2070]
Loss:[0.2056]
Loss:[0.1928]
Loss:[0.1901]
Loss:[0.1799]
Loss:[0.1762]
Loss:[0.1714]
Loss:[0.1696]
Loss:[0.1661]
Loss:[0.1652]
Loss:[0.1641]
Loss:[0.1537]
Loss:[0.1534]
Loss:[0.1506]
Loss:[0.1416]
Loss:[0.1380]
Loss:[0.1515]
Loss:[0.1516]
Loss:[0.1380]
Loss:[0.1336]
Loss:[0.1364]
Loss:[0.1447]
Loss:[0.1390]
Loss:[0.1313]
Loss:[0.1338]
Loss:[0.1310]
Loss:[0.1361]
Loss:[0.1279]
Loss:[0.1296]
Loss:[0.1277]
Loss:[0.1268]
Loss:[0.1179]
Loss:[0.1188]
Loss:[0.1279]
Loss:[0.1185]
Loss:[0.1172]
Loss:[0.1196]
Loss:[0.1230]
Loss:[0.1103]
Loss:[0.1104]
Loss:[0.1141]
Loss:[0.1156]
Loss:[0.1092]
Loss:[0.1097]
Loss:[0.1123]
Loss:[0.1164]
Loss:[0.1061]
Loss:[0.1085]
Loss:[0.1069]
Loss:[0.1017]
Loss:[0.1091]
Loss:[0.1039]
Loss:[0.1080]
Loss:[0.1082]
Loss:[0.1124]
Loss:[0.1060]
Loss:[0.1065]
Loss:[0.1068]
Loss:[0.1076]
Loss:[0.1024]
Loss:[0.0978]
Loss:[0.1044]
Loss:[0.1054]
Loss:[0.1034]
Loss:[0.1021]
Loss:[0.1014]
Loss:[0.0980]
Loss:[0.1024]
Loss:[0.1041]
Loss:[0.1041]
Loss:[0.1074]
Loss:[0.0933]
Loss:[0.0947]
Loss:[0.1123]
Loss:[0.1048]
Loss:[0.1028]
Loss:[0.0991]
Loss:[0.0956]
Loss:[0.1016]
Loss:[0.0931]
Loss:[0.0969]
Loss:[0.0910]
Loss:[0.0970]
Loss:[0.0910]
Loss:[0.0892]
Loss:[0.0903]
Loss:[0.0989]
Loss:[0.0945]
Loss:[0.0990]
Loss:[0.0972]
Loss:[0.1044]
Loss:[0.0922]
Loss:[0.0964]
Loss:[0.0896]
Loss:[0.0930]
Loss:[0.0912]
Loss:[0.0905]
Loss:[0.0955]
Loss:[0.1024]
Loss:[0.0966]
Loss:[0.0924]
Loss:[0.0926]
Loss:[0.1056]
Loss:[0.0937]
Loss:[0.1037]
Early stopping!
Loading 125th epoch
acc:[0.7330]
acc:[0.7330]
acc:[0.7360]
acc:[0.7350]
acc:[0.7360]
acc:[0.7330]
acc:[0.7340]
acc:[0.7330]
acc:[0.7330]
acc:[0.7380]
acc:[0.7370]
acc:[0.7310]
acc:[0.7330]
acc:[0.7330]
acc:[0.7360]
acc:[0.7320]
acc:[0.7350]
acc:[0.7360]
acc:[0.7330]
acc:[0.7300]
acc:[0.7330]
acc:[0.7330]
acc:[0.7380]
acc:[0.7370]
acc:[0.7330]
acc:[0.7370]
acc:[0.7340]
acc:[0.7320]
acc:[0.7380]
acc:[0.7350]
acc:[0.7380]
acc:[0.7330]
acc:[0.7380]
acc:[0.7360]
acc:[0.7320]
acc:[0.7360]
acc:[0.7330]
acc:[0.7360]
acc:[0.7340]
acc:[0.7340]
acc:[0.7350]
acc:[0.7360]
acc:[0.7380]
acc:[0.7360]
acc:[0.7340]
acc:[0.7340]
acc:[0.7330]
acc:[0.7340]
acc:[0.7340]
acc:[0.7330]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7345]
Mean:[73.4540]
Std :[0.2022]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.1, gpu=5, save_name='cite_best_dgi.pkl', seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6887]
Loss:[0.6862]
Loss:[0.6772]
Loss:[0.6792]
Loss:[0.6652]
Loss:[0.6640]
Loss:[0.6513]
Loss:[0.6417]
Loss:[0.6306]
Loss:[0.6175]
Loss:[0.6045]
Loss:[0.5883]
Loss:[0.5714]
Loss:[0.5606]
Loss:[0.5373]
Loss:[0.5235]
Loss:[0.5061]
Loss:[0.4814]
Loss:[0.4667]
Loss:[0.4555]
Loss:[0.4253]
Loss:[0.4097]
Loss:[0.4010]
Loss:[0.3828]
Loss:[0.3599]
Loss:[0.3490]
Loss:[0.3422]
Loss:[0.3214]
Loss:[0.3032]
Loss:[0.3032]
Loss:[0.2902]
Loss:[0.2698]
Loss:[0.2599]
Loss:[0.2557]
Loss:[0.2490]
Loss:[0.2358]
Loss:[0.2294]
Loss:[0.2189]
Loss:[0.2079]
Loss:[0.2071]
Loss:[0.2057]
Loss:[0.1928]
Loss:[0.1899]
Loss:[0.1798]
Loss:[0.1762]
Loss:[0.1714]
Loss:[0.1695]
Loss:[0.1659]
Loss:[0.1651]
Loss:[0.1641]
Loss:[0.1537]
Loss:[0.1534]
Loss:[0.1506]
Loss:[0.1417]
Loss:[0.1380]
Loss:[0.1515]
Loss:[0.1516]
Loss:[0.1380]
Loss:[0.1336]
Loss:[0.1363]
Loss:[0.1446]
Loss:[0.1392]
Loss:[0.1314]
Loss:[0.1337]
Loss:[0.1310]
Loss:[0.1362]
Loss:[0.1279]
Loss:[0.1295]
Loss:[0.1276]
Loss:[0.1269]
Loss:[0.1179]
Loss:[0.1187]
Loss:[0.1280]
Loss:[0.1186]
Loss:[0.1170]
Loss:[0.1196]
Loss:[0.1233]
Loss:[0.1105]
Loss:[0.1105]
Loss:[0.1141]
Loss:[0.1158]
Loss:[0.1094]
Loss:[0.1097]
Loss:[0.1124]
Loss:[0.1165]
Loss:[0.1062]
Loss:[0.1085]
Loss:[0.1068]
Loss:[0.1017]
Loss:[0.1093]
Loss:[0.1038]
Loss:[0.1078]
Loss:[0.1083]
Loss:[0.1125]
Loss:[0.1058]
Loss:[0.1065]
Loss:[0.1069]
Loss:[0.1076]
Loss:[0.1025]
Loss:[0.0978]
Loss:[0.1044]
Loss:[0.1054]
Loss:[0.1034]
Loss:[0.1021]
Loss:[0.1014]
Loss:[0.0981]
Loss:[0.1024]
Loss:[0.1040]
Loss:[0.1041]
Loss:[0.1075]
Loss:[0.0933]
Loss:[0.0948]
Loss:[0.1126]
Loss:[0.1047]
Loss:[0.1028]
Loss:[0.0993]
Loss:[0.0956]
Loss:[0.1015]
Loss:[0.0931]
Loss:[0.0968]
Loss:[0.0910]
Loss:[0.0970]
Loss:[0.0911]
Loss:[0.0891]
Loss:[0.0904]
Loss:[0.0991]
Loss:[0.0947]
Loss:[0.0990]
Loss:[0.0975]
Loss:[0.1049]
Loss:[0.0922]
Loss:[0.0967]
Loss:[0.0901]
Loss:[0.0929]
Loss:[0.0917]
Loss:[0.0913]
Loss:[0.0954]
Loss:[0.1035]
Loss:[0.0992]
Loss:[0.0923]
Loss:[0.0967]
Loss:[0.1145]
Loss:[0.0935]
Loss:[0.1237]
Early stopping!
Loading 125th epoch
acc:[0.7340]
acc:[0.7340]
acc:[0.7320]
acc:[0.7340]
acc:[0.7370]
acc:[0.7330]
acc:[0.7340]
acc:[0.7340]
acc:[0.7340]
acc:[0.7380]
acc:[0.7380]
acc:[0.7310]
acc:[0.7340]
acc:[0.7330]
acc:[0.7330]
acc:[0.7330]
acc:[0.7360]
acc:[0.7360]
acc:[0.7340]
acc:[0.7310]
acc:[0.7330]
acc:[0.7330]
acc:[0.7390]
acc:[0.7370]
acc:[0.7340]
acc:[0.7360]
acc:[0.7330]
acc:[0.7320]
acc:[0.7340]
acc:[0.7330]
acc:[0.7340]
acc:[0.7350]
acc:[0.7370]
acc:[0.7360]
acc:[0.7360]
acc:[0.7340]
acc:[0.7330]
acc:[0.7350]
acc:[0.7320]
acc:[0.7360]
acc:[0.7340]
acc:[0.7360]
acc:[0.7400]
acc:[0.7350]
acc:[0.7330]
acc:[0.7340]
acc:[0.7310]
acc:[0.7330]
acc:[0.7340]
acc:[0.7330]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7344]
Mean:[73.4360]
Std :[0.1998]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.2, gpu=5, save_name='cite_best_dgi.pkl', seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6887]
Loss:[0.6862]
Loss:[0.6772]
Loss:[0.6792]
Loss:[0.6652]
Loss:[0.6640]
Loss:[0.6513]
Loss:[0.6417]
Loss:[0.6306]
Loss:[0.6175]
Loss:[0.6046]
Loss:[0.5883]
Loss:[0.5715]
Loss:[0.5606]
Loss:[0.5373]
Loss:[0.5235]
Loss:[0.5061]
Loss:[0.4814]
Loss:[0.4668]
Loss:[0.4555]
Loss:[0.4253]
Loss:[0.4098]
Loss:[0.4011]
Loss:[0.3827]
Loss:[0.3599]
Loss:[0.3491]
Loss:[0.3422]
Loss:[0.3213]
Loss:[0.3034]
Loss:[0.3033]
Loss:[0.2901]
Loss:[0.2699]
Loss:[0.2601]
Loss:[0.2557]
Loss:[0.2490]
Loss:[0.2360]
Loss:[0.2295]
Loss:[0.2188]
Loss:[0.2079]
Loss:[0.2072]
Loss:[0.2059]
Loss:[0.1927]
Loss:[0.1897]
Loss:[0.1798]
Loss:[0.1763]
Loss:[0.1715]
Loss:[0.1694]
Loss:[0.1657]
Loss:[0.1651]
Loss:[0.1642]
Loss:[0.1537]
Loss:[0.1535]
Loss:[0.1506]
Loss:[0.1417]
Loss:[0.1380]
Loss:[0.1516]
Loss:[0.1516]
Loss:[0.1380]
Loss:[0.1336]
Loss:[0.1364]
Loss:[0.1446]
Loss:[0.1392]
Loss:[0.1314]
Loss:[0.1338]
Loss:[0.1309]
Loss:[0.1362]
Loss:[0.1280]
Loss:[0.1296]
Loss:[0.1276]
Loss:[0.1269]
Loss:[0.1179]
Loss:[0.1186]
Loss:[0.1280]
Loss:[0.1186]
Loss:[0.1170]
Loss:[0.1197]
Loss:[0.1233]
Loss:[0.1106]
Loss:[0.1105]
Loss:[0.1141]
Loss:[0.1158]
Loss:[0.1095]
Loss:[0.1097]
Loss:[0.1124]
Loss:[0.1165]
Loss:[0.1062]
Loss:[0.1085]
Loss:[0.1067]
Loss:[0.1017]
Loss:[0.1093]
Loss:[0.1038]
Loss:[0.1078]
Loss:[0.1082]
Loss:[0.1124]
Loss:[0.1058]
Loss:[0.1064]
Loss:[0.1068]
Loss:[0.1076]
Loss:[0.1025]
Loss:[0.0979]
Loss:[0.1044]
Loss:[0.1055]
Loss:[0.1034]
Loss:[0.1021]
Loss:[0.1014]
Loss:[0.0981]
Loss:[0.1024]
Loss:[0.1041]
Loss:[0.1042]
Loss:[0.1076]
Loss:[0.0934]
Loss:[0.0949]
Loss:[0.1127]
Loss:[0.1047]
Loss:[0.1029]
Loss:[0.0994]
Loss:[0.0956]
Loss:[0.1015]
Loss:[0.0930]
Loss:[0.0968]
Loss:[0.0910]
Loss:[0.0971]
Loss:[0.0912]
Loss:[0.0892]
Loss:[0.0904]
Loss:[0.0992]
Loss:[0.0950]
Loss:[0.0989]
Loss:[0.0979]
Loss:[0.1055]
Loss:[0.0921]
Loss:[0.0969]
Loss:[0.0907]
Loss:[0.0930]
Loss:[0.0923]
Loss:[0.0920]
Loss:[0.0954]
Loss:[0.1047]
Loss:[0.1018]
Loss:[0.0921]
Loss:[0.1014]
Loss:[0.1246]
Loss:[0.0941]
Loss:[0.1480]
Early stopping!
Loading 125th epoch
acc:[0.7320]
acc:[0.7360]
acc:[0.7330]
acc:[0.7360]
acc:[0.7350]
acc:[0.7330]
acc:[0.7350]
acc:[0.7340]
acc:[0.7350]
acc:[0.7380]
acc:[0.7390]
acc:[0.7300]
acc:[0.7350]
acc:[0.7330]
acc:[0.7380]
acc:[0.7340]
acc:[0.7340]
acc:[0.7350]
acc:[0.7330]
acc:[0.7300]
acc:[0.7340]
acc:[0.7350]
acc:[0.7400]
acc:[0.7370]
acc:[0.7350]
acc:[0.7360]
acc:[0.7350]
acc:[0.7320]
acc:[0.7350]
acc:[0.7330]
acc:[0.7360]
acc:[0.7330]
acc:[0.7370]
acc:[0.7380]
acc:[0.7350]
acc:[0.7330]
acc:[0.7320]
acc:[0.7340]
acc:[0.7360]
acc:[0.7360]
acc:[0.7330]
acc:[0.7370]
acc:[0.7370]
acc:[0.7360]
acc:[0.7350]
acc:[0.7330]
acc:[0.7330]
acc:[0.7330]
acc:[0.7350]
acc:[0.7320]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7347]
Mean:[73.4680]
Std :[0.2123]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.3, gpu=5, save_name='cite_best_dgi.pkl', seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6887]
Loss:[0.6862]
Loss:[0.6772]
Loss:[0.6792]
Loss:[0.6652]
Loss:[0.6640]
Loss:[0.6513]
Loss:[0.6417]
Loss:[0.6306]
Loss:[0.6175]
Loss:[0.6046]
Loss:[0.5883]
Loss:[0.5715]
Loss:[0.5606]
Loss:[0.5373]
Loss:[0.5235]
Loss:[0.5061]
Loss:[0.4814]
Loss:[0.4667]
Loss:[0.4555]
Loss:[0.4253]
Loss:[0.4097]
Loss:[0.4010]
Loss:[0.3828]
Loss:[0.3599]
Loss:[0.3490]
Loss:[0.3422]
Loss:[0.3214]
Loss:[0.3032]
Loss:[0.3032]
Loss:[0.2901]
Loss:[0.2699]
Loss:[0.2599]
Loss:[0.2556]
Loss:[0.2490]
Loss:[0.2358]
Loss:[0.2294]
Loss:[0.2189]
Loss:[0.2079]
Loss:[0.2071]
Loss:[0.2057]
Loss:[0.1928]
Loss:[0.1899]
Loss:[0.1798]
Loss:[0.1762]
Loss:[0.1714]
Loss:[0.1694]
Loss:[0.1659]
Loss:[0.1652]
Loss:[0.1641]
Loss:[0.1537]
Loss:[0.1535]
Loss:[0.1506]
Loss:[0.1417]
Loss:[0.1380]
Loss:[0.1515]
Loss:[0.1517]
Loss:[0.1381]
Loss:[0.1336]
Loss:[0.1363]
Loss:[0.1445]
Loss:[0.1391]
Loss:[0.1314]
Loss:[0.1338]
Loss:[0.1309]
Loss:[0.1362]
Loss:[0.1279]
Loss:[0.1295]
Loss:[0.1276]
Loss:[0.1269]
Loss:[0.1179]
Loss:[0.1186]
Loss:[0.1280]
Loss:[0.1186]
Loss:[0.1170]
Loss:[0.1197]
Loss:[0.1234]
Loss:[0.1105]
Loss:[0.1105]
Loss:[0.1141]
Loss:[0.1158]
Loss:[0.1094]
Loss:[0.1097]
Loss:[0.1124]
Loss:[0.1165]
Loss:[0.1062]
Loss:[0.1084]
Loss:[0.1067]
Loss:[0.1016]
Loss:[0.1093]
Loss:[0.1038]
Loss:[0.1077]
Loss:[0.1082]
Loss:[0.1124]
Loss:[0.1058]
Loss:[0.1065]
Loss:[0.1068]
Loss:[0.1076]
Loss:[0.1025]
Loss:[0.0978]
Loss:[0.1044]
Loss:[0.1054]
Loss:[0.1034]
Loss:[0.1021]
Loss:[0.1014]
Loss:[0.0981]
Loss:[0.1024]
Loss:[0.1040]
Loss:[0.1041]
Loss:[0.1075]
Loss:[0.0933]
Loss:[0.0949]
Loss:[0.1127]
Loss:[0.1047]
Loss:[0.1028]
Loss:[0.0994]
Loss:[0.0956]
Loss:[0.1014]
Loss:[0.0930]
Loss:[0.0968]
Loss:[0.0909]
Loss:[0.0970]
Loss:[0.0912]
Loss:[0.0892]
Loss:[0.0904]
Loss:[0.0993]
Loss:[0.0950]
Loss:[0.0989]
Loss:[0.0979]
Loss:[0.1057]
Loss:[0.0921]
Loss:[0.0971]
Loss:[0.0908]
Loss:[0.0929]
Loss:[0.0924]
Loss:[0.0924]
Loss:[0.0954]
Loss:[0.1052]
Loss:[0.1026]
Loss:[0.0922]
Loss:[0.1033]
Loss:[0.1285]
Loss:[0.0944]
Loss:[0.1570]
Early stopping!
Loading 125th epoch
acc:[0.7340]
acc:[0.7340]
acc:[0.7360]
acc:[0.7360]
acc:[0.7350]
acc:[0.7340]
acc:[0.7360]
acc:[0.7350]
acc:[0.7330]
acc:[0.7390]
acc:[0.7380]
acc:[0.7310]
acc:[0.7360]
acc:[0.7330]
acc:[0.7350]
acc:[0.7330]
acc:[0.7370]
acc:[0.7360]
acc:[0.7340]
acc:[0.7320]
acc:[0.7360]
acc:[0.7320]
acc:[0.7340]
acc:[0.7370]
acc:[0.7330]
acc:[0.7380]
acc:[0.7320]
acc:[0.7330]
acc:[0.7360]
acc:[0.7360]
acc:[0.7350]
acc:[0.7320]
acc:[0.7350]
acc:[0.7360]
acc:[0.7330]
acc:[0.7350]
acc:[0.7350]
acc:[0.7340]
acc:[0.7350]
acc:[0.7390]
acc:[0.7360]
acc:[0.7330]
acc:[0.7380]
acc:[0.7350]
acc:[0.7340]
acc:[0.7350]
acc:[0.7340]
acc:[0.7340]
acc:[0.7330]
acc:[0.7350]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7348]
Mean:[73.4800]
Std :[0.1852]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.4, gpu=5, save_name='cite_best_dgi.pkl', seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6887]
Loss:[0.6862]
Loss:[0.6772]
Loss:[0.6792]
Loss:[0.6652]
Loss:[0.6640]
Loss:[0.6513]
Loss:[0.6417]
Loss:[0.6306]
Loss:[0.6175]
Loss:[0.6046]
Loss:[0.5883]
Loss:[0.5715]
Loss:[0.5606]
Loss:[0.5373]
Loss:[0.5235]
Loss:[0.5061]
Loss:[0.4814]
Loss:[0.4668]
Loss:[0.4554]
Loss:[0.4253]
Loss:[0.4097]
Loss:[0.4010]
Loss:[0.3828]
Loss:[0.3599]
Loss:[0.3490]
Loss:[0.3422]
Loss:[0.3214]
Loss:[0.3032]
Loss:[0.3032]
Loss:[0.2902]
Loss:[0.2698]
Loss:[0.2597]
Loss:[0.2557]
Loss:[0.2490]
Loss:[0.2358]
Loss:[0.2293]
Loss:[0.2189]
Loss:[0.2080]
Loss:[0.2070]
Loss:[0.2057]
Loss:[0.1928]
Loss:[0.1900]
Loss:[0.1799]
Loss:[0.1762]
Loss:[0.1714]
Loss:[0.1695]
Loss:[0.1660]
Loss:[0.1652]
Loss:[0.1641]
Loss:[0.1537]
Loss:[0.1535]
Loss:[0.1506]
Loss:[0.1417]
Loss:[0.1380]
Loss:[0.1515]
Loss:[0.1517]
Loss:[0.1381]
Loss:[0.1336]
Loss:[0.1365]
Loss:[0.1448]
Loss:[0.1390]
Loss:[0.1312]
Loss:[0.1338]
Loss:[0.1310]
Loss:[0.1360]
Loss:[0.1279]
Loss:[0.1298]
Loss:[0.1279]
Loss:[0.1267]
Loss:[0.1180]
Loss:[0.1191]
Loss:[0.1279]
Loss:[0.1185]
Loss:[0.1174]
Loss:[0.1197]
Loss:[0.1228]
Loss:[0.1101]
Loss:[0.1104]
Loss:[0.1141]
Loss:[0.1154]
Loss:[0.1090]
Loss:[0.1097]
Loss:[0.1122]
Loss:[0.1163]
Loss:[0.1060]
Loss:[0.1085]
Loss:[0.1071]
Loss:[0.1018]
Loss:[0.1092]
Loss:[0.1041]
Loss:[0.1083]
Loss:[0.1082]
Loss:[0.1125]
Loss:[0.1062]
Loss:[0.1066]
Loss:[0.1069]
Loss:[0.1075]
Loss:[0.1024]
Loss:[0.0979]
Loss:[0.1044]
Loss:[0.1055]
Loss:[0.1033]
Loss:[0.1022]
Loss:[0.1013]
Loss:[0.0979]
Loss:[0.1024]
Loss:[0.1042]
Loss:[0.1039]
Loss:[0.1070]
Loss:[0.0933]
Loss:[0.0947]
Loss:[0.1121]
Loss:[0.1047]
Loss:[0.1028]
Loss:[0.0990]
Loss:[0.0956]
Loss:[0.1016]
Loss:[0.0932]
Loss:[0.0969]
Loss:[0.0910]
Loss:[0.0971]
Loss:[0.0910]
Loss:[0.0892]
Loss:[0.0904]
Loss:[0.0988]
Loss:[0.0944]
Loss:[0.0989]
Loss:[0.0971]
Loss:[0.1041]
Loss:[0.0922]
Loss:[0.0962]
Loss:[0.0895]
Loss:[0.0930]
Loss:[0.0909]
Loss:[0.0902]
Loss:[0.0955]
Loss:[0.1019]
Loss:[0.0958]
Loss:[0.0926]
Loss:[0.0912]
Loss:[0.1028]
Loss:[0.0940]
Loss:[0.0985]
Early stopping!
Loading 125th epoch
acc:[0.7340]
acc:[0.7340]
acc:[0.7360]
acc:[0.7360]
acc:[0.7350]
acc:[0.7340]
acc:[0.7350]
acc:[0.7330]
acc:[0.7340]
acc:[0.7390]
acc:[0.7350]
acc:[0.7330]
acc:[0.7360]
acc:[0.7330]
acc:[0.7360]
acc:[0.7330]
acc:[0.7370]
acc:[0.7390]
acc:[0.7320]
acc:[0.7300]
acc:[0.7340]
acc:[0.7320]
acc:[0.7370]
acc:[0.7350]
acc:[0.7340]
acc:[0.7350]
acc:[0.7340]
acc:[0.7340]
acc:[0.7360]
acc:[0.7330]
acc:[0.7370]
acc:[0.7330]
acc:[0.7350]
acc:[0.7340]
acc:[0.7340]
acc:[0.7370]
acc:[0.7340]
acc:[0.7370]
acc:[0.7360]
acc:[0.7400]
acc:[0.7350]
acc:[0.7360]
acc:[0.7370]
acc:[0.7350]
acc:[0.7340]
acc:[0.7370]
acc:[0.7320]
acc:[0.7320]
acc:[0.7350]
acc:[0.7360]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7349]
Mean:[73.4880]
Std :[0.1965]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.5, gpu=5, save_name='cite_best_dgi.pkl', seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6887]
Loss:[0.6862]
Loss:[0.6772]
Loss:[0.6792]
Loss:[0.6652]
Loss:[0.6640]
Loss:[0.6513]
Loss:[0.6417]
Loss:[0.6306]
Loss:[0.6175]
Loss:[0.6046]
Loss:[0.5883]
Loss:[0.5715]
Loss:[0.5606]
Loss:[0.5373]
Loss:[0.5235]
Loss:[0.5061]
Loss:[0.4814]
Loss:[0.4668]
Loss:[0.4555]
Loss:[0.4253]
Loss:[0.4098]
Loss:[0.4012]
Loss:[0.3827]
Loss:[0.3599]
Loss:[0.3493]
Loss:[0.3423]
Loss:[0.3212]
Loss:[0.3035]
Loss:[0.3036]
Loss:[0.2899]
Loss:[0.2700]
Loss:[0.2605]
Loss:[0.2556]
Loss:[0.2488]
Loss:[0.2363]
Loss:[0.2298]
Loss:[0.2186]
Loss:[0.2077]
Loss:[0.2075]
Loss:[0.2062]
Loss:[0.1926]
Loss:[0.1893]
Loss:[0.1797]
Loss:[0.1765]
Loss:[0.1716]
Loss:[0.1692]
Loss:[0.1655]
Loss:[0.1650]
Loss:[0.1642]
Loss:[0.1537]
Loss:[0.1535]
Loss:[0.1506]
Loss:[0.1418]
Loss:[0.1381]
Loss:[0.1517]
Loss:[0.1516]
Loss:[0.1380]
Loss:[0.1336]
Loss:[0.1362]
Loss:[0.1444]
Loss:[0.1393]
Loss:[0.1318]
Loss:[0.1337]
Loss:[0.1307]
Loss:[0.1365]
Loss:[0.1281]
Loss:[0.1292]
Loss:[0.1271]
Loss:[0.1272]
Loss:[0.1180]
Loss:[0.1181]
Loss:[0.1281]
Loss:[0.1189]
Loss:[0.1166]
Loss:[0.1197]
Loss:[0.1240]
Loss:[0.1112]
Loss:[0.1107]
Loss:[0.1141]
Loss:[0.1163]
Loss:[0.1101]
Loss:[0.1097]
Loss:[0.1126]
Loss:[0.1169]
Loss:[0.1064]
Loss:[0.1084]
Loss:[0.1064]
Loss:[0.1017]
Loss:[0.1094]
Loss:[0.1035]
Loss:[0.1072]
Loss:[0.1082]
Loss:[0.1123]
Loss:[0.1053]
Loss:[0.1063]
Loss:[0.1069]
Loss:[0.1079]
Loss:[0.1027]
Loss:[0.0978]
Loss:[0.1044]
Loss:[0.1056]
Loss:[0.1035]
Loss:[0.1021]
Loss:[0.1014]
Loss:[0.0984]
Loss:[0.1024]
Loss:[0.1039]
Loss:[0.1044]
Loss:[0.1083]
Loss:[0.0934]
Loss:[0.0952]
Loss:[0.1137]
Loss:[0.1044]
Loss:[0.1031]
Loss:[0.1003]
Loss:[0.0956]
Loss:[0.1014]
Loss:[0.0930]
Loss:[0.0967]
Loss:[0.0909]
Loss:[0.0972]
Loss:[0.0917]
Loss:[0.0892]
Loss:[0.0906]
Loss:[0.1003]
Loss:[0.0960]
Loss:[0.0990]
Loss:[0.0999]
Loss:[0.1085]
Loss:[0.0918]
Loss:[0.1003]
Loss:[0.0956]
Loss:[0.0929]
Loss:[0.1003]
Loss:[0.1045]
Loss:[0.0963]
Loss:[0.1337]
Loss:[0.1559]
Loss:[0.1045]
Loss:[0.1994]
Loss:[0.2275]
Loss:[0.2038]
Loss:[0.1254]
Early stopping!
Loading 125th epoch
acc:[0.7340]
acc:[0.7340]
acc:[0.7350]
acc:[0.7360]
acc:[0.7340]
acc:[0.7360]
acc:[0.7330]
acc:[0.7350]
acc:[0.7360]
acc:[0.7360]
acc:[0.7360]
acc:[0.7340]
acc:[0.7350]
acc:[0.7350]
acc:[0.7340]
acc:[0.7340]
acc:[0.7380]
acc:[0.7350]
acc:[0.7350]
acc:[0.7330]
acc:[0.7340]
acc:[0.7320]
acc:[0.7370]
acc:[0.7340]
acc:[0.7330]
acc:[0.7380]
acc:[0.7350]
acc:[0.7330]
acc:[0.7360]
acc:[0.7360]
acc:[0.7370]
acc:[0.7330]
acc:[0.7350]
acc:[0.7370]
acc:[0.7330]
acc:[0.7360]
acc:[0.7330]
acc:[0.7370]
acc:[0.7350]
acc:[0.7360]
acc:[0.7330]
acc:[0.7350]
acc:[0.7370]
acc:[0.7360]
acc:[0.7330]
acc:[0.7330]
acc:[0.7340]
acc:[0.7330]
acc:[0.7340]
acc:[0.7330]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7348]
Mean:[73.4780]
Std :[0.1516]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='node', dataset='citeseer', drop_percent=0.6, gpu=5, save_name='cite_best_dgi.pkl', seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[node]
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6887]
Loss:[0.6862]
Loss:[0.6772]
Loss:[0.6792]
Loss:[0.6652]
Loss:[0.6640]
Loss:[0.6513]
Loss:[0.6417]
Loss:[0.6306]
Loss:[0.6175]
Loss:[0.6046]
Loss:[0.5883]
Loss:[0.5715]
Loss:[0.5606]
Loss:[0.5373]
Loss:[0.5235]
Loss:[0.5061]
Loss:[0.4814]
Loss:[0.4667]
Loss:[0.4555]
Loss:[0.4253]
Loss:[0.4097]
Loss:[0.4010]
Loss:[0.3828]
Loss:[0.3599]
Loss:[0.3490]
Loss:[0.3423]
Loss:[0.3214]
Loss:[0.3032]
Loss:[0.3032]
Loss:[0.2902]
Loss:[0.2698]
Loss:[0.2598]
Loss:[0.2557]
Loss:[0.2490]
Loss:[0.2358]
Loss:[0.2293]
Loss:[0.2189]
Loss:[0.2079]
Loss:[0.2071]
Loss:[0.2057]
Loss:[0.1928]
Loss:[0.1899]
Loss:[0.1799]
Loss:[0.1762]
Loss:[0.1714]
Loss:[0.1695]
Loss:[0.1660]
Loss:[0.1652]
Loss:[0.1641]
Loss:[0.1537]
Loss:[0.1535]
Loss:[0.1506]
Loss:[0.1417]
Loss:[0.1380]
Loss:[0.1515]
Loss:[0.1516]
Loss:[0.1381]
Loss:[0.1336]
Loss:[0.1364]
Loss:[0.1447]
Loss:[0.1392]
Loss:[0.1313]
Loss:[0.1338]
Loss:[0.1310]
Loss:[0.1362]
Loss:[0.1278]
Loss:[0.1296]
Loss:[0.1277]
Loss:[0.1269]
Loss:[0.1179]
Loss:[0.1188]
Loss:[0.1279]
Loss:[0.1185]
Loss:[0.1171]
Loss:[0.1197]
Loss:[0.1231]
Loss:[0.1104]
Loss:[0.1105]
Loss:[0.1141]
Loss:[0.1156]
Loss:[0.1092]
Loss:[0.1097]
Loss:[0.1123]
Loss:[0.1164]
Loss:[0.1060]
Loss:[0.1084]
Loss:[0.1069]
Loss:[0.1017]
Loss:[0.1092]
Loss:[0.1040]
Loss:[0.1079]
Loss:[0.1082]
Loss:[0.1125]
Loss:[0.1060]
Loss:[0.1065]
Loss:[0.1068]
Loss:[0.1076]
Loss:[0.1024]
Loss:[0.0979]
Loss:[0.1045]
Loss:[0.1054]
Loss:[0.1034]
Loss:[0.1022]
Loss:[0.1014]
Loss:[0.0980]
Loss:[0.1024]
Loss:[0.1041]
Loss:[0.1040]
Loss:[0.1072]
Loss:[0.0933]
Loss:[0.0948]
Loss:[0.1122]
Loss:[0.1047]
Loss:[0.1027]
Loss:[0.0991]
Loss:[0.0956]
Loss:[0.1015]
Loss:[0.0930]
Loss:[0.0969]
Loss:[0.0910]
Loss:[0.0970]
Loss:[0.0910]
Loss:[0.0891]
Loss:[0.0903]
Loss:[0.0988]
Loss:[0.0945]
Loss:[0.0989]
Loss:[0.0972]
Loss:[0.1043]
Loss:[0.0921]
Loss:[0.0962]
Loss:[0.0897]
Loss:[0.0930]
Loss:[0.0911]
Loss:[0.0905]
Loss:[0.0955]
Loss:[0.1023]
Loss:[0.0964]
Loss:[0.0924]
Loss:[0.0923]
Loss:[0.1051]
Loss:[0.0938]
Loss:[0.1024]
Early stopping!
Loading 125th epoch
acc:[0.7350]
acc:[0.7330]
acc:[0.7360]
acc:[0.7360]
acc:[0.7370]
acc:[0.7350]
acc:[0.7370]
acc:[0.7340]
acc:[0.7340]
acc:[0.7380]
acc:[0.7380]
acc:[0.7320]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7340]
acc:[0.7350]
acc:[0.7380]
acc:[0.7330]
acc:[0.7320]
acc:[0.7330]
acc:[0.7340]
acc:[0.7350]
acc:[0.7360]
acc:[0.7330]
acc:[0.7350]
acc:[0.7360]
acc:[0.7340]
acc:[0.7360]
acc:[0.7350]
acc:[0.7370]
acc:[0.7330]
acc:[0.7340]
acc:[0.7350]
acc:[0.7350]
acc:[0.7360]
acc:[0.7340]
acc:[0.7370]
acc:[0.7350]
acc:[0.7350]
acc:[0.7330]
acc:[0.7350]
acc:[0.7370]
acc:[0.7380]
acc:[0.7360]
acc:[0.7340]
acc:[0.7340]
acc:[0.7320]
acc:[0.7340]
acc:[0.7360]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7350]
Mean:[73.4980]
Std :[0.1597]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.02, gpu=5, save_name='cite_best_dgi.pkl', seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6887]
Loss:[0.6862]
Loss:[0.6772]
Loss:[0.6792]
Loss:[0.6652]
Loss:[0.6640]
Loss:[0.6513]
Loss:[0.6417]
Loss:[0.6306]
Loss:[0.6175]
Loss:[0.6046]
Loss:[0.5883]
Loss:[0.5715]
Loss:[0.5606]
Loss:[0.5373]
Loss:[0.5235]
Loss:[0.5061]
Loss:[0.4814]
Loss:[0.4667]
Loss:[0.4555]
Loss:[0.4253]
Loss:[0.4097]
Loss:[0.4010]
Loss:[0.3828]
Loss:[0.3599]
Loss:[0.3489]
Loss:[0.3422]
Loss:[0.3215]
Loss:[0.3032]
Loss:[0.3032]
Loss:[0.2902]
Loss:[0.2698]
Loss:[0.2598]
Loss:[0.2557]
Loss:[0.2491]
Loss:[0.2358]
Loss:[0.2293]
Loss:[0.2189]
Loss:[0.2079]
Loss:[0.2071]
Loss:[0.2057]
Loss:[0.1928]
Loss:[0.1899]
Loss:[0.1798]
Loss:[0.1762]
Loss:[0.1714]
Loss:[0.1695]
Loss:[0.1660]
Loss:[0.1652]
Loss:[0.1641]
Loss:[0.1537]
Loss:[0.1535]
Loss:[0.1506]
Loss:[0.1417]
Loss:[0.1380]
Loss:[0.1515]
Loss:[0.1516]
Loss:[0.1380]
Loss:[0.1336]
Loss:[0.1364]
Loss:[0.1445]
Loss:[0.1391]
Loss:[0.1314]
Loss:[0.1338]
Loss:[0.1309]
Loss:[0.1361]
Loss:[0.1280]
Loss:[0.1297]
Loss:[0.1278]
Loss:[0.1269]
Loss:[0.1179]
Loss:[0.1187]
Loss:[0.1279]
Loss:[0.1186]
Loss:[0.1171]
Loss:[0.1197]
Loss:[0.1232]
Loss:[0.1104]
Loss:[0.1105]
Loss:[0.1141]
Loss:[0.1157]
Loss:[0.1093]
Loss:[0.1097]
Loss:[0.1123]
Loss:[0.1165]
Loss:[0.1061]
Loss:[0.1085]
Loss:[0.1068]
Loss:[0.1017]
Loss:[0.1092]
Loss:[0.1038]
Loss:[0.1079]
Loss:[0.1083]
Loss:[0.1124]
Loss:[0.1059]
Loss:[0.1065]
Loss:[0.1068]
Loss:[0.1076]
Loss:[0.1025]
Loss:[0.0979]
Loss:[0.1045]
Loss:[0.1054]
Loss:[0.1034]
Loss:[0.1022]
Loss:[0.1015]
Loss:[0.0981]
Loss:[0.1024]
Loss:[0.1041]
Loss:[0.1041]
Loss:[0.1075]
Loss:[0.0933]
Loss:[0.0949]
Loss:[0.1127]
Loss:[0.1046]
Loss:[0.1029]
Loss:[0.0994]
Loss:[0.0956]
Loss:[0.1015]
Loss:[0.0930]
Loss:[0.0968]
Loss:[0.0910]
Loss:[0.0971]
Loss:[0.0912]
Loss:[0.0892]
Loss:[0.0905]
Loss:[0.0993]
Loss:[0.0951]
Loss:[0.0990]
Loss:[0.0979]
Loss:[0.1055]
Loss:[0.0922]
Loss:[0.0970]
Loss:[0.0907]
Loss:[0.0930]
Loss:[0.0923]
Loss:[0.0920]
Loss:[0.0954]
Loss:[0.1048]
Loss:[0.1018]
Loss:[0.0921]
Loss:[0.1015]
Loss:[0.1248]
Loss:[0.0940]
Loss:[0.1479]
Early stopping!
Loading 125th epoch
acc:[0.7340]
acc:[0.7320]
acc:[0.7340]
acc:[0.7340]
acc:[0.7330]
acc:[0.7350]
acc:[0.7330]
acc:[0.7340]
acc:[0.7340]
acc:[0.7390]
acc:[0.7380]
acc:[0.7330]
acc:[0.7330]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7360]
acc:[0.7360]
acc:[0.7320]
acc:[0.7340]
acc:[0.7330]
acc:[0.7370]
acc:[0.7350]
acc:[0.7340]
acc:[0.7330]
acc:[0.7340]
acc:[0.7330]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7360]
acc:[0.7370]
acc:[0.7360]
acc:[0.7340]
acc:[0.7340]
acc:[0.7350]
acc:[0.7330]
acc:[0.7350]
acc:[0.7330]
acc:[0.7340]
acc:[0.7360]
acc:[0.7330]
acc:[0.7340]
acc:[0.7350]
acc:[0.7330]
acc:[0.7330]
acc:[0.7340]
acc:[0.7340]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7344]
Mean:[73.4440]
Std :[0.1445]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.05, gpu=5, save_name='cite_best_dgi.pkl', seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6887]
Loss:[0.6862]
Loss:[0.6772]
Loss:[0.6792]
Loss:[0.6652]
Loss:[0.6640]
Loss:[0.6513]
Loss:[0.6417]
Loss:[0.6306]
Loss:[0.6175]
Loss:[0.6046]
Loss:[0.5883]
Loss:[0.5715]
Loss:[0.5606]
Loss:[0.5373]
Loss:[0.5235]
Loss:[0.5061]
Loss:[0.4814]
Loss:[0.4667]
Loss:[0.4555]
Loss:[0.4253]
Loss:[0.4097]
Loss:[0.4010]
Loss:[0.3828]
Loss:[0.3599]
Loss:[0.3489]
Loss:[0.3422]
Loss:[0.3215]
Loss:[0.3032]
Loss:[0.3032]
Loss:[0.2902]
Loss:[0.2698]
Loss:[0.2598]
Loss:[0.2557]
Loss:[0.2491]
Loss:[0.2358]
Loss:[0.2293]
Loss:[0.2189]
Loss:[0.2079]
Loss:[0.2071]
Loss:[0.2057]
Loss:[0.1928]
Loss:[0.1899]
Loss:[0.1798]
Loss:[0.1762]
Loss:[0.1714]
Loss:[0.1695]
Loss:[0.1660]
Loss:[0.1652]
Loss:[0.1641]
Loss:[0.1537]
Loss:[0.1535]
Loss:[0.1506]
Loss:[0.1417]
Loss:[0.1380]
Loss:[0.1515]
Loss:[0.1516]
Loss:[0.1380]
Loss:[0.1336]
Loss:[0.1364]
Loss:[0.1445]
Loss:[0.1391]
Loss:[0.1314]
Loss:[0.1338]
Loss:[0.1309]
Loss:[0.1361]
Loss:[0.1280]
Loss:[0.1297]
Loss:[0.1278]
Loss:[0.1269]
Loss:[0.1179]
Loss:[0.1187]
Loss:[0.1279]
Loss:[0.1186]
Loss:[0.1171]
Loss:[0.1197]
Loss:[0.1232]
Loss:[0.1104]
Loss:[0.1105]
Loss:[0.1141]
Loss:[0.1157]
Loss:[0.1093]
Loss:[0.1097]
Loss:[0.1123]
Loss:[0.1165]
Loss:[0.1061]
Loss:[0.1085]
Loss:[0.1068]
Loss:[0.1017]
Loss:[0.1092]
Loss:[0.1038]
Loss:[0.1079]
Loss:[0.1083]
Loss:[0.1124]
Loss:[0.1059]
Loss:[0.1065]
Loss:[0.1068]
Loss:[0.1076]
Loss:[0.1025]
Loss:[0.0979]
Loss:[0.1045]
Loss:[0.1054]
Loss:[0.1034]
Loss:[0.1022]
Loss:[0.1015]
Loss:[0.0981]
Loss:[0.1024]
Loss:[0.1041]
Loss:[0.1041]
Loss:[0.1075]
Loss:[0.0933]
Loss:[0.0949]
Loss:[0.1127]
Loss:[0.1046]
Loss:[0.1029]
Loss:[0.0994]
Loss:[0.0956]
Loss:[0.1015]
Loss:[0.0930]
Loss:[0.0968]
Loss:[0.0910]
Loss:[0.0971]
Loss:[0.0912]
Loss:[0.0892]
Loss:[0.0905]
Loss:[0.0993]
Loss:[0.0951]
Loss:[0.0990]
Loss:[0.0979]
Loss:[0.1055]
Loss:[0.0922]
Loss:[0.0970]
Loss:[0.0907]
Loss:[0.0930]
Loss:[0.0923]
Loss:[0.0920]
Loss:[0.0954]
Loss:[0.1048]
Loss:[0.1018]
Loss:[0.0921]
Loss:[0.1015]
Loss:[0.1248]
Loss:[0.0940]
Loss:[0.1479]
Early stopping!
Loading 125th epoch
acc:[0.7340]
acc:[0.7320]
acc:[0.7340]
acc:[0.7340]
acc:[0.7330]
acc:[0.7350]
acc:[0.7330]
acc:[0.7340]
acc:[0.7340]
acc:[0.7390]
acc:[0.7380]
acc:[0.7330]
acc:[0.7330]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7360]
acc:[0.7360]
acc:[0.7320]
acc:[0.7340]
acc:[0.7330]
acc:[0.7370]
acc:[0.7350]
acc:[0.7340]
acc:[0.7330]
acc:[0.7340]
acc:[0.7330]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7360]
acc:[0.7370]
acc:[0.7360]
acc:[0.7340]
acc:[0.7340]
acc:[0.7350]
acc:[0.7330]
acc:[0.7350]
acc:[0.7330]
acc:[0.7340]
acc:[0.7360]
acc:[0.7330]
acc:[0.7340]
acc:[0.7350]
acc:[0.7330]
acc:[0.7330]
acc:[0.7340]
acc:[0.7340]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7344]
Mean:[73.4440]
Std :[0.1445]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.08, gpu=5, save_name='cite_best_dgi.pkl', seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6887]
Loss:[0.6862]
Loss:[0.6772]
Loss:[0.6792]
Loss:[0.6652]
Loss:[0.6640]
Loss:[0.6513]
Loss:[0.6417]
Loss:[0.6306]
Loss:[0.6175]
Loss:[0.6046]
Loss:[0.5883]
Loss:[0.5715]
Loss:[0.5606]
Loss:[0.5373]
Loss:[0.5235]
Loss:[0.5061]
Loss:[0.4814]
Loss:[0.4667]
Loss:[0.4555]
Loss:[0.4253]
Loss:[0.4097]
Loss:[0.4010]
Loss:[0.3828]
Loss:[0.3599]
Loss:[0.3489]
Loss:[0.3422]
Loss:[0.3215]
Loss:[0.3032]
Loss:[0.3032]
Loss:[0.2902]
Loss:[0.2698]
Loss:[0.2598]
Loss:[0.2557]
Loss:[0.2491]
Loss:[0.2358]
Loss:[0.2293]
Loss:[0.2189]
Loss:[0.2079]
Loss:[0.2071]
Loss:[0.2057]
Loss:[0.1928]
Loss:[0.1899]
Loss:[0.1798]
Loss:[0.1762]
Loss:[0.1714]
Loss:[0.1695]
Loss:[0.1660]
Loss:[0.1652]
Loss:[0.1641]
Loss:[0.1537]
Loss:[0.1535]
Loss:[0.1506]
Loss:[0.1417]
Loss:[0.1380]
Loss:[0.1515]
Loss:[0.1516]
Loss:[0.1380]
Loss:[0.1336]
Loss:[0.1364]
Loss:[0.1445]
Loss:[0.1391]
Loss:[0.1314]
Loss:[0.1338]
Loss:[0.1309]
Loss:[0.1361]
Loss:[0.1280]
Loss:[0.1297]
Loss:[0.1278]
Loss:[0.1269]
Loss:[0.1179]
Loss:[0.1187]
Loss:[0.1279]
Loss:[0.1186]
Loss:[0.1171]
Loss:[0.1197]
Loss:[0.1232]
Loss:[0.1104]
Loss:[0.1105]
Loss:[0.1141]
Loss:[0.1157]
Loss:[0.1093]
Loss:[0.1097]
Loss:[0.1123]
Loss:[0.1165]
Loss:[0.1061]
Loss:[0.1085]
Loss:[0.1068]
Loss:[0.1017]
Loss:[0.1092]
Loss:[0.1038]
Loss:[0.1079]
Loss:[0.1083]
Loss:[0.1124]
Loss:[0.1059]
Loss:[0.1065]
Loss:[0.1068]
Loss:[0.1076]
Loss:[0.1025]
Loss:[0.0979]
Loss:[0.1045]
Loss:[0.1054]
Loss:[0.1034]
Loss:[0.1022]
Loss:[0.1015]
Loss:[0.0981]
Loss:[0.1024]
Loss:[0.1041]
Loss:[0.1041]
Loss:[0.1075]
Loss:[0.0933]
Loss:[0.0949]
Loss:[0.1127]
Loss:[0.1046]
Loss:[0.1029]
Loss:[0.0994]
Loss:[0.0956]
Loss:[0.1015]
Loss:[0.0930]
Loss:[0.0968]
Loss:[0.0910]
Loss:[0.0971]
Loss:[0.0912]
Loss:[0.0892]
Loss:[0.0905]
Loss:[0.0993]
Loss:[0.0951]
Loss:[0.0990]
Loss:[0.0979]
Loss:[0.1055]
Loss:[0.0922]
Loss:[0.0970]
Loss:[0.0907]
Loss:[0.0930]
Loss:[0.0923]
Loss:[0.0920]
Loss:[0.0954]
Loss:[0.1048]
Loss:[0.1018]
Loss:[0.0921]
Loss:[0.1015]
Loss:[0.1248]
Loss:[0.0940]
Loss:[0.1479]
Early stopping!
Loading 125th epoch
acc:[0.7340]
acc:[0.7320]
acc:[0.7340]
acc:[0.7340]
acc:[0.7330]
acc:[0.7350]
acc:[0.7330]
acc:[0.7340]
acc:[0.7340]
acc:[0.7390]
acc:[0.7380]
acc:[0.7330]
acc:[0.7330]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7360]
acc:[0.7360]
acc:[0.7320]
acc:[0.7340]
acc:[0.7330]
acc:[0.7370]
acc:[0.7350]
acc:[0.7340]
acc:[0.7330]
acc:[0.7340]
acc:[0.7330]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7360]
acc:[0.7370]
acc:[0.7360]
acc:[0.7340]
acc:[0.7340]
acc:[0.7350]
acc:[0.7330]
acc:[0.7350]
acc:[0.7330]
acc:[0.7340]
acc:[0.7360]
acc:[0.7330]
acc:[0.7340]
acc:[0.7350]
acc:[0.7330]
acc:[0.7330]
acc:[0.7340]
acc:[0.7340]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7344]
Mean:[73.4440]
Std :[0.1445]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.1, gpu=5, save_name='cite_best_dgi.pkl', seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6887]
Loss:[0.6862]
Loss:[0.6772]
Loss:[0.6792]
Loss:[0.6652]
Loss:[0.6640]
Loss:[0.6513]
Loss:[0.6417]
Loss:[0.6306]
Loss:[0.6175]
Loss:[0.6046]
Loss:[0.5883]
Loss:[0.5715]
Loss:[0.5606]
Loss:[0.5373]
Loss:[0.5235]
Loss:[0.5061]
Loss:[0.4814]
Loss:[0.4667]
Loss:[0.4555]
Loss:[0.4253]
Loss:[0.4097]
Loss:[0.4010]
Loss:[0.3828]
Loss:[0.3599]
Loss:[0.3489]
Loss:[0.3422]
Loss:[0.3215]
Loss:[0.3032]
Loss:[0.3032]
Loss:[0.2902]
Loss:[0.2698]
Loss:[0.2598]
Loss:[0.2557]
Loss:[0.2491]
Loss:[0.2358]
Loss:[0.2293]
Loss:[0.2189]
Loss:[0.2079]
Loss:[0.2071]
Loss:[0.2057]
Loss:[0.1928]
Loss:[0.1899]
Loss:[0.1798]
Loss:[0.1762]
Loss:[0.1714]
Loss:[0.1695]
Loss:[0.1660]
Loss:[0.1652]
Loss:[0.1641]
Loss:[0.1537]
Loss:[0.1535]
Loss:[0.1506]
Loss:[0.1417]
Loss:[0.1380]
Loss:[0.1515]
Loss:[0.1516]
Loss:[0.1380]
Loss:[0.1336]
Loss:[0.1364]
Loss:[0.1445]
Loss:[0.1391]
Loss:[0.1314]
Loss:[0.1338]
Loss:[0.1309]
Loss:[0.1361]
Loss:[0.1280]
Loss:[0.1297]
Loss:[0.1278]
Loss:[0.1269]
Loss:[0.1179]
Loss:[0.1187]
Loss:[0.1279]
Loss:[0.1186]
Loss:[0.1171]
Loss:[0.1197]
Loss:[0.1232]
Loss:[0.1104]
Loss:[0.1105]
Loss:[0.1141]
Loss:[0.1157]
Loss:[0.1093]
Loss:[0.1097]
Loss:[0.1123]
Loss:[0.1165]
Loss:[0.1061]
Loss:[0.1085]
Loss:[0.1068]
Loss:[0.1017]
Loss:[0.1092]
Loss:[0.1038]
Loss:[0.1079]
Loss:[0.1083]
Loss:[0.1124]
Loss:[0.1059]
Loss:[0.1065]
Loss:[0.1068]
Loss:[0.1076]
Loss:[0.1025]
Loss:[0.0979]
Loss:[0.1045]
Loss:[0.1054]
Loss:[0.1034]
Loss:[0.1022]
Loss:[0.1015]
Loss:[0.0981]
Loss:[0.1024]
Loss:[0.1041]
Loss:[0.1041]
Loss:[0.1075]
Loss:[0.0933]
Loss:[0.0949]
Loss:[0.1127]
Loss:[0.1046]
Loss:[0.1029]
Loss:[0.0994]
Loss:[0.0956]
Loss:[0.1015]
Loss:[0.0930]
Loss:[0.0968]
Loss:[0.0910]
Loss:[0.0971]
Loss:[0.0912]
Loss:[0.0892]
Loss:[0.0905]
Loss:[0.0993]
Loss:[0.0951]
Loss:[0.0990]
Loss:[0.0979]
Loss:[0.1055]
Loss:[0.0922]
Loss:[0.0970]
Loss:[0.0907]
Loss:[0.0930]
Loss:[0.0923]
Loss:[0.0920]
Loss:[0.0954]
Loss:[0.1048]
Loss:[0.1018]
Loss:[0.0921]
Loss:[0.1015]
Loss:[0.1248]
Loss:[0.0940]
Loss:[0.1479]
Early stopping!
Loading 125th epoch
acc:[0.7340]
acc:[0.7320]
acc:[0.7340]
acc:[0.7340]
acc:[0.7330]
acc:[0.7350]
acc:[0.7330]
acc:[0.7340]
acc:[0.7340]
acc:[0.7390]
acc:[0.7380]
acc:[0.7330]
acc:[0.7330]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7360]
acc:[0.7360]
acc:[0.7320]
acc:[0.7340]
acc:[0.7330]
acc:[0.7370]
acc:[0.7350]
acc:[0.7340]
acc:[0.7330]
acc:[0.7340]
acc:[0.7330]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7360]
acc:[0.7370]
acc:[0.7360]
acc:[0.7340]
acc:[0.7340]
acc:[0.7350]
acc:[0.7330]
acc:[0.7350]
acc:[0.7330]
acc:[0.7340]
acc:[0.7360]
acc:[0.7330]
acc:[0.7340]
acc:[0.7350]
acc:[0.7330]
acc:[0.7330]
acc:[0.7340]
acc:[0.7340]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7344]
Mean:[73.4440]
Std :[0.1445]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.2, gpu=5, save_name='cite_best_dgi.pkl', seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6887]
Loss:[0.6862]
Loss:[0.6772]
Loss:[0.6792]
Loss:[0.6652]
Loss:[0.6640]
Loss:[0.6513]
Loss:[0.6417]
Loss:[0.6306]
Loss:[0.6175]
Loss:[0.6046]
Loss:[0.5883]
Loss:[0.5715]
Loss:[0.5606]
Loss:[0.5373]
Loss:[0.5235]
Loss:[0.5061]
Loss:[0.4814]
Loss:[0.4667]
Loss:[0.4555]
Loss:[0.4253]
Loss:[0.4097]
Loss:[0.4010]
Loss:[0.3828]
Loss:[0.3599]
Loss:[0.3489]
Loss:[0.3422]
Loss:[0.3215]
Loss:[0.3032]
Loss:[0.3032]
Loss:[0.2902]
Loss:[0.2698]
Loss:[0.2598]
Loss:[0.2557]
Loss:[0.2491]
Loss:[0.2358]
Loss:[0.2293]
Loss:[0.2189]
Loss:[0.2079]
Loss:[0.2071]
Loss:[0.2057]
Loss:[0.1928]
Loss:[0.1899]
Loss:[0.1798]
Loss:[0.1762]
Loss:[0.1714]
Loss:[0.1695]
Loss:[0.1660]
Loss:[0.1652]
Loss:[0.1641]
Loss:[0.1537]
Loss:[0.1535]
Loss:[0.1506]
Loss:[0.1417]
Loss:[0.1380]
Loss:[0.1515]
Loss:[0.1516]
Loss:[0.1380]
Loss:[0.1336]
Loss:[0.1364]
Loss:[0.1445]
Loss:[0.1391]
Loss:[0.1314]
Loss:[0.1338]
Loss:[0.1309]
Loss:[0.1361]
Loss:[0.1280]
Loss:[0.1297]
Loss:[0.1278]
Loss:[0.1269]
Loss:[0.1179]
Loss:[0.1187]
Loss:[0.1279]
Loss:[0.1186]
Loss:[0.1171]
Loss:[0.1197]
Loss:[0.1232]
Loss:[0.1104]
Loss:[0.1105]
Loss:[0.1141]
Loss:[0.1157]
Loss:[0.1093]
Loss:[0.1097]
Loss:[0.1123]
Loss:[0.1165]
Loss:[0.1061]
Loss:[0.1085]
Loss:[0.1068]
Loss:[0.1017]
Loss:[0.1092]
Loss:[0.1038]
Loss:[0.1079]
Loss:[0.1083]
Loss:[0.1124]
Loss:[0.1059]
Loss:[0.1065]
Loss:[0.1068]
Loss:[0.1076]
Loss:[0.1025]
Loss:[0.0979]
Loss:[0.1045]
Loss:[0.1054]
Loss:[0.1034]
Loss:[0.1022]
Loss:[0.1015]
Loss:[0.0981]
Loss:[0.1024]
Loss:[0.1041]
Loss:[0.1041]
Loss:[0.1075]
Loss:[0.0933]
Loss:[0.0949]
Loss:[0.1127]
Loss:[0.1046]
Loss:[0.1029]
Loss:[0.0994]
Loss:[0.0956]
Loss:[0.1015]
Loss:[0.0930]
Loss:[0.0968]
Loss:[0.0910]
Loss:[0.0971]
Loss:[0.0912]
Loss:[0.0892]
Loss:[0.0905]
Loss:[0.0993]
Loss:[0.0951]
Loss:[0.0990]
Loss:[0.0979]
Loss:[0.1055]
Loss:[0.0922]
Loss:[0.0970]
Loss:[0.0907]
Loss:[0.0930]
Loss:[0.0923]
Loss:[0.0920]
Loss:[0.0954]
Loss:[0.1048]
Loss:[0.1018]
Loss:[0.0921]
Loss:[0.1015]
Loss:[0.1248]
Loss:[0.0940]
Loss:[0.1479]
Early stopping!
Loading 125th epoch
acc:[0.7340]
acc:[0.7320]
acc:[0.7340]
acc:[0.7340]
acc:[0.7330]
acc:[0.7350]
acc:[0.7330]
acc:[0.7340]
acc:[0.7340]
acc:[0.7390]
acc:[0.7380]
acc:[0.7330]
acc:[0.7330]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7360]
acc:[0.7360]
acc:[0.7320]
acc:[0.7340]
acc:[0.7330]
acc:[0.7370]
acc:[0.7350]
acc:[0.7340]
acc:[0.7330]
acc:[0.7340]
acc:[0.7330]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7360]
acc:[0.7370]
acc:[0.7360]
acc:[0.7340]
acc:[0.7340]
acc:[0.7350]
acc:[0.7330]
acc:[0.7350]
acc:[0.7330]
acc:[0.7340]
acc:[0.7360]
acc:[0.7330]
acc:[0.7340]
acc:[0.7350]
acc:[0.7330]
acc:[0.7330]
acc:[0.7340]
acc:[0.7340]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7344]
Mean:[73.4440]
Std :[0.1445]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.3, gpu=5, save_name='cite_best_dgi.pkl', seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6887]
Loss:[0.6862]
Loss:[0.6772]
Loss:[0.6792]
Loss:[0.6652]
Loss:[0.6640]
Loss:[0.6513]
Loss:[0.6417]
Loss:[0.6306]
Loss:[0.6175]
Loss:[0.6046]
Loss:[0.5883]
Loss:[0.5715]
Loss:[0.5606]
Loss:[0.5373]
Loss:[0.5235]
Loss:[0.5061]
Loss:[0.4814]
Loss:[0.4667]
Loss:[0.4555]
Loss:[0.4253]
Loss:[0.4097]
Loss:[0.4010]
Loss:[0.3828]
Loss:[0.3599]
Loss:[0.3489]
Loss:[0.3422]
Loss:[0.3215]
Loss:[0.3032]
Loss:[0.3032]
Loss:[0.2902]
Loss:[0.2698]
Loss:[0.2598]
Loss:[0.2557]
Loss:[0.2491]
Loss:[0.2358]
Loss:[0.2293]
Loss:[0.2189]
Loss:[0.2079]
Loss:[0.2071]
Loss:[0.2057]
Loss:[0.1928]
Loss:[0.1899]
Loss:[0.1798]
Loss:[0.1762]
Loss:[0.1714]
Loss:[0.1695]
Loss:[0.1660]
Loss:[0.1652]
Loss:[0.1641]
Loss:[0.1537]
Loss:[0.1535]
Loss:[0.1506]
Loss:[0.1417]
Loss:[0.1380]
Loss:[0.1515]
Loss:[0.1516]
Loss:[0.1380]
Loss:[0.1336]
Loss:[0.1364]
Loss:[0.1445]
Loss:[0.1391]
Loss:[0.1314]
Loss:[0.1338]
Loss:[0.1309]
Loss:[0.1361]
Loss:[0.1280]
Loss:[0.1297]
Loss:[0.1278]
Loss:[0.1269]
Loss:[0.1179]
Loss:[0.1187]
Loss:[0.1279]
Loss:[0.1186]
Loss:[0.1171]
Loss:[0.1197]
Loss:[0.1232]
Loss:[0.1104]
Loss:[0.1105]
Loss:[0.1141]
Loss:[0.1157]
Loss:[0.1093]
Loss:[0.1097]
Loss:[0.1123]
Loss:[0.1165]
Loss:[0.1061]
Loss:[0.1085]
Loss:[0.1068]
Loss:[0.1017]
Loss:[0.1092]
Loss:[0.1038]
Loss:[0.1079]
Loss:[0.1083]
Loss:[0.1124]
Loss:[0.1059]
Loss:[0.1065]
Loss:[0.1068]
Loss:[0.1076]
Loss:[0.1025]
Loss:[0.0979]
Loss:[0.1045]
Loss:[0.1054]
Loss:[0.1034]
Loss:[0.1022]
Loss:[0.1015]
Loss:[0.0981]
Loss:[0.1024]
Loss:[0.1041]
Loss:[0.1041]
Loss:[0.1075]
Loss:[0.0933]
Loss:[0.0949]
Loss:[0.1127]
Loss:[0.1046]
Loss:[0.1029]
Loss:[0.0994]
Loss:[0.0956]
Loss:[0.1015]
Loss:[0.0930]
Loss:[0.0968]
Loss:[0.0910]
Loss:[0.0971]
Loss:[0.0912]
Loss:[0.0892]
Loss:[0.0905]
Loss:[0.0993]
Loss:[0.0951]
Loss:[0.0990]
Loss:[0.0979]
Loss:[0.1055]
Loss:[0.0922]
Loss:[0.0970]
Loss:[0.0907]
Loss:[0.0930]
Loss:[0.0923]
Loss:[0.0920]
Loss:[0.0954]
Loss:[0.1048]
Loss:[0.1018]
Loss:[0.0921]
Loss:[0.1015]
Loss:[0.1248]
Loss:[0.0940]
Loss:[0.1479]
Early stopping!
Loading 125th epoch
acc:[0.7340]
acc:[0.7320]
acc:[0.7340]
acc:[0.7340]
acc:[0.7330]
acc:[0.7350]
acc:[0.7330]
acc:[0.7340]
acc:[0.7340]
acc:[0.7390]
acc:[0.7380]
acc:[0.7330]
acc:[0.7330]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7360]
acc:[0.7360]
acc:[0.7320]
acc:[0.7340]
acc:[0.7330]
acc:[0.7370]
acc:[0.7350]
acc:[0.7340]
acc:[0.7330]
acc:[0.7340]
acc:[0.7330]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7360]
acc:[0.7370]
acc:[0.7360]
acc:[0.7340]
acc:[0.7340]
acc:[0.7350]
acc:[0.7330]
acc:[0.7350]
acc:[0.7330]
acc:[0.7340]
acc:[0.7360]
acc:[0.7330]
acc:[0.7340]
acc:[0.7350]
acc:[0.7330]
acc:[0.7330]
acc:[0.7340]
acc:[0.7340]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7344]
Mean:[73.4440]
Std :[0.1445]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.4, gpu=5, save_name='cite_best_dgi.pkl', seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6887]
Loss:[0.6862]
Loss:[0.6772]
Loss:[0.6792]
Loss:[0.6652]
Loss:[0.6640]
Loss:[0.6513]
Loss:[0.6417]
Loss:[0.6306]
Loss:[0.6175]
Loss:[0.6046]
Loss:[0.5883]
Loss:[0.5715]
Loss:[0.5606]
Loss:[0.5373]
Loss:[0.5235]
Loss:[0.5061]
Loss:[0.4814]
Loss:[0.4667]
Loss:[0.4555]
Loss:[0.4253]
Loss:[0.4096]
Loss:[0.4009]
Loss:[0.3829]
Loss:[0.3599]
Loss:[0.3488]
Loss:[0.3422]
Loss:[0.3215]
Loss:[0.3030]
Loss:[0.3029]
Loss:[0.2903]
Loss:[0.2698]
Loss:[0.2594]
Loss:[0.2557]
Loss:[0.2492]
Loss:[0.2356]
Loss:[0.2290]
Loss:[0.2190]
Loss:[0.2081]
Loss:[0.2068]
Loss:[0.2053]
Loss:[0.1929]
Loss:[0.1904]
Loss:[0.1800]
Loss:[0.1760]
Loss:[0.1712]
Loss:[0.1696]
Loss:[0.1662]
Loss:[0.1652]
Loss:[0.1640]
Loss:[0.1536]
Loss:[0.1535]
Loss:[0.1506]
Loss:[0.1416]
Loss:[0.1379]
Loss:[0.1514]
Loss:[0.1517]
Loss:[0.1381]
Loss:[0.1336]
Loss:[0.1364]
Loss:[0.1446]
Loss:[0.1389]
Loss:[0.1312]
Loss:[0.1338]
Loss:[0.1310]
Loss:[0.1360]
Loss:[0.1279]
Loss:[0.1298]
Loss:[0.1278]
Loss:[0.1268]
Loss:[0.1179]
Loss:[0.1189]
Loss:[0.1278]
Loss:[0.1185]
Loss:[0.1173]
Loss:[0.1196]
Loss:[0.1229]
Loss:[0.1102]
Loss:[0.1104]
Loss:[0.1141]
Loss:[0.1155]
Loss:[0.1091]
Loss:[0.1097]
Loss:[0.1122]
Loss:[0.1163]
Loss:[0.1061]
Loss:[0.1086]
Loss:[0.1070]
Loss:[0.1017]
Loss:[0.1092]
Loss:[0.1040]
Loss:[0.1082]
Loss:[0.1083]
Loss:[0.1124]
Loss:[0.1062]
Loss:[0.1067]
Loss:[0.1068]
Loss:[0.1075]
Loss:[0.1024]
Loss:[0.0979]
Loss:[0.1045]
Loss:[0.1054]
Loss:[0.1035]
Loss:[0.1022]
Loss:[0.1014]
Loss:[0.0980]
Loss:[0.1024]
Loss:[0.1042]
Loss:[0.1040]
Loss:[0.1070]
Loss:[0.0933]
Loss:[0.0948]
Loss:[0.1122]
Loss:[0.1046]
Loss:[0.1028]
Loss:[0.0991]
Loss:[0.0956]
Loss:[0.1015]
Loss:[0.0931]
Loss:[0.0969]
Loss:[0.0910]
Loss:[0.0970]
Loss:[0.0910]
Loss:[0.0892]
Loss:[0.0904]
Loss:[0.0990]
Loss:[0.0946]
Loss:[0.0989]
Loss:[0.0973]
Loss:[0.1046]
Loss:[0.0922]
Loss:[0.0964]
Loss:[0.0897]
Loss:[0.0930]
Loss:[0.0912]
Loss:[0.0905]
Loss:[0.0955]
Loss:[0.1025]
Loss:[0.0970]
Loss:[0.0924]
Loss:[0.0931]
Loss:[0.1067]
Loss:[0.0937]
Loss:[0.1055]
Early stopping!
Loading 125th epoch
acc:[0.7350]
acc:[0.7340]
acc:[0.7360]
acc:[0.7350]
acc:[0.7360]
acc:[0.7330]
acc:[0.7370]
acc:[0.7340]
acc:[0.7360]
acc:[0.7380]
acc:[0.7400]
acc:[0.7330]
acc:[0.7360]
acc:[0.7370]
acc:[0.7380]
acc:[0.7370]
acc:[0.7360]
acc:[0.7370]
acc:[0.7340]
acc:[0.7340]
acc:[0.7360]
acc:[0.7370]
acc:[0.7390]
acc:[0.7380]
acc:[0.7360]
acc:[0.7380]
acc:[0.7360]
acc:[0.7360]
acc:[0.7370]
acc:[0.7350]
acc:[0.7360]
acc:[0.7350]
acc:[0.7370]
acc:[0.7380]
acc:[0.7360]
acc:[0.7350]
acc:[0.7340]
acc:[0.7380]
acc:[0.7360]
acc:[0.7370]
acc:[0.7350]
acc:[0.7370]
acc:[0.7380]
acc:[0.7350]
acc:[0.7350]
acc:[0.7350]
acc:[0.7340]
acc:[0.7340]
acc:[0.7360]
acc:[0.7370]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7360]
Mean:[73.6040]
Std :[0.1551]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.5, gpu=5, save_name='cite_best_dgi.pkl', seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6887]
Loss:[0.6862]
Loss:[0.6772]
Loss:[0.6792]
Loss:[0.6652]
Loss:[0.6640]
Loss:[0.6513]
Loss:[0.6417]
Loss:[0.6306]
Loss:[0.6175]
Loss:[0.6046]
Loss:[0.5883]
Loss:[0.5715]
Loss:[0.5606]
Loss:[0.5373]
Loss:[0.5235]
Loss:[0.5061]
Loss:[0.4814]
Loss:[0.4667]
Loss:[0.4554]
Loss:[0.4253]
Loss:[0.4097]
Loss:[0.4010]
Loss:[0.3828]
Loss:[0.3599]
Loss:[0.3488]
Loss:[0.3422]
Loss:[0.3215]
Loss:[0.3031]
Loss:[0.3030]
Loss:[0.2903]
Loss:[0.2698]
Loss:[0.2595]
Loss:[0.2557]
Loss:[0.2491]
Loss:[0.2356]
Loss:[0.2291]
Loss:[0.2190]
Loss:[0.2081]
Loss:[0.2069]
Loss:[0.2054]
Loss:[0.1929]
Loss:[0.1903]
Loss:[0.1800]
Loss:[0.1761]
Loss:[0.1713]
Loss:[0.1696]
Loss:[0.1662]
Loss:[0.1652]
Loss:[0.1640]
Loss:[0.1537]
Loss:[0.1535]
Loss:[0.1506]
Loss:[0.1416]
Loss:[0.1379]
Loss:[0.1514]
Loss:[0.1516]
Loss:[0.1381]
Loss:[0.1336]
Loss:[0.1364]
Loss:[0.1447]
Loss:[0.1390]
Loss:[0.1311]
Loss:[0.1339]
Loss:[0.1310]
Loss:[0.1360]
Loss:[0.1278]
Loss:[0.1299]
Loss:[0.1279]
Loss:[0.1267]
Loss:[0.1180]
Loss:[0.1191]
Loss:[0.1278]
Loss:[0.1185]
Loss:[0.1174]
Loss:[0.1196]
Loss:[0.1227]
Loss:[0.1101]
Loss:[0.1104]
Loss:[0.1140]
Loss:[0.1154]
Loss:[0.1090]
Loss:[0.1097]
Loss:[0.1122]
Loss:[0.1163]
Loss:[0.1060]
Loss:[0.1086]
Loss:[0.1071]
Loss:[0.1018]
Loss:[0.1091]
Loss:[0.1041]
Loss:[0.1084]
Loss:[0.1082]
Loss:[0.1124]
Loss:[0.1062]
Loss:[0.1067]
Loss:[0.1068]
Loss:[0.1075]
Loss:[0.1024]
Loss:[0.0980]
Loss:[0.1045]
Loss:[0.1054]
Loss:[0.1034]
Loss:[0.1022]
Loss:[0.1014]
Loss:[0.0979]
Loss:[0.1024]
Loss:[0.1042]
Loss:[0.1039]
Loss:[0.1069]
Loss:[0.0933]
Loss:[0.0947]
Loss:[0.1118]
Loss:[0.1047]
Loss:[0.1028]
Loss:[0.0989]
Loss:[0.0956]
Loss:[0.1016]
Loss:[0.0932]
Loss:[0.0970]
Loss:[0.0910]
Loss:[0.0971]
Loss:[0.0910]
Loss:[0.0891]
Loss:[0.0904]
Loss:[0.0987]
Loss:[0.0943]
Loss:[0.0989]
Loss:[0.0969]
Loss:[0.1038]
Loss:[0.0922]
Loss:[0.0961]
Loss:[0.0892]
Loss:[0.0930]
Loss:[0.0907]
Loss:[0.0899]
Loss:[0.0956]
Loss:[0.1017]
Loss:[0.0951]
Loss:[0.0926]
Loss:[0.0906]
Loss:[0.1013]
Loss:[0.0943]
Loss:[0.0958]
Early stopping!
Loading 125th epoch
acc:[0.7360]
acc:[0.7340]
acc:[0.7350]
acc:[0.7330]
acc:[0.7350]
acc:[0.7340]
acc:[0.7330]
acc:[0.7340]
acc:[0.7330]
acc:[0.7390]
acc:[0.7360]
acc:[0.7320]
acc:[0.7350]
acc:[0.7370]
acc:[0.7360]
acc:[0.7350]
acc:[0.7350]
acc:[0.7370]
acc:[0.7360]
acc:[0.7320]
acc:[0.7330]
acc:[0.7340]
acc:[0.7360]
acc:[0.7360]
acc:[0.7340]
acc:[0.7370]
acc:[0.7330]
acc:[0.7330]
acc:[0.7350]
acc:[0.7320]
acc:[0.7340]
acc:[0.7350]
acc:[0.7350]
acc:[0.7360]
acc:[0.7350]
acc:[0.7330]
acc:[0.7330]
acc:[0.7350]
acc:[0.7340]
acc:[0.7360]
acc:[0.7340]
acc:[0.7340]
acc:[0.7390]
acc:[0.7340]
acc:[0.7340]
acc:[0.7350]
acc:[0.7340]
acc:[0.7350]
acc:[0.7340]
acc:[0.7330]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7346]
Mean:[73.4640]
Std :[0.1575]
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Namespace(aug_type='subgraph', dataset='citeseer', drop_percent=0.6, gpu=5, save_name='cite_best_dgi.pkl', seed=31)
----------------------------------------------------------------------------------------------------
/data3/Syd/06_IMPROVE/07_Contrastive_GNN/08_DGI_double/utils/process.py:183: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Begin Aug:[subgraph]
Using CUDA
Loss:[0.6931]
Loss:[0.6930]
Loss:[0.6887]
Loss:[0.6862]
Loss:[0.6772]
Loss:[0.6792]
Loss:[0.6652]
Loss:[0.6640]
Loss:[0.6513]
Loss:[0.6417]
Loss:[0.6306]
Loss:[0.6175]
Loss:[0.6045]
Loss:[0.5883]
Loss:[0.5715]
Loss:[0.5606]
Loss:[0.5373]
Loss:[0.5235]
Loss:[0.5061]
Loss:[0.4814]
Loss:[0.4667]
Loss:[0.4555]
Loss:[0.4253]
Loss:[0.4096]
Loss:[0.4010]
Loss:[0.3829]
Loss:[0.3599]
Loss:[0.3489]
Loss:[0.3422]
Loss:[0.3215]
Loss:[0.3031]
Loss:[0.3031]
Loss:[0.2902]
Loss:[0.2698]
Loss:[0.2597]
Loss:[0.2557]
Loss:[0.2491]
Loss:[0.2358]
Loss:[0.2293]
Loss:[0.2189]
Loss:[0.2079]
Loss:[0.2070]
Loss:[0.2057]
Loss:[0.1928]
Loss:[0.1900]
Loss:[0.1799]
Loss:[0.1762]
Loss:[0.1714]
Loss:[0.1695]
Loss:[0.1660]
Loss:[0.1652]
Loss:[0.1641]
Loss:[0.1537]
Loss:[0.1534]
Loss:[0.1506]
Loss:[0.1417]
Loss:[0.1380]
Loss:[0.1515]
Loss:[0.1515]
Loss:[0.1379]
Loss:[0.1335]
Loss:[0.1364]
Loss:[0.1446]
Loss:[0.1391]
Loss:[0.1313]
Loss:[0.1339]
Loss:[0.1310]
Loss:[0.1362]
Loss:[0.1279]
Loss:[0.1297]
Loss:[0.1279]
Loss:[0.1268]
Loss:[0.1179]
Loss:[0.1189]
Loss:[0.1278]
Loss:[0.1185]
Loss:[0.1172]
Loss:[0.1197]
Loss:[0.1231]
Loss:[0.1103]
Loss:[0.1104]
Loss:[0.1140]
Loss:[0.1156]
Loss:[0.1092]
Loss:[0.1096]
Loss:[0.1123]
Loss:[0.1164]
Loss:[0.1060]
Loss:[0.1085]
Loss:[0.1069]
Loss:[0.1017]
Loss:[0.1091]
Loss:[0.1038]
Loss:[0.1080]
Loss:[0.1082]
Loss:[0.1123]
Loss:[0.1059]
Loss:[0.1065]
Loss:[0.1068]
Loss:[0.1076]
Loss:[0.1024]
Loss:[0.0979]
Loss:[0.1044]
Loss:[0.1054]
Loss:[0.1034]
Loss:[0.1021]
Loss:[0.1015]
Loss:[0.0981]
Loss:[0.1024]
Loss:[0.1040]
Loss:[0.1041]
Loss:[0.1073]
Loss:[0.0934]
Loss:[0.0948]
Loss:[0.1124]
Loss:[0.1046]
Loss:[0.1028]
Loss:[0.0993]
Loss:[0.0955]
Loss:[0.1014]
Loss:[0.0930]
Loss:[0.0969]
Loss:[0.0909]
Loss:[0.0970]
Loss:[0.0911]
Loss:[0.0892]
Loss:[0.0905]
Loss:[0.0992]
Loss:[0.0948]
Loss:[0.0990]
Loss:[0.0977]
Loss:[0.1052]
Loss:[0.0921]
Loss:[0.0967]
Loss:[0.0902]
Loss:[0.0929]
Loss:[0.0916]
Loss:[0.0913]
Loss:[0.0953]
Loss:[0.1036]
Loss:[0.0994]
Loss:[0.0922]
Loss:[0.0968]
Loss:[0.1148]
Loss:[0.0935]
Loss:[0.1239]
Early stopping!
Loading 125th epoch
acc:[0.7350]
acc:[0.7340]
acc:[0.7340]
acc:[0.7350]
acc:[0.7330]
acc:[0.7320]
acc:[0.7340]
acc:[0.7370]
acc:[0.7360]
acc:[0.7380]
acc:[0.7400]
acc:[0.7340]
acc:[0.7370]
acc:[0.7350]
acc:[0.7350]
acc:[0.7320]
acc:[0.7370]
acc:[0.7360]
acc:[0.7350]
acc:[0.7310]
acc:[0.7340]
acc:[0.7330]
acc:[0.7350]
acc:[0.7380]
acc:[0.7340]
acc:[0.7390]
acc:[0.7340]
acc:[0.7330]
acc:[0.7350]
acc:[0.7360]
acc:[0.7360]
acc:[0.7350]
acc:[0.7340]
acc:[0.7360]
acc:[0.7340]
acc:[0.7350]
acc:[0.7340]
acc:[0.7360]
acc:[0.7370]
acc:[0.7410]
acc:[0.7360]
acc:[0.7340]
acc:[0.7380]
acc:[0.7340]
acc:[0.7330]
acc:[0.7340]
acc:[0.7340]
acc:[0.7320]
acc:[0.7350]
acc:[0.7340]
----------------------------------------------------------------------------------------------------
Average accuracy:[0.7351]
Mean:[73.5060]
Std :[0.2024]
----------------------------------------------------------------------------------------------------
